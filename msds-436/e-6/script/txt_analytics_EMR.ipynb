{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### Author Attribution Using Spark ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named nltk",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-38e0acd9307d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named nltk"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import shutil\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o14.wholeTextFiles.\n: java.io.IOException: No FileSystem for scheme: s3\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:500)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:469)\n\tat org.apache.spark.SparkContext$$anonfun$wholeTextFiles$1.apply(SparkContext.scala:875)\n\tat org.apache.spark.SparkContext$$anonfun$wholeTextFiles$1.apply(SparkContext.scala:870)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.SparkContext.withScope(SparkContext.scala:699)\n\tat org.apache.spark.SparkContext.wholeTextFiles(SparkContext.scala:870)\n\tat org.apache.spark.api.java.JavaSparkContext.wholeTextFiles(JavaSparkContext.scala:214)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-00b29206c03f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# if you happen to do the same operation from outside EMR  it will throw an \"Unknown filesystem\" error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwholeTextFiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3://aws-big-data-project/input_files/*.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_unicode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/spark-2.4.4-bin-hadoop2.7/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mwholeTextFiles\u001b[0;34m(self, path, minPartitions, use_unicode)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \"\"\"\n\u001b[1;32m    643\u001b[0m         \u001b[0mminPartitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminPartitions\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultMinPartitions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         return RDD(self._jsc.wholeTextFiles(path, minPartitions), self,\n\u001b[0m\u001b[1;32m    645\u001b[0m                    PairDeserializer(UTF8Deserializer(use_unicode), UTF8Deserializer(use_unicode)))\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o14.wholeTextFiles.\n: java.io.IOException: No FileSystem for scheme: s3\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:500)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setInputPaths(FileInputFormat.java:469)\n\tat org.apache.spark.SparkContext$$anonfun$wholeTextFiles$1.apply(SparkContext.scala:875)\n\tat org.apache.spark.SparkContext$$anonfun$wholeTextFiles$1.apply(SparkContext.scala:870)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.SparkContext.withScope(SparkContext.scala:699)\n\tat org.apache.spark.SparkContext.wholeTextFiles(SparkContext.scala:870)\n\tat org.apache.spark.api.java.JavaSparkContext.wholeTextFiles(JavaSparkContext.scala:214)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "# The method wholeTextFiles reads from a filesystem\n",
    "# s3 is not a filesystem However it has been configured to act as a filesystem in AWS EMR.\n",
    "# if you happen to do the same operation from outside EMR  it will throw an \"Unknown filesystem\" error\n",
    "\n",
    "data = sc.wholeTextFiles(\"s3://aws-big-data-project/input_files/*.txt\",use_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS TO PERFORM VARIOUS OPERATIONS\n",
    "\n",
    "def parse_text(text):\n",
    "    regex = re.compile(r'[\\ufeff\\n\\r\\t]')\n",
    "    t = regex.sub(\" \", text[1])\n",
    "    q = re.sub(r'\\d', ' ', t)\n",
    "    return (text[0],q)\n",
    "\n",
    "def deep_parse_text(text):\n",
    "    text = text.replace('\\ufeff', ' ')\n",
    "    text = re.sub(r'[-!$%^*&>:;,.?#/@\"()\\[\\]]',' ',text.lower().replace('\\r\\n', ' '))\n",
    "    text = re.sub(r'\\d', ' ', text)\n",
    "    text = re.sub(r'twain|chapter',' ',text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def filter_text(text, lst):\n",
    "    file = text[0]\n",
    "    bg = text[1]\n",
    "    res=[]\n",
    "\n",
    "    for j in bg:\n",
    "        if j in lst:\n",
    "            res.append(j)\n",
    "    return (file,res)\n",
    "\n",
    "def token_to_pos(ch):\n",
    "    tokens = nltk.word_tokenize(ch[1])\n",
    "    return [p[1] for p in nltk.pos_tag(tokens)]\n",
    "\n",
    "def Syn(ch):\n",
    "    chapters_pos = token_to_pos(ch)\n",
    "    pos_list = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS']\n",
    "    fvs_syntax = [[ch.count(pos) for pos in pos_list] for ch in chapters_pos]\n",
    "    fvs_syntax = fvs_syntax[0]\n",
    "\n",
    "    result=[]\n",
    "\n",
    "    for i in fvs_syntax:\n",
    "        for j in chapters_pos:\n",
    "            result.append(float(format(float(i/len(j)),'.4f')))\n",
    "\n",
    "    r1=result[0]\n",
    "    r2=result[1]\n",
    "    r3=result[2]\n",
    "    r4=result[3]\n",
    "    r5=result[4]\n",
    "    r6=result[5]\n",
    "\n",
    "    return (ch[0],r1,r2,r3,r4,r5,r6)\n",
    "\n",
    "def LexicalFeatures(chapters):\n",
    "    sentence_tokenizer = nltk.data.load('/usr/share/nltk_data/tokenizers/punkt/english.pickle')\n",
    "    word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    tokens = nltk.word_tokenize(chapters[1].lower())\n",
    "    words = word_tokenizer.tokenize(chapters[1].lower())\n",
    "    sentences = sentence_tokenizer.tokenize(chapters[1])\n",
    "    vocab = set(words)\n",
    "\n",
    "    words_per_sentence = np.array([len(word_tokenizer.tokenize(s)) for s in sentences])\n",
    "    fvs_lexical_0 = format(words_per_sentence.mean(),'.4f')\n",
    "    fvs_lexical_1 = format(words_per_sentence.std(),'.4f')\n",
    "    fvs_lexical_2 = format(len(vocab) / float(len(words)),'.4f')\n",
    "    fvs_punct_0 = format(tokens.count(',') / float(len(sentences)),'.4f')\n",
    "    fvs_punct_1 = format(tokens.count(';') / float(len(sentences)),'.4f')\n",
    "    fvs_punct_2 = format(tokens.count(':') / float(len(sentences)),'.4f')\n",
    "\n",
    "    return (chapters[0],float(fvs_lexical_0), float(fvs_lexical_1),float(fvs_lexical_2),float(fvs_punct_0),float(fvs_punct_1),float(fvs_punct_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f10c950472e40de966c9bf2fca5efb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PARSE THE DATA & PERFORM CLEANING \n",
    "\n",
    "data_parse = data.map(lambda x: (x[0].split(\"/\")[-1],deep_parse_text(x[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3191309046c412a8cd474de291c53e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TOKENIZE THE TEXT, REMOVE STOPWORDS & COUNT THE WORD REPEATITION \n",
    "\n",
    "textDF = spark.createDataFrame(data_parse, [\"label\", \"text\"])\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "wordsDataFrame = tokenizer.transform(textDF)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"stwords\")\n",
    "DF = remover.transform(wordsDataFrame)\n",
    "\n",
    "pair = DF.rdd.flatMap(lambda df: df[3]).map(lambda token: (token,1)).reduceByKey(lambda v1,v2 : v1+v2).sortBy(lambda x: x[1],False)\n",
    "\n",
    "stp = pair.map(lambda x: x[0]).collect()[0:20]\n",
    "\n",
    "\n",
    "filrdd = DF.rdd.map(lambda df: (df[0],df[3])).map(lambda l: filter_text(l,stp))\n",
    "FDF = spark.createDataFrame(filrdd, [\"label\", \"finalword\"])\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"finalword\", outputCol=\"features\")\n",
    "model = cv.fit(FDF)\n",
    "result = model.transform(FDF)\n",
    "finaldata = result.select(\"label\",\"features\")\n",
    "\n",
    "word_df = finaldata.rdd.map(lambda x: Row(Doc_Name=x[0],f1=float(x[1].toArray()[0]),f2=float(x[1].toArray()[1]),f3=float(x[1].toArray()[2]),f4=float(x[1].toArray()[3]),f5=float(x[1].toArray()[4]),f6=float(x[1].toArray()[5]),f7=float(x[1].toArray()[6]),f8=float(x[1].toArray()[7]),f9=float(x[1].toArray()[8]),f10=float(x[1].toArray()[9]),f11=float(x[1].toArray()[10]),f12=float(x[1].toArray()[11]),f13=float(x[1].toArray()[12]),f14=float(x[1].toArray()[13]),f15=float(x[1].toArray()[14]),f16=float(x[1].toArray()[15]),f17=float(x[1].toArray()[16]),f18=float(x[1].toArray()[17]),f19=float(x[1].toArray()[18]),f20=float(x[1].toArray()[19]))).toDF()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c296e4cc3b4887bbf97dfc25b51272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CREATE BIGRAMS & COUNT THEM\n",
    "\n",
    "bigram = NGram(n=2, inputCol=\"words\", outputCol=\"bigrams\")\n",
    "bigramDF = bigram.transform(wordsDataFrame)\n",
    "\n",
    "bipair = bigramDF.rdd.flatMap(lambda df: df[3]).map(lambda token: (token,1)).reduceByKey(lambda v1,v2 : v1+v2).sortBy(lambda x: x[1],False)\n",
    "bistp = bipair.map(lambda x: x[0]).collect()[0:10]\n",
    "\n",
    "bifilrdd = bigramDF.rdd.map(lambda df: (df[0],df[3])).map(lambda l: filter_text(l,bistp))\n",
    "biFDF = spark.createDataFrame(bifilrdd, [\"label\", \"finalword\"])\n",
    "\n",
    "bicv = CountVectorizer(inputCol=\"finalword\", outputCol=\"features\")\n",
    "bimodel = cv.fit(biFDF)\n",
    "biresult = bimodel.transform(biFDF)\n",
    "bifinaldata = biresult.select(\"label\",\"features\")\n",
    "\n",
    "bigram_df = bifinaldata.rdd.map(lambda x: Row(Doc_Name=x[0],bg1=float(x[1].toArray()[0]),bg2=float(x[1].toArray()[1]),bg3=float(x[1].toArray()[2]),bg4=float(x[1].toArray()[3]),bg5=float(x[1].toArray()[4]),bg6=float(x[1].toArray()[5]),bg7=float(x[1].toArray()[6]),bg8=float(x[1].toArray()[7]),bg9=float(x[1].toArray()[8]),bg10=float(x[1].toArray()[9]))).toDF()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7946cb1e7764790a54de839a445b848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CREATE TRIGRAMS & COUNT THEM\n",
    "\n",
    "trigram = NGram(n=3, inputCol=\"words\", outputCol=\"trigrams\")\n",
    "trigramDF = trigram.transform(wordsDataFrame)\n",
    "\n",
    "tripair = trigramDF.rdd.flatMap(lambda df: df[3]).map(lambda token: (token,1)).reduceByKey(lambda v1,v2 : v1+v2).sortBy(lambda x: x[1],False)\n",
    "tristp = tripair.map(lambda x: x[0]).collect()[0:10]\n",
    "\n",
    "trifilrdd = trigramDF.rdd.map(lambda df: (df[0],df[3])).map(lambda l: filter_text(l,tristp))\n",
    "triFDF = spark.createDataFrame(trifilrdd, [\"label\", \"finalword\"])\n",
    "\n",
    "tricv = CountVectorizer(inputCol=\"finalword\", outputCol=\"features\")\n",
    "trimodel = cv.fit(triFDF)\n",
    "triresult = trimodel.transform(triFDF)\n",
    "trifinaldata = triresult.select(\"label\",\"features\")\n",
    "\n",
    "trigram_df = trifinaldata.rdd.map(lambda x: Row(Doc_Name=x[0],tg1=float(x[1].toArray()[0]),tg2=float(x[1].toArray()[1]),tg3=float(x[1].toArray()[2]),tg4=float(x[1].toArray()[3]),tg5=float(x[1].toArray()[4]),tg6=float(x[1].toArray()[5]),tg7=float(x[1].toArray()[6]),tg8=float(x[1].toArray()[7]),tg9=float(x[1].toArray()[8]),tg10=float(x[1].toArray()[9]))).toDF()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c06738610a1437191f06cf0219a42c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CREATE 4GRAMS & COUNT THEM\n",
    "\n",
    "qgram = NGram(n=4, inputCol=\"words\", outputCol=\"qgrams\")\n",
    "qgramDF = qgram.transform(wordsDataFrame)\n",
    "\n",
    "qpair = qgramDF.rdd.flatMap(lambda df: df[3]).map(lambda token: (token,1)).reduceByKey(lambda v1,v2 : v1+v2).sortBy(lambda x: x[1],False)\n",
    "qstp = qpair.map(lambda x: x[0]).collect()[0:10]\n",
    "\n",
    "qfilrdd = qgramDF.rdd.map(lambda df: (df[0],df[3])).map(lambda l: filter_text(l,qstp))\n",
    "qFDF = spark.createDataFrame(qfilrdd, [\"label\", \"finalword\"])\n",
    "\n",
    "qcv = CountVectorizer(inputCol=\"finalword\", outputCol=\"features\")\n",
    "qmodel = cv.fit(qFDF)\n",
    "qresult = qmodel.transform(qFDF)\n",
    "qfinaldata = qresult.select(\"label\",\"features\")\n",
    "\n",
    "qgram_df = qfinaldata.rdd.map(lambda x: Row(Doc_Name=x[0],qg1=float(x[1].toArray()[0]),qg2=float(x[1].toArray()[1]),qg3=float(x[1].toArray()[2]),qg4=float(x[1].toArray()[3]),qg5=float(x[1].toArray()[4]),qg6=float(x[1].toArray()[5]),qg7=float(x[1].toArray()[6]),qg8=float(x[1].toArray()[7]),qg9=float(x[1].toArray()[8]),qg10=float(x[1].toArray()[9]))).toDF()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c8934ef74940e183fce29932c08833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CREATE LEXICAL & SYNTACTICAL FEATURES\n",
    "\n",
    "data2 = data.map(lambda x: (x[0].split(\"/\")[-1],x[1].lower()))\n",
    "\n",
    "data_new = data2.map(parse_text)\n",
    "\n",
    "lexical=data_new.map(LexicalFeatures)\n",
    "\n",
    "syntactic=data_new.map(Syn)\n",
    "\n",
    "lexical_df=spark.createDataFrame(lexical, ['Doc_Name','wrds_sent_mean', 'wrds_sent_std','Lex_diver','Commas_sent','Semicolon_sent','Colons_sent'])\n",
    "syntactic_df=spark.createDataFrame(syntactic, ['Doc_Name','NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57af035e6310400fbb6054ad766df567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CREATE INDIVIDUAL FEATURE LEVEL DATAFRAMES\n",
    "\n",
    "lexical_df.createOrReplaceTempView(\"lexical_df\")\n",
    "syntactic_df.createOrReplaceTempView(\"syntactic_df\")\n",
    "word_df.createOrReplaceTempView(\"word_df\")\n",
    "bigram_df.createOrReplaceTempView(\"bigram_df\")\n",
    "trigram_df.createOrReplaceTempView(\"trigram_df\")\n",
    "qgram_df.createOrReplaceTempView(\"qgram_df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50367b555f74ca4a73fc7e99514e80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# COMBINE ALL THE DATAFRAMES USING SQL\n",
    "\n",
    "final_df=spark.sql(\" SELECT CASE WHEN ld.Doc_Name == 'Austen1.txt' THEN '0' \\\n",
    "                                 WHEN ld.Doc_Name == 'Austen2.txt' THEN '0' \\\n",
    "                                 WHEN ld.Doc_Name == 'Austen3.txt' THEN '0' \\\n",
    "                                 WHEN ld.Doc_Name == 'Austen4.txt' THEN '0' \\\n",
    "                                 WHEN ld.Doc_Name == 'Austen5.txt' THEN '0' \\\n",
    "                                 WHEN ld.Doc_Name == 'Dickens1.txt' THEN '1' \\\n",
    "                                 WHEN ld.Doc_Name == 'Dickens2.txt' THEN '1' \\\n",
    "                                 WHEN ld.Doc_Name == 'Dickens3.txt' THEN '1' \\\n",
    "                                 WHEN ld.Doc_Name == 'Dickens4.txt' THEN '1' \\\n",
    "                                 WHEN ld.Doc_Name == 'Dickens5.txt' THEN '1' \\\n",
    "                                 WHEN ld.Doc_Name == 'Twain1.txt' THEN '2' \\\n",
    "                                 WHEN ld.Doc_Name == 'Twain2.txt' THEN '2' \\\n",
    "                                 WHEN ld.Doc_Name == 'Twain3.txt' THEN '2' \\\n",
    "                                 WHEN ld.Doc_Name == 'Twain4.txt' THEN '2' \\\n",
    "                                 WHEN ld.Doc_Name == 'Twain5.txt' THEN '2' END Author,\\\n",
    "                                 ld.wrds_sent_mean, ld.wrds_sent_std, ld.Lex_diver, ld.Commas_sent, ld.Semicolon_sent, ld.Colons_sent, \\\n",
    "                                 sd.NN, sd.NNP, sd.DT, sd.IN, sd.JJ, sd.NNS, wd.f1, wd.f2,  wd.f3, wd.f4, wd.f5, wd.f6, wd.f7, wd.f8, \\\n",
    "                                 wd.f9, wd.f10, wd.f11, wd.f12, wd.f13, wd.f14, wd.f15, wd.f16, wd.f17, wd.f18, wd.f19, wd.f20, bd.bg1, \\\n",
    "                                 bd.bg2, bd.bg3, bd.bg4, bd.bg5, bd.bg6, bd.bg7, bd.bg8, bd.bg9, bd.bg10, td.tg1, td.tg2, td.tg3, td.tg4, \\\n",
    "                                 td.tg5, td.tg6, td.tg7, td.tg8, td.tg9, td.tg10, qd.qg1, qd.qg2, qd.qg3, qd.qg4, qd.qg5, qd.qg6, qd.qg7, \\\n",
    "                                 qd.qg8, qd.qg9, qd.qg10,ld.Doc_Name \\\n",
    "                      FROM lexical_df ld, syntactic_df sd, word_df wd, bigram_df bd, trigram_df td, qgram_df qd \\\n",
    "                      WHERE ld.Doc_Name = sd.Doc_Name and ld.Doc_Name = wd.Doc_Name \\\n",
    "                        and ld.Doc_Name = bd.Doc_Name and ld.Doc_Name = td.Doc_Name and ld.Doc_Name = qd.Doc_Name \")\n",
    "\n",
    "final_df.createOrReplaceTempView(\"final_df\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97986d5d8add4f86b15cd53dc1f0a542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024e1282b9b2456299fd2be0457b9b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CREATE TRAINING & TEST DATAFRAMES - SPLIT THE FINAL DATAFRAME INTO 2\n",
    "\n",
    "training_df=spark.sql(\"SELECT Author, wrds_sent_mean, wrds_sent_std, Lex_diver, Commas_sent, Semicolon_sent, Colons_sent, NN, NNP, DT, IN, JJ, NNS, \\\n",
    "                           f1, f2,  f3, f4, f5, f6, f7, f8, f9, f10, f11, f12, f13, f14, f15, f16, f17, f18, f19, f20, bg1, bg2, bg3, bg4, bg5, \\\n",
    "                           bg6, bg7, bg8, bg9, bg10, tg1, tg2, tg3, tg4, tg5, tg6, tg7, tg8, tg9, tg10, qg1, qg2, qg3, qg4, qg5, qg6, qg7, qg8, \\\n",
    "                           qg9, qg10 FROM final_df WHERE Doc_Name not in ('Austen2.txt', 'Dickens3.txt', 'Twain5.txt')\")\n",
    "\n",
    "test_df=spark.sql(\"SELECT Author, wrds_sent_mean, wrds_sent_std, Lex_diver, Commas_sent, Semicolon_sent, Colons_sent, NN, NNP, DT, IN, JJ, NNS, \\\n",
    "                           f1, f2,  f3, f4, f5, f6, f7, f8, f9, f10, f11, f12, f13, f14, f15, f16, f17, f18, f19, f20, bg1, bg2, bg3, bg4, bg5, \\\n",
    "                           bg6, bg7, bg8, bg9, bg10, tg1, tg2, tg3, tg4, tg5, tg6, tg7, tg8, tg9, tg10, qg1, qg2, qg3, qg4, qg5, qg6, qg7, qg8, \\\n",
    "                           qg9, qg10 FROM final_df WHERE Doc_Name in ('Austen2.txt', 'Dickens3.txt', 'Twain5.txt')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d13db0ab234dda9b551254302d8add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CONVERT TO RDD\n",
    "\n",
    "training= training_df.rdd.map(lambda row: LabeledPoint(row[0], row[1:]))\n",
    "test= test_df.rdd.map(lambda row: LabeledPoint(row[0], row[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc0802d0cfe4430b7be2fdc0c6cfa62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Logistic Regression : \n",
      "\n",
      "[[4. 0. 0.]\n",
      " [0. 4. 0.]\n",
      " [0. 0. 4.]]\n",
      "\n",
      "Summary Of Statistics For Logistic Regression\n",
      "\n",
      "Total Accuracy  : 1.00\n",
      "\n",
      "Total Precision : 1.00\t Precision 1: 1.00\t Precision 2: 1.00\t Precision 3: 1.00\n",
      "\n",
      "Total Recall    : 1.00\t Recall 1   : 1.00\t Recall 2   : 1.00\t Recall 3   : 1.00\n",
      "\n",
      "Total fMeasure  : 1.00"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "\n",
    "                           #   LOGISTIC REGRESSION   #                           \n",
    "\n",
    "#################################################################################\n",
    "\n",
    "## TRAINING THE MODEL\n",
    "\n",
    "modelLR = LogisticRegressionWithLBFGS.train(training, numClasses=3)\n",
    "predictionAndLabelsLR = training.map(lambda l: (float(modelLR.predict(l.features)), l.label))\n",
    "metricsLR = MulticlassMetrics(predictionAndLabelsLR)\n",
    "\n",
    "print('Confusion Matrix for Logistic Regression : \\n\\n' + str(metricsLR.confusionMatrix().toArray()) +\n",
    "'\\n\\nSummary Of Statistics For Logistic Regression\\n\\nTotal Accuracy  : ' + str(format(metricsLR.accuracy,'.2f')) +\n",
    "'\\n\\nTotal Precision : ' + str(format(metricsLR.precision(),'.2f')) + '\\t Precision 1: ' + str(format(metricsLR.precision(0),'.2f')) + '\\t Precision 2: ' + str(format(metricsLR.precision(1),'.2f')) + '\\t Precision 3: ' + str(format(metricsLR.precision(2),'.2f')) +\n",
    "'\\n\\nTotal Recall    : ' + str(format(metricsLR.recall(),'.2f')) + '\\t Recall 1   : ' + str(format(metricsLR.recall(0),'.2f')) + '\\t Recall 2   : ' + str(format(metricsLR.recall(1),'.2f')) + '\\t Recall 3   : ' + str(format(metricsLR.recall(2),'.2f')) +\n",
    "'\\n\\nTotal fMeasure  : ' + str(format(metricsLR.fMeasure(),'.2f')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5107b9ff9a7b4e35aed04ee187af9416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions using test data:\n",
      "\n",
      "[(1.0, 1.0), (2.0, 2.0), (0.0, 0.0)]"
     ]
    }
   ],
   "source": [
    "## PREDICTIONS USING THE MODEL\n",
    "\n",
    "print('\\nPredictions using test data:\\n')\n",
    "predictionAndLabelsLR_test = test.map(lambda l: (float(modelLR.predict(l.features)), l.label))\n",
    "predictionAndLabelsLR_test.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4f04912ec646acac9916addae7aaae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Naive Bayes : \n",
      "\n",
      "[[4. 0. 0.]\n",
      " [1. 2. 1.]\n",
      " [0. 0. 4.]]\n",
      "\n",
      "Summary Of Statistics For Naive Bayes\n",
      "\n",
      "Total Accuracy  : 0.83\n",
      "\n",
      "Total Precision : 0.83\t Precision 1: 0.80\t Precision 2: 1.00\t Precision 3: 0.80\n",
      "\n",
      "Total Recall    : 0.83\t Recall 1   : 1.00\t Recall 2   : 0.50\t Recall 3   : 1.00\n",
      "\n",
      "Total fMeasure  : 0.83"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "\n",
    "                               #   NAIVE BAYES   #                               \n",
    "\n",
    "#################################################################################\n",
    "\n",
    "## TRAINING THE MODEL\n",
    "\n",
    "modelNB = NaiveBayes.train(training)\n",
    "predictionAndLabelsNB = training.map(lambda l: (float(modelNB.predict(l.features)), l.label))\n",
    "metricsNB = MulticlassMetrics(predictionAndLabelsNB)\n",
    "\n",
    "print('Confusion Matrix for Naive Bayes : \\n\\n' + str(metricsNB.confusionMatrix().toArray()) +\n",
    "'\\n\\nSummary Of Statistics For Naive Bayes\\n\\nTotal Accuracy  : ' + str(format(metricsNB.accuracy,'.2f')) +\n",
    "'\\n\\nTotal Precision : ' + str(format(metricsNB.precision(),'.2f')) + '\\t Precision 1: ' + str(format(metricsNB.precision(0),'.2f')) + '\\t Precision 2: ' + str(format(metricsNB.precision(1),'.2f')) + '\\t Precision 3: ' + str(format(metricsNB.precision(2),'.2f')) +\n",
    "'\\n\\nTotal Recall    : ' + str(format(metricsNB.recall(),'.2f')) + '\\t Recall 1   : ' + str(format(metricsNB.recall(0),'.2f')) + '\\t Recall 2   : ' + str(format(metricsNB.recall(1),'.2f')) + '\\t Recall 3   : ' + str(format(metricsNB.recall(2),'.2f')) +\n",
    "'\\n\\nTotal fMeasure  : ' + str(format(metricsNB.fMeasure(),'.2f'))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33cf0940277641cd8a02f5ede8aa7ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions using test data:\n",
      "\n",
      "[(1.0, 1.0), (2.0, 2.0), (0.0, 0.0)]"
     ]
    }
   ],
   "source": [
    "## PREDICTIONS USING THE MODEL\n",
    "\n",
    "print(\"\\nPredictions using test data:\\n\")\n",
    "predictionAndLabels_testNB = test.map(lambda l: (float(modelNB.predict(l.features)), l.label))\n",
    "predictionAndLabels_testNB.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################### The End ######################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
