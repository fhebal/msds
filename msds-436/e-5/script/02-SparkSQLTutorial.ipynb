{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################################<br>\n",
    "File: SparkSQL tutorial <br>\n",
    "Desc: Introduction to SparkSQL <br>\n",
    "Auth: Shreenidhi Bharadwaj<br>\n",
    "Date: 9/29/2019<br>\n",
    "ALL RIGHTS RESERVED | DO NOT DISTRIBUTE<br>\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is to introduce you to the fundamentals of Spark SQL.\n",
    "Quick summary of how Spark SQL works:\n",
    "a) It uses DataFrames, which are collections of data distributed across data nodes.\n",
    "b) Driver program typically launches the application. Executors, which run on data nodes,\n",
    "    are responsible for running the actual code.\n",
    "c) Results are typically returned to the driver.\n",
    "You will need a SQL context. You create one as follows:\n",
    "sqlContext = SQLContext(sc) where sc is the SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets ignore warnings for now\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0\r\n"
     ]
    }
   ],
   "source": [
    "!ps -aef | grep jupyter-lab -m 1 | cut -d ' ' -f 2 | xargs lsof -p | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SPARK & SQL Context\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A DataFrame is like a relational table. It consists of Row objects\n",
    "#A row object can be created as follows\n",
    "from pyspark.sql import Row\n",
    "cricketer = Row(name='Viv Richards', age = 62, country='WI', batting_average=58.67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Viv Richards'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cricketer.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Viv Richards', 62, 58.67)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cricketer.name, cricketer.age, cricketer.batting_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Viv Richards', 62, 58.67)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cricketer['name'], cricketer['age'], cricketer['batting_average']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can create a table of cricketers from a Python list of tuples\n",
    "cricketers = [('Viv Richards', 62, \"WI\", 58.67), ('Greg Chappell', 64, \"AUS\", 56.83),\\\n",
    "              ('Doug Walters', 68, \"AUS\", 48.9), ('VVS Laxman', 41, \"IND\", 46.7),\\\n",
    "              ('Sachin Tendulkar', 42, 'IND', 57.8), ('Rahul Dravid', 41, 'IND', 54.5),\\\n",
    "              ('Garry Sobers', 78, \"WI\", 58.9), ('Rohan Kanhai', 75, \"WI\", 51.3),\\\n",
    "              ('GR Vishwanath', 68, \"IND\", 44.3)]\n",
    "cricketers_df = sqlContext.createDataFrame(cricketers, ['Name','Age','Country','Average'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---+-------+-------+\n",
      "|            Name|Age|Country|Average|\n",
      "+----------------+---+-------+-------+\n",
      "|    Viv Richards| 62|     WI|  58.67|\n",
      "|   Greg Chappell| 64|    AUS|  56.83|\n",
      "|    Doug Walters| 68|    AUS|   48.9|\n",
      "|      VVS Laxman| 41|    IND|   46.7|\n",
      "|Sachin Tendulkar| 42|    IND|   57.8|\n",
      "|    Rahul Dravid| 41|    IND|   54.5|\n",
      "|    Garry Sobers| 78|     WI|   58.9|\n",
      "|    Rohan Kanhai| 75|     WI|   51.3|\n",
      "|   GR Vishwanath| 68|    IND|   44.3|\n",
      "+----------------+---+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use show() to display records; show(n) will display n records\n",
    "cricketers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+-------+-------+\n",
      "|        Name|Age|Country|Average|\n",
      "+------------+---+-------+-------+\n",
      "|Viv Richards| 62|     WI|  58.67|\n",
      "|Garry Sobers| 78|     WI|   58.9|\n",
      "|Rohan Kanhai| 75|     WI|   51.3|\n",
      "+------------+---+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use filter to limit records - example shows cricketers from WI (West Indies)\n",
    "cricketers_df.filter(cricketers_df.Country == \"WI\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Average: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let us look at the schema of our dataframe\n",
    "cricketers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+-------+-------+\n",
      "|        Name|Age|Country|Average|\n",
      "+------------+---+-------+-------+\n",
      "|Garry Sobers| 78|     WI|   58.9|\n",
      "|Rohan Kanhai| 75|     WI|   51.3|\n",
      "+------------+---+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display cricketers who are over 70\n",
    "cricketers_df.where(cricketers_df.Age > 70).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| ID|       NAME|\n",
      "+---+-----------+\n",
      "|IND|      INDIA|\n",
      "| WI|WEST INDIES|\n",
      "|AUS|  AUSTRALIA|\n",
      "|ENG|    ENGLAND|\n",
      "| NZ|NEW ZEALAND|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let us create another dataframe for Country Abbreviations and Country Names\n",
    "countries = [(\"IND\", \"INDIA\"), (\"WI\", \"WEST INDIES\"), (\"AUS\", \"AUSTRALIA\"), \\\n",
    "             (\"ENG\", \"ENGLAND\"), (\"NZ\", \"NEW ZEALAND\")]\n",
    "countries_df = sqlContext.createDataFrame(countries, [\"ID\",\"NAME\"])\n",
    "countries_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+-------+-------+---+-----------+\n",
      "|            Name| Age|Country|Average| ID|       NAME|\n",
      "+----------------+----+-------+-------+---+-----------+\n",
      "|   Greg Chappell|  64|    AUS|  56.83|AUS|  AUSTRALIA|\n",
      "|    Doug Walters|  68|    AUS|   48.9|AUS|  AUSTRALIA|\n",
      "|    Viv Richards|  62|     WI|  58.67| WI|WEST INDIES|\n",
      "|    Garry Sobers|  78|     WI|   58.9| WI|WEST INDIES|\n",
      "|    Rohan Kanhai|  75|     WI|   51.3| WI|WEST INDIES|\n",
      "|            null|null|   null|   null|ENG|    ENGLAND|\n",
      "|            null|null|   null|   null| NZ|NEW ZEALAND|\n",
      "|      VVS Laxman|  41|    IND|   46.7|IND|      INDIA|\n",
      "|Sachin Tendulkar|  42|    IND|   57.8|IND|      INDIA|\n",
      "|    Rahul Dravid|  41|    IND|   54.5|IND|      INDIA|\n",
      "|   GR Vishwanath|  68|    IND|   44.3|IND|      INDIA|\n",
      "+----------------+----+-------+-------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let us try out different join operations\n",
    "outer_join_df = cricketers_df.join(countries_df, cricketers_df.Country == countries_df.ID, 'outer')\n",
    "outer_join_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---+-------+-------+---+-----------+\n",
      "|            Name|Age|Country|Average| ID|       NAME|\n",
      "+----------------+---+-------+-------+---+-----------+\n",
      "|   Greg Chappell| 64|    AUS|  56.83|AUS|  AUSTRALIA|\n",
      "|    Doug Walters| 68|    AUS|   48.9|AUS|  AUSTRALIA|\n",
      "|    Viv Richards| 62|     WI|  58.67| WI|WEST INDIES|\n",
      "|    Garry Sobers| 78|     WI|   58.9| WI|WEST INDIES|\n",
      "|    Rohan Kanhai| 75|     WI|   51.3| WI|WEST INDIES|\n",
      "|      VVS Laxman| 41|    IND|   46.7|IND|      INDIA|\n",
      "|Sachin Tendulkar| 42|    IND|   57.8|IND|      INDIA|\n",
      "|    Rahul Dravid| 41|    IND|   54.5|IND|      INDIA|\n",
      "|   GR Vishwanath| 68|    IND|   44.3|IND|      INDIA|\n",
      "+----------------+---+-------+-------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inner_join_df = cricketers_df.join(countries_df, cricketers_df.Country == countries_df.ID, 'inner')\n",
    "inner_join_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---+-------+-------+---+-----------+\n",
      "|            Name|Age|Country|Average| ID|       NAME|\n",
      "+----------------+---+-------+-------+---+-----------+\n",
      "|   Greg Chappell| 64|    AUS|  56.83|AUS|  AUSTRALIA|\n",
      "|    Doug Walters| 68|    AUS|   48.9|AUS|  AUSTRALIA|\n",
      "|    Viv Richards| 62|     WI|  58.67| WI|WEST INDIES|\n",
      "|    Garry Sobers| 78|     WI|   58.9| WI|WEST INDIES|\n",
      "|    Rohan Kanhai| 75|     WI|   51.3| WI|WEST INDIES|\n",
      "|      VVS Laxman| 41|    IND|   46.7|IND|      INDIA|\n",
      "|Sachin Tendulkar| 42|    IND|   57.8|IND|      INDIA|\n",
      "|    Rahul Dravid| 41|    IND|   54.5|IND|      INDIA|\n",
      "|   GR Vishwanath| 68|    IND|   44.3|IND|      INDIA|\n",
      "+----------------+---+-------+-------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left_outer_join_df = cricketers_df.join(countries_df, cricketers_df.Country == countries_df.ID, 'left_outer')\n",
    "left_outer_join_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+-------+-------+---+-----------+\n",
      "|            Name| Age|Country|Average| ID|       NAME|\n",
      "+----------------+----+-------+-------+---+-----------+\n",
      "|   Greg Chappell|  64|    AUS|  56.83|AUS|  AUSTRALIA|\n",
      "|    Doug Walters|  68|    AUS|   48.9|AUS|  AUSTRALIA|\n",
      "|    Viv Richards|  62|     WI|  58.67| WI|WEST INDIES|\n",
      "|    Garry Sobers|  78|     WI|   58.9| WI|WEST INDIES|\n",
      "|    Rohan Kanhai|  75|     WI|   51.3| WI|WEST INDIES|\n",
      "|            null|null|   null|   null|ENG|    ENGLAND|\n",
      "|            null|null|   null|   null| NZ|NEW ZEALAND|\n",
      "|      VVS Laxman|  41|    IND|   46.7|IND|      INDIA|\n",
      "|Sachin Tendulkar|  42|    IND|   57.8|IND|      INDIA|\n",
      "|    Rahul Dravid|  41|    IND|   54.5|IND|      INDIA|\n",
      "|   GR Vishwanath|  68|    IND|   44.3|IND|      INDIA|\n",
      "+----------------+----+-------+-------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "right_outer_join_df = cricketers_df.join(countries_df, cricketers_df.Country == countries_df.ID, 'right_outer')\n",
    "right_outer_join_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+-------+\n",
      "|            Name|Country_Name|Average|\n",
      "+----------------+------------+-------+\n",
      "|   Greg Chappell|   AUSTRALIA|  56.83|\n",
      "|    Doug Walters|   AUSTRALIA|   48.9|\n",
      "|    Viv Richards| WEST INDIES|  58.67|\n",
      "|    Garry Sobers| WEST INDIES|   58.9|\n",
      "|    Rohan Kanhai| WEST INDIES|   51.3|\n",
      "|      VVS Laxman|       INDIA|   46.7|\n",
      "|Sachin Tendulkar|       INDIA|   57.8|\n",
      "|    Rahul Dravid|       INDIA|   54.5|\n",
      "|   GR Vishwanath|       INDIA|   44.3|\n",
      "+----------------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inner_join_df = cricketers_df.join(countries_df, cricketers_df.Country == countries_df.ID, 'inner')\\\n",
    "                .select(cricketers_df.Name, countries_df.NAME.alias(\"Country_Name\"), 'Average').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|The history of th...|\n",
      "|                    |\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#reading from a text file\n",
    "text_df = sqlContext.read.text(\"../data/sample.txt\")\n",
    "text_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     words|\n",
      "+----------+\n",
      "|       the|\n",
      "|   history|\n",
      "|        of|\n",
      "|       the|\n",
      "|    united|\n",
      "|   states,|\n",
      "|         a|\n",
      "|   country|\n",
      "|        in|\n",
      "|     north|\n",
      "|  america,|\n",
      "|   started|\n",
      "|      with|\n",
      "|       the|\n",
      "|   arrival|\n",
      "|        of|\n",
      "|indigenous|\n",
      "|    people|\n",
      "|      from|\n",
      "|   siberia|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let us get the words into a column\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import udf, explode\n",
    "my_func = udf(lambda s: s.lower().split(), ArrayType(StringType()))\n",
    "words_df = text_df.select(my_func(text_df.value).alias('words'))\n",
    "\n",
    "#words_df.show()\n",
    "#we need to \"explode\" the list of words so that we have one word per row\n",
    "exploded_df = words_df.select(explode(words_df.words).alias(\"words\"))\n",
    "exploded_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|   words|count|\n",
      "+--------+-----+\n",
      "|     the|   17|\n",
      "|      of|    8|\n",
      "|      in|    4|\n",
      "|     and|    3|\n",
      "|    most|    2|\n",
      "| british|    2|\n",
      "|       a|    2|\n",
      "|  people|    2|\n",
      "|colonies|    2|\n",
      "|  before|    2|\n",
      "| arrival|    2|\n",
      "|      to|    2|\n",
      "|  united|    2|\n",
      "|   after|    2|\n",
      "| started|    2|\n",
      "|      by|    1|\n",
      "|   along|    1|\n",
      "|   their|    1|\n",
      "|  1760s,|    1|\n",
      "|numerous|    1|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get the count for each word, sorted in descending order\n",
    "exploded_df.groupBy('words').count().sort(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n",
      "+---+--------+-------------+-------------+\n",
      "| id|category|categoryIndex|  categoryVec|\n",
      "+---+--------+-------------+-------------+\n",
      "|  0|       a|          0.0|(2,[0],[1.0])|\n",
      "|  1|       b|          2.0|    (2,[],[])|\n",
      "|  2|       c|          1.0|(2,[1],[1.0])|\n",
      "|  3|       a|          0.0|(2,[0],[1.0])|\n",
      "|  4|       a|          0.0|(2,[0],[1.0])|\n",
      "|  5|       c|          1.0|(2,[1],[1.0])|\n",
      "+---+--------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "df = sqlContext.createDataFrame([\n",
    "    (0, \"a\"),\n",
    "    (1, \"b\"),\n",
    "    (2, \"c\"),\n",
    "    (3, \"a\"),\n",
    "    (4, \"a\"),\n",
    "    (5, \"c\")\n",
    "], [\"id\", \"category\"])\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "indexed.show()\n",
    "encoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\")\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized using L^1 norm\n",
      "+---+--------------+------------------+\n",
      "| id|      features|      normFeatures|\n",
      "+---+--------------+------------------+\n",
      "|  0|[1.0,0.5,-1.0]|    [0.4,0.2,-0.4]|\n",
      "|  1| [2.0,1.0,1.0]|   [0.5,0.25,0.25]|\n",
      "|  2|[4.0,10.0,2.0]|[0.25,0.625,0.125]|\n",
      "+---+--------------+------------------+\n",
      "\n",
      "Normalized using L^inf norm\n",
      "+---+--------------+--------------+\n",
      "| id|      features|  normFeatures|\n",
      "+---+--------------+--------------+\n",
      "|  0|[1.0,0.5,-1.0]|[1.0,0.5,-1.0]|\n",
      "|  1| [2.0,1.0,1.0]| [1.0,0.5,0.5]|\n",
      "|  2|[4.0,10.0,2.0]| [0.4,1.0,0.2]|\n",
      "+---+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "dataFrame = sqlContext.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.5, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.0, 1.0]),),\n",
    "    (2, Vectors.dense([4.0, 10.0, 2.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "# Normalize each Vector using $L^1$ norm.\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    "l1NormData = normalizer.transform(dataFrame)\n",
    "print(\"Normalized using L^1 norm\")\n",
    "l1NormData.show()\n",
    "\n",
    "# Normalize each Vector using $L^\\infty$ norm.\n",
    "lInfNormData = normalizer.transform(dataFrame, {normalizer.p: float(\"inf\")})\n",
    "print(\"Normalized using L^inf norm\")\n",
    "lInfNormData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------------------------------------------------------+\n",
      "|id |features      |scaledFeatures                                              |\n",
      "+---+--------------+------------------------------------------------------------+\n",
      "|0  |[1.0,0.5,-1.0]|[0.6546536707079772,0.09352195295828244,-0.6546536707079771]|\n",
      "|1  |[2.0,1.0,1.0] |[1.3093073414159544,0.1870439059165649,0.6546536707079771]  |\n",
      "|2  |[4.0,10.0,2.0]|[2.618614682831909,1.870439059165649,1.3093073414159542]    |\n",
      "+---+--------------+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(dataFrame)\n",
    "scaledData.show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\n",
      "+-----------------------+-------+\n",
      "|features               |clicked|\n",
      "+-----------------------+-------+\n",
      "|[18.0,1.0,0.0,10.0,0.5]|1.0    |\n",
      "+-----------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "dataset = sqlContext.createDataFrame(\n",
    "    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],\n",
    "    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"])\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"hour\", \"mobile\", \"userFeatures\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(dataset)\n",
    "print(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")\n",
    "output.select(\"features\", \"clicked\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|category|\n",
      "+---+--------+\n",
      "|  0|       a|\n",
      "|  1|       b|\n",
      "|  2|       c|\n",
      "|  3|       a|\n",
      "|  4|       a|\n",
      "|  5|    null|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dealing with missing values\n",
    "df = sqlContext.createDataFrame([\n",
    "    (0, \"a\"),\n",
    "    (1, \"b\"),\n",
    "    (2, \"c\"),\n",
    "    (3, \"a\"),\n",
    "    (4, \"a\"),\n",
    "    (5, None)\n",
    "], [\"id\", \"category\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|category|\n",
      "+---+--------+\n",
      "|  0|       a|\n",
      "|  1|       b|\n",
      "|  2|       c|\n",
      "|  3|       a|\n",
      "|  4|       a|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dropped_df = df.na.drop()\n",
    "dropped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|category|\n",
      "+---+--------+\n",
      "|  0|       a|\n",
      "|  1|       b|\n",
      "|  2|       c|\n",
      "|  3|       a|\n",
      "|  4|       a|\n",
      "|  5|       b|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "replaced_df = df.na.fill(\"b\")\n",
    "replaced_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
