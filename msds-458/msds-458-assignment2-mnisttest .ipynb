{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment #1\n",
    "Choosing the number of hidden neurons\n",
    "* The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "* The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "* The number of hidden neurons should be less than twice the size of the input layer.\n",
    "\n",
    "https://www.heatonresearch.com/2017/06/01/hidden-layers.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment #2\n",
    "Choosing the number of hidden layers\n",
    "* In artificial neural networks, hidden layers are required if and only if the data must be separated non-linearly.\n",
    "\n",
    "https://towardsdatascience.com/beginners-ask-how-many-hidden-layers-neurons-to-use-in-artificial-neural-networks-51466afa0d3e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment #3\n",
    "Varying all other hyperparameters (Optimizer, Activation, Loss, Learning Rate, Momentum) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment #4\n",
    "Varying the DropOut and other "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment #5\n",
    "GridSearch best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment #6\n",
    "Data augmentation (e.g. combining groups of similar clothes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment #7\n",
    "Ensembled CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard integrated circuit can be seen as a digital network of activation functions that can be \"ON\" (1) or \"OFF\" (0), depending on input. This is similar to the behavior of the linear perceptron in neural networks. However, only nonlinear activation functions allow such networks to compute nontrivial problems using only a small number of nodes, and such activation functions are called nonlinearities\n",
    "\n",
    "## References\n",
    "1. https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/\n",
    "2. https://arxiv.org/pdf/1801.09403.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = [\n",
    "    'deserialize', 'elu', 'exponential', 'get', 'hard_sigmoid', 'linear', 'relu','selu', \n",
    "    'serialize', 'sigmoid', 'softmax', 'softplus', 'softsign', 'swish', 'tanh',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otimizers\n",
    "Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses.Optimization algorithms or strategies are responsible for reducing the losses and to provide the most accurate results possible.\n",
    "## Conclusions\n",
    "* Adam is the best optimizer. If one wants to train the neural network in less time and more efficiently than Adam is the optimizer.\n",
    "* For sparse data use the optimizers with dynamic learning rate.\n",
    "* If, want to use gradient descent algorithm than min-batch gradient descent is the best option.\n",
    "* TL;DR Adam works well in practice and outperforms other Adaptive techniques.\n",
    "* Use SGD+Nesterov for shallow networks, and either Adam or RMSprop for deepnets.\n",
    "\n",
    "## References\n",
    "1. https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6#:~:text=Optimizers%20are%20algorithms%20or%20methods,order%20to%20reduce%20the%20losses.&text=Optimization%20algorithms%20or%20strategies%20are,the%20most%20accurate%20results%20possible.\n",
    "2. https://www.dlology.com/blog/quick-notes-on-how-to-choose-optimizer-in-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = [\n",
    "    'SGD', \n",
    "    'RMSprop', \n",
    "    'Adam', \n",
    "    'Adadelta', \n",
    "    'Adagrad', \n",
    "    'Adamax', \n",
    "    'Nadam', \n",
    "    'Ftrl', \n",
    "    'rmsprop',\n",
    "]\n",
    "\n",
    "# Adam = RMSprop + Momentum\n",
    "# keras.optimizers.SGD(lr=0.01, nesterov=True)  <-- SGD + Nesterov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [\n",
    "    'BinaryCrossentropy',\n",
    "    'CategoricalCrossentropy',\n",
    "    'SparseCategoricalCrossentropy',\n",
    "    'Poisson',\n",
    "    'binary_crossentropy',\n",
    "    'categorical_crossentropy',\n",
    "    'sparse_categorical_crossentropy',\n",
    "    'poisson',\n",
    "    'KLDivergence',\n",
    "    'kl_divergence',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load MNIST data set:\n",
      "x_train:\t(60000, 28, 28)\n",
      "y_train:\t(60000,)\n",
      "x_test:\t\t(10000, 28, 28)\n",
      "y_test:\t\t(10000,)\n",
      "\n",
      "Encode y data:\n",
      "First ten entries of y_train:\n",
      " [5 0 4 1 9 2 1 3 1 4]\n",
      "\n",
      "First ten rows of one-hot y_train:\n",
      " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "y_train_encoded shape:  (60000, 10)\n",
      "y_test_encoded shape:  (10000, 10)\n",
      "x_train_reshaped shape:  (60000, 784)\n",
      "x_test_reshaped shape:  (10000, 784)\n",
      "\n",
      "Min-Max Normalization:\n",
      "{0.0, 0.011764706, 0.53333336, 0.07058824, 0.49411765, 0.6862745, 0.101960786, 0.6509804, 1.0, 0.96862745, 0.49803922, 0.11764706, 0.14117648, 0.36862746, 0.6039216, 0.6666667, 0.043137256, 0.05490196, 0.03529412, 0.85882354, 0.7764706, 0.7137255, 0.94509804, 0.3137255, 0.6117647, 0.41960785, 0.25882354, 0.32156864, 0.21960784, 0.8039216, 0.8666667, 0.8980392, 0.52156866, 0.7882353, 0.18039216, 0.30588236, 0.44705883, 0.3529412, 0.15294118, 0.6745098, 0.9490196, 0.7647059, 0.88235295, 0.99215686, 0.2509804, 0.19215687, 0.93333334, 0.9843137, 0.74509805, 0.7294118, 0.5882353, 0.50980395, 0.8862745, 0.105882354, 0.09019608, 0.16862746, 0.13725491, 0.21568628, 0.46666667, 0.3647059, 0.27450982, 0.8352941, 0.7176471, 0.5803922, 0.8117647, 0.9764706, 0.83137256, 0.98039216, 0.95686275, 0.003921569, 0.73333335, 0.54509807, 0.5294118, 0.67058825, 0.007843138, 0.31764707, 0.0627451, 0.09411765, 0.42352942, 0.627451, 0.9411765, 0.9882353, 0.5176471, 0.09803922, 0.1764706}\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(\"Load MNIST data set:\")\n",
    "(x_train, y_train), (x_test, y_test)= mnist.load_data()\n",
    "print('x_train:\\t{}'.format(x_train.shape))\n",
    "print('y_train:\\t{}'.format(y_train.shape))\n",
    "print('x_test:\\t\\t{}'.format(x_test.shape))\n",
    "print('y_test:\\t\\t{}'.format(y_test.shape))\n",
    "\n",
    "print()\n",
    "print(\"Encode y data:\") \n",
    "y_train_encoded = to_categorical(y_train)\n",
    "y_test_encoded = to_categorical(y_test)\n",
    "print(\"First ten entries of y_train:\\n {}\\n\".format(y_train[0:10]))\n",
    "print(\"First ten rows of one-hot y_train:\\n {}\".format(y_train_encoded[0:10,]))\n",
    "print()\n",
    "print('y_train_encoded shape: ', y_train_encoded.shape)\n",
    "print('y_test_encoded shape: ', y_test_encoded.shape)\n",
    "x_train_reshaped = np.reshape(x_train, (60000, 784))\n",
    "x_test_reshaped = np.reshape(x_test, (10000, 784))\n",
    "print('x_train_reshaped shape: ', x_train_reshaped.shape)\n",
    "print('x_test_reshaped shape: ', x_test_reshaped.shape)\n",
    "\n",
    "\n",
    "print()\n",
    "print('Min-Max Normalization:')\n",
    "x_train_norm = x_train_reshaped.astype('float32') / 255\n",
    "x_test_norm = x_test_reshaped.astype('float32') / 255\n",
    "print(set(x_train_norm[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learn_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>momentum</th>\n",
       "      <th>nb_epoch</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [activation, batch_size, learn_rate, loss, momentum, nb_epoch, optimizer, mean_test_score, std_test_score, mean_fit_time]\n",
       "Index: []"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the learning rate and momentum\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "#from keras.optimizers import SGD\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(learn_rate=0.6, momentum=0, optimizer='adam', activation='relu', loss='binary_crossentropy'):\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(name = \"hidden_layer\", units=512, input_shape = (784,), activation=activation))\n",
    "    model.add(Dense(name = \"output_layer\", units=10, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model)\n",
    "# overfitting is really the issue for machine learning such a big problem\n",
    "# we will face this all the time, methods to address this include DropOut\n",
    "# research other ways to mitigate overfitting\n",
    "# how much dropout (20%, 10%, vary the dropout? dropout rates, hyperparameter tuning)\n",
    "# you should vary the nodes but i don't know how, in param_grid?\n",
    "# plot the results\n",
    "# document that i did NOT vary the node numbers etc...\n",
    "# scikit learn and tensorflow spun out of Google\n",
    "# does it matter which place the layers go?\n",
    "# squeeze between layers\n",
    "# vary the architechture of the model programatically!\n",
    "\n",
    "# Flatten layer <-- it essentially creates the 784 thing\n",
    "# The dense layer <-- this is the hidden layer\n",
    "\n",
    "#units_1 = [10,20,30,512,]\n",
    "#units_2 = [0.3]#[0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "\n",
    "# rmsprop and adam will differentiat based on batching...\n",
    "# some optimizers are better for \n",
    "# when you change between cpu gpu and tpu, optimized for certain things\n",
    "# if you are prebatching data - you will runa cycle where everythign through the processing units are zeros\n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        'activation', 'batch_size', 'learn_rate', 'loss',\n",
    "        'momentum', 'nb_epoch', 'optimizer', 'mean_test_score',\n",
    "        'std_test_score', 'mean_fit_time'\n",
    "    ])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(grid_result):\n",
    "    para = pd.DataFrame.from_dict(grid_result.cv_results_['params'])\n",
    "    mean = pd.DataFrame(grid_result.cv_results_['mean_test_score'],columns=['mean_test_score'])\n",
    "    stds = pd.DataFrame(grid_result.cv_results_['std_test_score'],columns=['std_test_score'])\n",
    "    time = pd.DataFrame(grid_result.cv_results_['mean_fit_time'],columns=['mean_fit_time'])\n",
    "    \n",
    "    df = para.join(mean.join(stds)).join(time).sort_values('mean_test_score', ascending=False)\n",
    "    df.reset_index().drop(columns=['index'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard integrated circuit can be seen as a digital network of activation functions that can be \"ON\" (1) or \"OFF\" (0), depending on input. This is similar to the behavior of the linear perceptron in neural networks. However, only nonlinear activation functions allow such networks to compute nontrivial problems using only a small number of nodes, and such activation functions are called nonlinearities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0873 - accuracy: 0.8790\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0584 - accuracy: 0.9212\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0874 - accuracy: 0.8769\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0621 - accuracy: 0.9136\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0892 - accuracy: 0.8763\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0594 - accuracy: 0.9204\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0731 - accuracy: 0.8955\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0415 - accuracy: 0.9458\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0757 - accuracy: 0.8907\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0442 - accuracy: 0.9422\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0738 - accuracy: 0.8934\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0410 - accuracy: 0.9464\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.8289\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0724 - accuracy: 0.8990\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1175 - accuracy: 0.8333\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0738 - accuracy: 0.8928\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1211 - accuracy: 0.8312\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0703 - accuracy: 0.9018\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0949 - accuracy: 0.8745\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0777 - accuracy: 0.9014\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0943 - accuracy: 0.8734\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0807 - accuracy: 0.8942\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0966 - accuracy: 0.8719\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.8994\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0770 - accuracy: 0.8925\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0413 - accuracy: 0.9416\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0769 - accuracy: 0.8927\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0434 - accuracy: 0.9400\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0790 - accuracy: 0.8908\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0419 - accuracy: 0.9434\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0868 - accuracy: 0.8786\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0626 - accuracy: 0.9143\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0837 - accuracy: 0.8826\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0659 - accuracy: 0.9123\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0877 - accuracy: 0.8773\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0617 - accuracy: 0.9179\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1199 - accuracy: 0.8359\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.9008\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1155 - accuracy: 0.8424\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0720 - accuracy: 0.8986\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1178 - accuracy: 0.8353\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.9047\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.5275 - accuracy: 0.1397\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4285 - accuracy: 0.1141\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.5321 - accuracy: 0.1283\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4341 - accuracy: 0.1052\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.5278 - accuracy: 0.1305\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4282 - accuracy: 0.0996\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0977 - accuracy: 0.8582\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0643 - accuracy: 0.9141\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0960 - accuracy: 0.8577\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0652 - accuracy: 0.9121\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0950 - accuracy: 0.8594\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0618 - accuracy: 0.9169\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0898 - accuracy: 0.8746\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0555 - accuracy: 0.9205\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0888 - accuracy: 0.8767\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0580 - accuracy: 0.9187\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0892 - accuracy: 0.8766\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0549 - accuracy: 0.9225\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0852 - accuracy: 0.8816\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0500 - accuracy: 0.9314\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0854 - accuracy: 0.8821\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0520 - accuracy: 0.9279\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0856 - accuracy: 0.8803\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0496 - accuracy: 0.9331\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0850 - accuracy: 0.8784\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0547 - accuracy: 0.9233\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0832 - accuracy: 0.8842\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0573 - accuracy: 0.9206\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0849 - accuracy: 0.8808\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0549 - accuracy: 0.9241\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0649 - accuracy: 0.9082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learn_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>momentum</th>\n",
       "      <th>nb_epoch</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>exponential</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.944800</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>1.156888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.941650</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>1.184122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>swish</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.930800</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>1.584021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tanh</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>1.233272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>softsign</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.920583</td>\n",
       "      <td>0.001511</td>\n",
       "      <td>1.468389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     activation batch_size  learn_rate                 loss momentum nb_epoch  \\\n",
       "1   exponential        100        0.01  binary_crossentropy        0       10   \n",
       "4          relu        100        0.01  binary_crossentropy        0       10   \n",
       "10        swish        100        0.01  binary_crossentropy        0       10   \n",
       "11         tanh        100        0.01  binary_crossentropy        0       10   \n",
       "9      softsign        100        0.01  binary_crossentropy        0       10   \n",
       "\n",
       "   optimizer  mean_test_score  std_test_score  mean_fit_time  \n",
       "1       adam         0.944800        0.001855       1.156888  \n",
       "4       adam         0.941650        0.001390       1.184122  \n",
       "10      adam         0.930800        0.002165       1.584021  \n",
       "11      adam         0.922700        0.001520       1.233272  \n",
       "9       adam         0.920583        0.001511       1.468389  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search parameters\n",
    "learn_rate = [0.01]\n",
    "momentum = [0]\n",
    "optimizers = ['adam']\n",
    "epochs = [10]\n",
    "batches = [100]\n",
    "activation = [\n",
    "    'elu', 'exponential', 'hard_sigmoid', 'linear', \n",
    "    'relu', 'selu', 'sigmoid', 'softmax', 'softplus', \n",
    "    'softsign', 'swish', 'tanh',\n",
    "]\n",
    "loss = ['binary_crossentropy']\n",
    "\n",
    "param_grid = dict(\n",
    "    learn_rate=learn_rate,\n",
    "    momentum=momentum,\n",
    "    optimizer=optimizers,\n",
    "    nb_epoch=epochs,\n",
    "    batch_size=batches,\n",
    "    activation=activation,\n",
    "    loss=loss\n",
    ")\n",
    "\n",
    "#history = model.fit(x_train_norm,y_train_encoded, epochs=20,validation_split=0.20,batch_size = 10)\n",
    "grid_result = GridSearchCV(estimator=model, param_grid=param_grid, cv=3).fit(x_train_norm, y_train_encoded)\n",
    "df = print_results(grid_result)\n",
    "results_df = results_df.append(df)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum\n",
    "Momentum was invented for reducing high variance in SGD and softens the convergence. It accelerates the convergence towards the relevant direction and reduces the fluctuation to the irrelevant direction. One more hyperparameter is used in this method known as momentum symbolized by ‘γ’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0770 - accuracy: 0.8921\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0414 - accuracy: 0.9420\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0763 - accuracy: 0.8945\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0430 - accuracy: 0.9385\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0788 - accuracy: 0.8903\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0413 - accuracy: 0.9431\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0784 - accuracy: 0.8914\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0406 - accuracy: 0.9421\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0755 - accuracy: 0.8921\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0424 - accuracy: 0.9392\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0776 - accuracy: 0.8907\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0407 - accuracy: 0.9449\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0795 - accuracy: 0.8870\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0423 - accuracy: 0.9398\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0775 - accuracy: 0.8904\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0430 - accuracy: 0.9390\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0785 - accuracy: 0.8913\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0409 - accuracy: 0.9422\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0785 - accuracy: 0.8891\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0434 - accuracy: 0.9368\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.0759 - accuracy: 0.8940\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0422 - accuracy: 0.9398\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0770 - accuracy: 0.8918\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0414 - accuracy: 0.9434\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0790 - accuracy: 0.8885\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0414 - accuracy: 0.9404\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0745 - accuracy: 0.8933\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0424 - accuracy: 0.9402\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0779 - accuracy: 0.8912\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0422 - accuracy: 0.9409\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0773 - accuracy: 0.8922\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0413 - accuracy: 0.9405\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0775 - accuracy: 0.8932\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0433 - accuracy: 0.9369\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0783 - accuracy: 0.8925\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0420 - accuracy: 0.9401\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0653 - accuracy: 0.9088\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learn_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>momentum</th>\n",
       "      <th>nb_epoch</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>exponential</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.944800</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>1.156888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.941650</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>1.184122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>swish</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.930800</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>1.584021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tanh</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>1.233272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>softsign</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.920583</td>\n",
       "      <td>0.001511</td>\n",
       "      <td>1.468389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     activation batch_size  learn_rate                 loss momentum nb_epoch  \\\n",
       "1   exponential        100        0.01  binary_crossentropy        0       10   \n",
       "4          relu        100        0.01  binary_crossentropy        0       10   \n",
       "10        swish        100        0.01  binary_crossentropy        0       10   \n",
       "11         tanh        100        0.01  binary_crossentropy        0       10   \n",
       "9      softsign        100        0.01  binary_crossentropy        0       10   \n",
       "\n",
       "   optimizer  mean_test_score  std_test_score  mean_fit_time  \n",
       "1       adam         0.944800        0.001855       1.156888  \n",
       "4       adam         0.941650        0.001390       1.184122  \n",
       "10      adam         0.930800        0.002165       1.584021  \n",
       "11      adam         0.922700        0.001520       1.233272  \n",
       "9       adam         0.920583        0.001511       1.468389  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search parameters\n",
    "learn_rate = [0.01]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "optimizers = ['adam']\n",
    "epochs = [10]\n",
    "batches = [100]\n",
    "activation = ['relu']\n",
    "loss = ['binary_crossentropy']\n",
    "\n",
    "param_grid = dict(\n",
    "    learn_rate=learn_rate,\n",
    "    momentum=momentum,\n",
    "    optimizer=optimizers,\n",
    "    nb_epoch=epochs,\n",
    "    batch_size=batches,\n",
    "    activation=activation,\n",
    "    loss=loss\n",
    ")\n",
    "\n",
    "#history = model.fit(x_train_norm,y_train_encoded, epochs=20,validation_split=0.20,batch_size = 10)\n",
    "grid_result = GridSearchCV(estimator=model, param_grid=param_grid, cv=3).fit(x_train_norm, y_train_encoded)\n",
    "df = print_results(grid_result)\n",
    "results_df = results_df.append(df)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate\n",
    "In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. Since it influences to what extent newly acquired information overrides old information, it metaphorically represents the speed at which a machine learning model \"learns\". In the adaptive control literature, the learning rate is commonly referred to as gain.\n",
    "\n",
    "In setting a learning rate, there is a trade-off between the rate of convergence and overshooting. While the descent direction is usually determined from the gradient of the loss function, the learning rate determines how big a step is taken in that direction. A too high learning rate will make the learning jump over minima but a too low learning rate will either take too long to converge or get stuck in an undesirable local minimum.\n",
    "\n",
    "In order to achieve faster convergence, prevent oscillations and getting stuck in undesirable local minima the learning rate is often varied during training either in accordance to a learning rate schedule or by using an adaptive learning rate. The learning rate and its adjustments may also differ per parameter, in which case it is a diagonal matrix that can be interpreted as an approximation to the inverse of the Hessian matrix in Newton's method. The learning rate is related to the step length determined by inexact line search in quasi-Newton methods and related optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0769 - accuracy: 0.8928\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0411 - accuracy: 0.9429\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0773 - accuracy: 0.8901\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0436 - accuracy: 0.9364\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0762 - accuracy: 0.8943\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0408 - accuracy: 0.9433\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0767 - accuracy: 0.8934\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0417 - accuracy: 0.9407\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0774 - accuracy: 0.8941\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0424 - accuracy: 0.9403\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0769 - accuracy: 0.8920\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0398 - accuracy: 0.9443\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0775 - accuracy: 0.8926\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0429 - accuracy: 0.9380\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0776 - accuracy: 0.8921\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0442 - accuracy: 0.9365\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0797 - accuracy: 0.8863\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0422 - accuracy: 0.9410\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0794 - accuracy: 0.8868\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0427 - accuracy: 0.9380\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0762 - accuracy: 0.8925\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0439 - accuracy: 0.9374\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0783 - accuracy: 0.8902\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0412 - accuracy: 0.9414\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0787 - accuracy: 0.8894\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0417 - accuracy: 0.9415\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0790 - accuracy: 0.8910\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0444 - accuracy: 0.9372\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0800 - accuracy: 0.8881\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0421 - accuracy: 0.9411\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0773 - accuracy: 0.8918\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0409 - accuracy: 0.9422\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0772 - accuracy: 0.8914\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0452 - accuracy: 0.9355\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0800 - accuracy: 0.8887\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0418 - accuracy: 0.9403\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.0625 - accuracy: 0.9116\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learn_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>momentum</th>\n",
       "      <th>nb_epoch</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>exponential</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.944800</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>1.156888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.941650</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>1.184122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>swish</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.930800</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>1.584021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tanh</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>1.233272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>softsign</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.920583</td>\n",
       "      <td>0.001511</td>\n",
       "      <td>1.468389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>elu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.918433</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>1.193996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>selu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.914817</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>1.191628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>softplus</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.914367</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>1.461108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.901400</td>\n",
       "      <td>0.002526</td>\n",
       "      <td>1.182006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>linear</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.898350</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>1.154006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hard_sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.897900</td>\n",
       "      <td>0.003756</td>\n",
       "      <td>1.402831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>softmax</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.106267</td>\n",
       "      <td>0.005967</td>\n",
       "      <td>1.539642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.942100</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>1.321403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.941233</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>1.204483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.8</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.940517</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>1.281978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.940350</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>1.236278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.6</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.940017</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>1.551555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.9</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.939150</td>\n",
       "      <td>0.001635</td>\n",
       "      <td>1.315121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.941783</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>1.444349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.940850</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>1.211069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      activation batch_size  learn_rate                 loss momentum  \\\n",
       "1    exponential        100        0.01  binary_crossentropy        0   \n",
       "4           relu        100        0.01  binary_crossentropy        0   \n",
       "10         swish        100        0.01  binary_crossentropy        0   \n",
       "11          tanh        100        0.01  binary_crossentropy        0   \n",
       "9       softsign        100        0.01  binary_crossentropy        0   \n",
       "0            elu        100        0.01  binary_crossentropy        0   \n",
       "5           selu        100        0.01  binary_crossentropy        0   \n",
       "8       softplus        100        0.01  binary_crossentropy        0   \n",
       "6        sigmoid        100        0.01  binary_crossentropy        0   \n",
       "3         linear        100        0.01  binary_crossentropy        0   \n",
       "2   hard_sigmoid        100        0.01  binary_crossentropy        0   \n",
       "7        softmax        100        0.01  binary_crossentropy        0   \n",
       "1           relu        100        0.01  binary_crossentropy      0.2   \n",
       "0           relu        100        0.01  binary_crossentropy        0   \n",
       "4           relu        100        0.01  binary_crossentropy      0.8   \n",
       "2           relu        100        0.01  binary_crossentropy      0.4   \n",
       "3           relu        100        0.01  binary_crossentropy      0.6   \n",
       "5           relu        100        0.01  binary_crossentropy      0.9   \n",
       "1           relu        100        0.10  binary_crossentropy        0   \n",
       "0           relu        100        0.01  binary_crossentropy        0   \n",
       "\n",
       "   nb_epoch optimizer  mean_test_score  std_test_score  mean_fit_time  \n",
       "1        10      adam         0.944800        0.001855       1.156888  \n",
       "4        10      adam         0.941650        0.001390       1.184122  \n",
       "10       10      adam         0.930800        0.002165       1.584021  \n",
       "11       10      adam         0.922700        0.001520       1.233272  \n",
       "9        10      adam         0.920583        0.001511       1.468389  \n",
       "0        10      adam         0.918433        0.003433       1.193996  \n",
       "5        10      adam         0.914817        0.002321       1.191628  \n",
       "8        10      adam         0.914367        0.001969       1.461108  \n",
       "6        10      adam         0.901400        0.002526       1.182006  \n",
       "3        10      adam         0.898350        0.003012       1.154006  \n",
       "2        10      adam         0.897900        0.003756       1.402831  \n",
       "7        10      adam         0.106267        0.005967       1.539642  \n",
       "1        10      adam         0.942100        0.002307       1.321403  \n",
       "0        10      adam         0.941233        0.001955       1.204483  \n",
       "4        10      adam         0.940517        0.000278       1.281978  \n",
       "2        10      adam         0.940350        0.001353       1.236278  \n",
       "3        10      adam         0.940017        0.002697       1.551555  \n",
       "5        10      adam         0.939150        0.001635       1.315121  \n",
       "1        10      adam         0.941783        0.001785       1.444349  \n",
       "0        10      adam         0.940850        0.003150       1.211069  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search parameters\n",
    "learn_rate = [0.01, 0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "momentum = [0.0]\n",
    "optimizers = ['adam']\n",
    "epochs = [10]\n",
    "batches = [100]\n",
    "activation = ['relu']\n",
    "loss = ['binary_crossentropy']\n",
    "\n",
    "param_grid = dict(\n",
    "    learn_rate=learn_rate,\n",
    "    momentum=momentum,\n",
    "    optimizer=optimizers,\n",
    "    nb_epoch=epochs,\n",
    "    batch_size=batches,\n",
    "    activation=activation,\n",
    "    loss=loss\n",
    ")\n",
    "\n",
    "#history = model.fit(x_train_norm,y_train_encoded, epochs=20,validation_split=0.20,batch_size = 10)\n",
    "grid_result = GridSearchCV(estimator=model, param_grid=param_grid, cv=3).fit(x_train_norm, y_train_encoded)\n",
    "df = print_results(grid_result)\n",
    "results_df = results_df.append(df)\n",
    "results_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otimizers\n",
    "Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses.Optimization algorithms or strategies are responsible for reducing the losses and to provide the most accurate results possible.\n",
    "\n",
    "Adam is the best optimizer. If one wants to train the neural network in less time and more efficiently than Adam is the optimizer. For sparse data use the optimizers with dynamic learning rate. If, want to use gradient descent algorithm than min-batch gradient descent is the best option. TL;DR Adam works well in practice and outperforms other Adaptive techniques. Use SGD+Nesterov for shallow networks, and either Adam or RMSprop for deepnets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3854 - accuracy: 0.2658\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3040 - accuracy: 0.5137\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3662 - accuracy: 0.3128\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2975 - accuracy: 0.5673\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3779 - accuracy: 0.3571\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3004 - accuracy: 0.5420\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0673 - accuracy: 0.9040\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0381 - accuracy: 0.9438\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0659 - accuracy: 0.9064\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0373 - accuracy: 0.9445\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0665 - accuracy: 0.9056\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0363 - accuracy: 0.9478\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0777 - accuracy: 0.8920\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0416 - accuracy: 0.9406\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0784 - accuracy: 0.8903\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0442 - accuracy: 0.9390\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0773 - accuracy: 0.8916\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0407 - accuracy: 0.9418\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.6930 - accuracy: 0.0827\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.6492 - accuracy: 0.0901\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.6937 - accuracy: 0.1116\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.6475 - accuracy: 0.1207\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.6556 - accuracy: 0.1511\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.6170 - accuracy: 0.1558\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.4923 - accuracy: 0.1956\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3695 - accuracy: 0.2870\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.4960 - accuracy: 0.1932\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3723 - accuracy: 0.2833\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.5062 - accuracy: 0.1317\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3854 - accuracy: 0.2285\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1075 - accuracy: 0.8589\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0662 - accuracy: 0.9121\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1061 - accuracy: 0.8635\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0682 - accuracy: 0.9080\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1070 - accuracy: 0.8609\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0642 - accuracy: 0.9150\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0759 - accuracy: 0.8967\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0396 - accuracy: 0.9420\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0733 - accuracy: 0.9002\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0406 - accuracy: 0.9431\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0762 - accuracy: 0.8975\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0402 - accuracy: 0.9421\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.6871 - accuracy: 0.1110\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6824 - accuracy: 0.1141\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.6870 - accuracy: 0.1097\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6824 - accuracy: 0.1141\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.6870 - accuracy: 0.1135\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.6824 - accuracy: 0.1089\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.0555 - accuracy: 0.9214\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learn_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>momentum</th>\n",
       "      <th>nb_epoch</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>exponential</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.944800</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>1.156888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.941650</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>1.184122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>swish</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.930800</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>1.584021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tanh</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>1.233272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>softsign</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.920583</td>\n",
       "      <td>0.001511</td>\n",
       "      <td>1.468389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>elu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.918433</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>1.193996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>selu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.914817</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>1.191628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>softplus</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.914367</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>1.461108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.901400</td>\n",
       "      <td>0.002526</td>\n",
       "      <td>1.182006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>linear</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.898350</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>1.154006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hard_sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.897900</td>\n",
       "      <td>0.003756</td>\n",
       "      <td>1.402831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>softmax</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.106267</td>\n",
       "      <td>0.005967</td>\n",
       "      <td>1.539642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.942100</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>1.321403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.941233</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>1.204483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.8</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.940517</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>1.281978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.940350</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>1.236278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.6</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.940017</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>1.551555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.9</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.939150</td>\n",
       "      <td>0.001635</td>\n",
       "      <td>1.315121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.941783</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>1.444349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.940850</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>1.211069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      activation batch_size  learn_rate                 loss momentum  \\\n",
       "1    exponential        100        0.01  binary_crossentropy        0   \n",
       "4           relu        100        0.01  binary_crossentropy        0   \n",
       "10         swish        100        0.01  binary_crossentropy        0   \n",
       "11          tanh        100        0.01  binary_crossentropy        0   \n",
       "9       softsign        100        0.01  binary_crossentropy        0   \n",
       "0            elu        100        0.01  binary_crossentropy        0   \n",
       "5           selu        100        0.01  binary_crossentropy        0   \n",
       "8       softplus        100        0.01  binary_crossentropy        0   \n",
       "6        sigmoid        100        0.01  binary_crossentropy        0   \n",
       "3         linear        100        0.01  binary_crossentropy        0   \n",
       "2   hard_sigmoid        100        0.01  binary_crossentropy        0   \n",
       "7        softmax        100        0.01  binary_crossentropy        0   \n",
       "1           relu        100        0.01  binary_crossentropy      0.2   \n",
       "0           relu        100        0.01  binary_crossentropy        0   \n",
       "4           relu        100        0.01  binary_crossentropy      0.8   \n",
       "2           relu        100        0.01  binary_crossentropy      0.4   \n",
       "3           relu        100        0.01  binary_crossentropy      0.6   \n",
       "5           relu        100        0.01  binary_crossentropy      0.9   \n",
       "1           relu        100        0.10  binary_crossentropy        0   \n",
       "0           relu        100        0.01  binary_crossentropy        0   \n",
       "\n",
       "   nb_epoch optimizer  mean_test_score  std_test_score  mean_fit_time  \n",
       "1        10      adam         0.944800        0.001855       1.156888  \n",
       "4        10      adam         0.941650        0.001390       1.184122  \n",
       "10       10      adam         0.930800        0.002165       1.584021  \n",
       "11       10      adam         0.922700        0.001520       1.233272  \n",
       "9        10      adam         0.920583        0.001511       1.468389  \n",
       "0        10      adam         0.918433        0.003433       1.193996  \n",
       "5        10      adam         0.914817        0.002321       1.191628  \n",
       "8        10      adam         0.914367        0.001969       1.461108  \n",
       "6        10      adam         0.901400        0.002526       1.182006  \n",
       "3        10      adam         0.898350        0.003012       1.154006  \n",
       "2        10      adam         0.897900        0.003756       1.402831  \n",
       "7        10      adam         0.106267        0.005967       1.539642  \n",
       "1        10      adam         0.942100        0.002307       1.321403  \n",
       "0        10      adam         0.941233        0.001955       1.204483  \n",
       "4        10      adam         0.940517        0.000278       1.281978  \n",
       "2        10      adam         0.940350        0.001353       1.236278  \n",
       "3        10      adam         0.940017        0.002697       1.551555  \n",
       "5        10      adam         0.939150        0.001635       1.315121  \n",
       "1        10      adam         0.941783        0.001785       1.444349  \n",
       "0        10      adam         0.940850        0.003150       1.211069  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search parameters\n",
    "learn_rate = [0.01]\n",
    "momentum = [0.0]\n",
    "optimizers = ['SGD', 'RMSprop', 'Adam', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl']\n",
    "epochs = [10]\n",
    "batches = [100]\n",
    "activation = ['relu']\n",
    "loss = ['binary_crossentropy']\n",
    "\n",
    "param_grid = dict(\n",
    "    learn_rate=learn_rate,\n",
    "    momentum=momentum,\n",
    "    optimizer=optimizers,\n",
    "    nb_epoch=epochs,\n",
    "    batch_size=batches,\n",
    "    activation=activation,\n",
    "    loss=loss\n",
    ")\n",
    "\n",
    "#history = model.fit(x_train_norm,y_train_encoded, epochs=20,validation_split=0.20,batch_size = 10)\n",
    "grid_result = GridSearchCV(estimator=model, param_grid=param_grid, cv=3).fit(x_train_norm, y_train_encoded)\n",
    "df = print_results(grid_result)\n",
    "results_df = results_df.append(df)\n",
    "results_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "A loss function is used to optimize the parameter values in a neural network model. Loss functions map a set of parameter values for the network onto a scalar value that indicates how well those parameter accomplish the task the network is intended to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0782 - accuracy: 0.8908\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0410 - accuracy: 0.9428\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0758 - accuracy: 0.8939\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0428 - accuracy: 0.9390\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0763 - accuracy: 0.8931\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0430 - accuracy: 0.9381\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3306 - accuracy: 0.9093\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1759 - accuracy: 0.9487\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3196 - accuracy: 0.9123\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1838 - accuracy: 0.9464\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3254 - accuracy: 0.9112\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2043 - accuracy: 0.9385\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1492 - accuracy: 0.8895\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1255 - accuracy: 0.9407\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1490 - accuracy: 0.8904\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9367\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1479 - accuracy: 0.8924\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1244 - accuracy: 0.9430\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0059 - accuracy: 0.4203\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 1.3037e-05 - accuracy: 0.3235\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.5481\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 6.6637e-06 - accuracy: 0.4225\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0061 - accuracy: 0.5431\n",
      "200/200 [==============================] - 0s 1ms/step - loss: -4.1448e-07 - accuracy: 0.4072\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 0.2718 - accuracy: 0.9240\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learn_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>momentum</th>\n",
       "      <th>nb_epoch</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>exponential</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.944800</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>1.156888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.941650</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>1.184122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>swish</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.930800</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>1.584021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tanh</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>1.233272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>softsign</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.920583</td>\n",
       "      <td>0.001511</td>\n",
       "      <td>1.468389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>elu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.918433</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>1.193996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>selu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.914817</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>1.191628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>softplus</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.914367</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>1.461108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.901400</td>\n",
       "      <td>0.002526</td>\n",
       "      <td>1.182006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>linear</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.898350</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>1.154006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hard_sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.897900</td>\n",
       "      <td>0.003756</td>\n",
       "      <td>1.402831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>softmax</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.106267</td>\n",
       "      <td>0.005967</td>\n",
       "      <td>1.539642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.942100</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>1.321403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.941233</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>1.204483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.8</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.940517</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>1.281978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.940350</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>1.236278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.6</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.940017</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>1.551555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.9</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.939150</td>\n",
       "      <td>0.001635</td>\n",
       "      <td>1.315121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.941783</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>1.444349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.940850</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>1.211069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      activation batch_size  learn_rate                 loss momentum  \\\n",
       "1    exponential        100        0.01  binary_crossentropy        0   \n",
       "4           relu        100        0.01  binary_crossentropy        0   \n",
       "10         swish        100        0.01  binary_crossentropy        0   \n",
       "11          tanh        100        0.01  binary_crossentropy        0   \n",
       "9       softsign        100        0.01  binary_crossentropy        0   \n",
       "0            elu        100        0.01  binary_crossentropy        0   \n",
       "5           selu        100        0.01  binary_crossentropy        0   \n",
       "8       softplus        100        0.01  binary_crossentropy        0   \n",
       "6        sigmoid        100        0.01  binary_crossentropy        0   \n",
       "3         linear        100        0.01  binary_crossentropy        0   \n",
       "2   hard_sigmoid        100        0.01  binary_crossentropy        0   \n",
       "7        softmax        100        0.01  binary_crossentropy        0   \n",
       "1           relu        100        0.01  binary_crossentropy      0.2   \n",
       "0           relu        100        0.01  binary_crossentropy        0   \n",
       "4           relu        100        0.01  binary_crossentropy      0.8   \n",
       "2           relu        100        0.01  binary_crossentropy      0.4   \n",
       "3           relu        100        0.01  binary_crossentropy      0.6   \n",
       "5           relu        100        0.01  binary_crossentropy      0.9   \n",
       "1           relu        100        0.10  binary_crossentropy        0   \n",
       "0           relu        100        0.01  binary_crossentropy        0   \n",
       "\n",
       "   nb_epoch optimizer  mean_test_score  std_test_score  mean_fit_time  \n",
       "1        10      adam         0.944800        0.001855       1.156888  \n",
       "4        10      adam         0.941650        0.001390       1.184122  \n",
       "10       10      adam         0.930800        0.002165       1.584021  \n",
       "11       10      adam         0.922700        0.001520       1.233272  \n",
       "9        10      adam         0.920583        0.001511       1.468389  \n",
       "0        10      adam         0.918433        0.003433       1.193996  \n",
       "5        10      adam         0.914817        0.002321       1.191628  \n",
       "8        10      adam         0.914367        0.001969       1.461108  \n",
       "6        10      adam         0.901400        0.002526       1.182006  \n",
       "3        10      adam         0.898350        0.003012       1.154006  \n",
       "2        10      adam         0.897900        0.003756       1.402831  \n",
       "7        10      adam         0.106267        0.005967       1.539642  \n",
       "1        10      adam         0.942100        0.002307       1.321403  \n",
       "0        10      adam         0.941233        0.001955       1.204483  \n",
       "4        10      adam         0.940517        0.000278       1.281978  \n",
       "2        10      adam         0.940350        0.001353       1.236278  \n",
       "3        10      adam         0.940017        0.002697       1.551555  \n",
       "5        10      adam         0.939150        0.001635       1.315121  \n",
       "1        10      adam         0.941783        0.001785       1.444349  \n",
       "0        10      adam         0.940850        0.003150       1.211069  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search parameters\n",
    "learn_rate = [0.01]\n",
    "momentum = [0.0]\n",
    "optimizers = ['adam']\n",
    "epochs = [10]\n",
    "batches = [100]\n",
    "activation = ['relu']\n",
    "loss = [\n",
    "    'BinaryCrossentropy', 'CategoricalCrossentropy', #'SparseCategoricalCrossentropy',\n",
    "    'Poisson', 'KLDivergence'\n",
    "]\n",
    "\n",
    "param_grid = dict(\n",
    "    learn_rate=learn_rate,\n",
    "    momentum=momentum,\n",
    "    optimizer=optimizers,\n",
    "    nb_epoch=epochs,\n",
    "    batch_size=batches,\n",
    "    activation=activation,\n",
    "    loss=loss\n",
    ")\n",
    "\n",
    "#history = model.fit(x_train_norm,y_train_encoded, epochs=20,validation_split=0.20,batch_size = 10)\n",
    "grid_result = GridSearchCV(estimator=model, param_grid=param_grid, cv=3).fit(x_train_norm, y_train_encoded)\n",
    "df = print_results(grid_result)\n",
    "results_df = results_df.append(df)\n",
    "results_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.0423 - accuracy: 0.9363\n",
      "2000/2000 [==============================] - 2s 805us/step - loss: 0.0248 - accuracy: 0.9646\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.0426 - accuracy: 0.9369\n",
      "2000/2000 [==============================] - 2s 813us/step - loss: 0.0247 - accuracy: 0.9643\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.0428 - accuracy: 0.9359\n",
      "2000/2000 [==============================] - 2s 822us/step - loss: 0.0252 - accuracy: 0.9635\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0619 - accuracy: 0.9126\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 0.0327 - accuracy: 0.9530\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0615 - accuracy: 0.9121\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 0.0345 - accuracy: 0.9517\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0605 - accuracy: 0.9128\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 0.0334 - accuracy: 0.9530\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0783 - accuracy: 0.8910\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0416 - accuracy: 0.9421\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0783 - accuracy: 0.8949\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0441 - accuracy: 0.9366\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0764 - accuracy: 0.8913\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0402 - accuracy: 0.9433\n",
      "6000/6000 [==============================] - 8s 1ms/step - loss: 0.0361 - accuracy: 0.9467\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learn_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>momentum</th>\n",
       "      <th>nb_epoch</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>exponential</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.944800</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>1.156888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.941650</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>1.184122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>swish</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.930800</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>1.584021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tanh</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>1.233272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>softsign</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.920583</td>\n",
       "      <td>0.001511</td>\n",
       "      <td>1.468389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>elu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.918433</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>1.193996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>selu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.914817</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>1.191628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>softplus</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.914367</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>1.461108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.901400</td>\n",
       "      <td>0.002526</td>\n",
       "      <td>1.182006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>linear</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.898350</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>1.154006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hard_sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.897900</td>\n",
       "      <td>0.003756</td>\n",
       "      <td>1.402831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>softmax</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.106267</td>\n",
       "      <td>0.005967</td>\n",
       "      <td>1.539642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.942100</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>1.321403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.941233</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>1.204483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.8</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.940517</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>1.281978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.940350</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>1.236278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.6</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.940017</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>1.551555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.9</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.939150</td>\n",
       "      <td>0.001635</td>\n",
       "      <td>1.315121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.941783</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>1.444349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.940850</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>1.211069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      activation batch_size  learn_rate                 loss momentum  \\\n",
       "1    exponential        100        0.01  binary_crossentropy        0   \n",
       "4           relu        100        0.01  binary_crossentropy        0   \n",
       "10         swish        100        0.01  binary_crossentropy        0   \n",
       "11          tanh        100        0.01  binary_crossentropy        0   \n",
       "9       softsign        100        0.01  binary_crossentropy        0   \n",
       "0            elu        100        0.01  binary_crossentropy        0   \n",
       "5           selu        100        0.01  binary_crossentropy        0   \n",
       "8       softplus        100        0.01  binary_crossentropy        0   \n",
       "6        sigmoid        100        0.01  binary_crossentropy        0   \n",
       "3         linear        100        0.01  binary_crossentropy        0   \n",
       "2   hard_sigmoid        100        0.01  binary_crossentropy        0   \n",
       "7        softmax        100        0.01  binary_crossentropy        0   \n",
       "1           relu        100        0.01  binary_crossentropy      0.2   \n",
       "0           relu        100        0.01  binary_crossentropy        0   \n",
       "4           relu        100        0.01  binary_crossentropy      0.8   \n",
       "2           relu        100        0.01  binary_crossentropy      0.4   \n",
       "3           relu        100        0.01  binary_crossentropy      0.6   \n",
       "5           relu        100        0.01  binary_crossentropy      0.9   \n",
       "1           relu        100        0.10  binary_crossentropy        0   \n",
       "0           relu        100        0.01  binary_crossentropy        0   \n",
       "\n",
       "   nb_epoch optimizer  mean_test_score  std_test_score  mean_fit_time  \n",
       "1        10      adam         0.944800        0.001855       1.156888  \n",
       "4        10      adam         0.941650        0.001390       1.184122  \n",
       "10       10      adam         0.930800        0.002165       1.584021  \n",
       "11       10      adam         0.922700        0.001520       1.233272  \n",
       "9        10      adam         0.920583        0.001511       1.468389  \n",
       "0        10      adam         0.918433        0.003433       1.193996  \n",
       "5        10      adam         0.914817        0.002321       1.191628  \n",
       "8        10      adam         0.914367        0.001969       1.461108  \n",
       "6        10      adam         0.901400        0.002526       1.182006  \n",
       "3        10      adam         0.898350        0.003012       1.154006  \n",
       "2        10      adam         0.897900        0.003756       1.402831  \n",
       "7        10      adam         0.106267        0.005967       1.539642  \n",
       "1        10      adam         0.942100        0.002307       1.321403  \n",
       "0        10      adam         0.941233        0.001955       1.204483  \n",
       "4        10      adam         0.940517        0.000278       1.281978  \n",
       "2        10      adam         0.940350        0.001353       1.236278  \n",
       "3        10      adam         0.940017        0.002697       1.551555  \n",
       "5        10      adam         0.939150        0.001635       1.315121  \n",
       "1        10      adam         0.941783        0.001785       1.444349  \n",
       "0        10      adam         0.940850        0.003150       1.211069  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search parameters\n",
    "learn_rate = [0.01]\n",
    "momentum = [0.0]\n",
    "optimizers = ['adam']\n",
    "epochs = [10]\n",
    "batches = [10,50,100]\n",
    "activation = ['relu']\n",
    "loss = ['BinaryCrossentropy']\n",
    "\n",
    "param_grid = dict(\n",
    "    learn_rate=learn_rate,\n",
    "    momentum=momentum,\n",
    "    optimizer=optimizers,\n",
    "    nb_epoch=epochs,\n",
    "    batch_size=batches,\n",
    "    activation=activation,\n",
    "    loss=loss\n",
    ")\n",
    "\n",
    "#history = model.fit(x_train_norm,y_train_encoded, epochs=20,validation_split=0.20,batch_size = 10)\n",
    "grid_result = GridSearchCV(estimator=model, param_grid=param_grid, cv=3).fit(x_train_norm, y_train_encoded)\n",
    "df = print_results(grid_result)\n",
    "results_df = results_df.append(df)\n",
    "results_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0665 - accuracy: 0.9060\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0349 - accuracy: 0.9476\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0647 - accuracy: 0.9087\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0360 - accuracy: 0.9489\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0666 - accuracy: 0.9056\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0338 - accuracy: 0.9517\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0800 - accuracy: 0.8887\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0421 - accuracy: 0.9417\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0763 - accuracy: 0.8931\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0425 - accuracy: 0.9395\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0791 - accuracy: 0.8867\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0429 - accuracy: 0.9374\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0739 - accuracy: 0.8985\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0392 - accuracy: 0.9438\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0747 - accuracy: 0.8993\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0415 - accuracy: 0.9409\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0748 - accuracy: 0.8986\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0388 - accuracy: 0.9458\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3274 - accuracy: 0.9079\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2195 - accuracy: 0.9328\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3159 - accuracy: 0.9107\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1947 - accuracy: 0.9456\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3230 - accuracy: 0.9075\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1828 - accuracy: 0.9469\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3248 - accuracy: 0.9100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1704 - accuracy: 0.9513\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3211 - accuracy: 0.9123\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2130 - accuracy: 0.9369\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3261 - accuracy: 0.9102\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1805 - accuracy: 0.9464\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3321 - accuracy: 0.9098\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1897 - accuracy: 0.9431\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3280 - accuracy: 0.9102\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1863 - accuracy: 0.9456\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3279 - accuracy: 0.9116\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1671 - accuracy: 0.9527\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1416 - accuracy: 0.9052\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9470\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1409 - accuracy: 0.9075\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.9434\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1408 - accuracy: 0.9066\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.9491\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1481 - accuracy: 0.8914\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1251 - accuracy: 0.9395\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1475 - accuracy: 0.8960\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1252 - accuracy: 0.9403\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1491 - accuracy: 0.8892\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1247 - accuracy: 0.9432\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1476 - accuracy: 0.8966\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.9434\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1470 - accuracy: 0.8994\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1250 - accuracy: 0.9415\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1458 - accuracy: 0.8998\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1239 - accuracy: 0.9414\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0665 - accuracy: 0.9047\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0356 - accuracy: 0.9464\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0667 - accuracy: 0.9081\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0357 - accuracy: 0.9480\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0665 - accuracy: 0.9068\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0354 - accuracy: 0.9496\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0763 - accuracy: 0.8940\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0408 - accuracy: 0.9421\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0769 - accuracy: 0.8932\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0460 - accuracy: 0.9336\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0777 - accuracy: 0.8914\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0406 - accuracy: 0.9414\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0762 - accuracy: 0.8957\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0401 - accuracy: 0.9420\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0751 - accuracy: 0.8985\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0412 - accuracy: 0.9427\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0773 - accuracy: 0.8967\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0396 - accuracy: 0.9460\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3234 - accuracy: 0.9061\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1775 - accuracy: 0.9484\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3227 - accuracy: 0.9089\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1884 - accuracy: 0.9453\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3173 - accuracy: 0.9105\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2295 - accuracy: 0.9262\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3260 - accuracy: 0.9097\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1751 - accuracy: 0.9481\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3226 - accuracy: 0.9133\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1854 - accuracy: 0.9460\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3277 - accuracy: 0.9101\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1749 - accuracy: 0.9505\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3276 - accuracy: 0.9119\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1713 - accuracy: 0.9508\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3275 - accuracy: 0.9129\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1845 - accuracy: 0.9463\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3300 - accuracy: 0.9112\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1748 - accuracy: 0.9505\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1416 - accuracy: 0.9051\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1217 - accuracy: 0.9461\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1410 - accuracy: 0.9055\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9467\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1411 - accuracy: 0.9061\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1211 - accuracy: 0.9503\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1482 - accuracy: 0.8913\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9402\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1494 - accuracy: 0.8914\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1259 - accuracy: 0.9376\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1484 - accuracy: 0.8928\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1247 - accuracy: 0.9433\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1476 - accuracy: 0.8995\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1241 - accuracy: 0.9448\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1467 - accuracy: 0.8992\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1243 - accuracy: 0.9431\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.1471 - accuracy: 0.8978\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1239 - accuracy: 0.9448\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0668 - accuracy: 0.9047\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0367 - accuracy: 0.9440\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0669 - accuracy: 0.9075\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0367 - accuracy: 0.9459\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0681 - accuracy: 0.9055\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0344 - accuracy: 0.9520\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0786 - accuracy: 0.8886\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0417 - accuracy: 0.9413\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0766 - accuracy: 0.8924\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0426 - accuracy: 0.9403\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0763 - accuracy: 0.8933\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0409 - accuracy: 0.9427\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0760 - accuracy: 0.8971\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0394 - accuracy: 0.9444\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0753 - accuracy: 0.8976\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0422 - accuracy: 0.9411\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0744 - accuracy: 0.8986\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0392 - accuracy: 0.9451\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3164 - accuracy: 0.9096\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1652 - accuracy: 0.9506\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3204 - accuracy: 0.9093\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1821 - accuracy: 0.9473\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3282 - accuracy: 0.9063\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1810 - accuracy: 0.9466\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3288 - accuracy: 0.9096\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1763 - accuracy: 0.9484\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3259 - accuracy: 0.9110\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1923 - accuracy: 0.9456\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3288 - accuracy: 0.9096\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1853 - accuracy: 0.9456\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3232 - accuracy: 0.9134\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1671 - accuracy: 0.9516\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3246 - accuracy: 0.9113\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1831 - accuracy: 0.9473\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3292 - accuracy: 0.9106\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1805 - accuracy: 0.9488\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1415 - accuracy: 0.9040\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.9446\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1409 - accuracy: 0.9066\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1220 - accuracy: 0.9460\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1417 - accuracy: 0.9056\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1221 - accuracy: 0.9467\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1474 - accuracy: 0.8911\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1247 - accuracy: 0.9410\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1482 - accuracy: 0.8937\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1257 - accuracy: 0.9400\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1494 - accuracy: 0.8888\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1263 - accuracy: 0.9367\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1468 - accuracy: 0.8986\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1235 - accuracy: 0.9445\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1457 - accuracy: 0.9017\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1242 - accuracy: 0.9420\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1473 - accuracy: 0.8964\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1239 - accuracy: 0.9437\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0670 - accuracy: 0.9054\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0373 - accuracy: 0.9455\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0669 - accuracy: 0.9067\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0391 - accuracy: 0.9437\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0662 - accuracy: 0.9069\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0358 - accuracy: 0.9489\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0795 - accuracy: 0.8887\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0425 - accuracy: 0.9394\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0790 - accuracy: 0.8904\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0431 - accuracy: 0.9388\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0799 - accuracy: 0.8902\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0426 - accuracy: 0.9420\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0752 - accuracy: 0.8969\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0395 - accuracy: 0.9447\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0763 - accuracy: 0.8963\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0424 - accuracy: 0.9410\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0744 - accuracy: 0.8986\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0391 - accuracy: 0.9462\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3207 - accuracy: 0.9069\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1942 - accuracy: 0.9427\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3216 - accuracy: 0.9100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1818 - accuracy: 0.9471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3215 - accuracy: 0.9077\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1876 - accuracy: 0.9463\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3288 - accuracy: 0.9091\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1784 - accuracy: 0.9460\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3214 - accuracy: 0.9119\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1810 - accuracy: 0.9464\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3360 - accuracy: 0.9096\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1783 - accuracy: 0.9492\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3330 - accuracy: 0.9097\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1809 - accuracy: 0.9453\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3239 - accuracy: 0.9145\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1887 - accuracy: 0.9445\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3268 - accuracy: 0.9127\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1713 - accuracy: 0.9507\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1408 - accuracy: 0.9057\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9481\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1413 - accuracy: 0.9048\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1222 - accuracy: 0.9452\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1415 - accuracy: 0.9061\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1209 - accuracy: 0.9492\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1496 - accuracy: 0.8899\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1262 - accuracy: 0.9377\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1489 - accuracy: 0.8941\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1271 - accuracy: 0.9341\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1494 - accuracy: 0.8909\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1254 - accuracy: 0.9395\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1473 - accuracy: 0.8958\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1239 - accuracy: 0.9448\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1464 - accuracy: 0.9000\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1243 - accuracy: 0.9414\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1474 - accuracy: 0.8954\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.9455\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0738 - accuracy: 0.8919\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0446 - accuracy: 0.9369\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0731 - accuracy: 0.8951\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0432 - accuracy: 0.9417\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0760 - accuracy: 0.8899\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0416 - accuracy: 0.9427\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0774 - accuracy: 0.8884\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0434 - accuracy: 0.9417\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0787 - accuracy: 0.8865\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0434 - accuracy: 0.9428\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0742 - accuracy: 0.8942\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0413 - accuracy: 0.9457\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0758 - accuracy: 0.8935\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0424 - accuracy: 0.9445\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0726 - accuracy: 0.8980\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0427 - accuracy: 0.9434\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0775 - accuracy: 0.8933\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0441 - accuracy: 0.9445\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.4042 - accuracy: 0.8693\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2038 - accuracy: 0.9422\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3669 - accuracy: 0.8887\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2281 - accuracy: 0.9343\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3838 - accuracy: 0.8862\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2180 - accuracy: 0.9372\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3308 - accuracy: 0.9008\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1867 - accuracy: 0.9467\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3284 - accuracy: 0.9045\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2002 - accuracy: 0.9420\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3364 - accuracy: 0.9017\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1950 - accuracy: 0.9462\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3494 - accuracy: 0.8953\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2048 - accuracy: 0.9414\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3342 - accuracy: 0.9052\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2199 - accuracy: 0.9360\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3419 - accuracy: 0.9013\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2082 - accuracy: 0.9399\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1449 - accuracy: 0.8906\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1247 - accuracy: 0.9388\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1450 - accuracy: 0.8921\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1266 - accuracy: 0.9351\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1452 - accuracy: 0.8929\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1247 - accuracy: 0.9398\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1443 - accuracy: 0.8955\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1260 - accuracy: 0.9388\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1456 - accuracy: 0.8895\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1267 - accuracy: 0.9360\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1434 - accuracy: 0.8945\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1248 - accuracy: 0.9430\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1436 - accuracy: 0.8992\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1248 - accuracy: 0.9413\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1440 - accuracy: 0.8988\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1253 - accuracy: 0.9405\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1455 - accuracy: 0.8954\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1242 - accuracy: 0.9452\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0747 - accuracy: 0.8915\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0460 - accuracy: 0.9352\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0756 - accuracy: 0.8917\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0456 - accuracy: 0.9379\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0731 - accuracy: 0.8931\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0455 - accuracy: 0.9370\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0776 - accuracy: 0.8876\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0430 - accuracy: 0.9417\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0724 - accuracy: 0.8917\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0428 - accuracy: 0.9429\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0770 - accuracy: 0.8877\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0414 - accuracy: 0.9452\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0747 - accuracy: 0.8960\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0423 - accuracy: 0.9419\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0718 - accuracy: 0.8997\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0449 - accuracy: 0.9369\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0773 - accuracy: 0.8960\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0421 - accuracy: 0.9456\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3827 - accuracy: 0.8849\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2211 - accuracy: 0.9312\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3742 - accuracy: 0.8912\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2469 - accuracy: 0.9291\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3816 - accuracy: 0.8855\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1954 - accuracy: 0.9463\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3489 - accuracy: 0.8884\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2020 - accuracy: 0.9412\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3507 - accuracy: 0.8893\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2120 - accuracy: 0.9385\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3650 - accuracy: 0.8852\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2182 - accuracy: 0.9377\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3433 - accuracy: 0.8996\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2338 - accuracy: 0.9284\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3346 - accuracy: 0.9026\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2137 - accuracy: 0.9410\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3379 - accuracy: 0.9027\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2175 - accuracy: 0.9379\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1450 - accuracy: 0.8924\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1232 - accuracy: 0.9435\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1442 - accuracy: 0.8943\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1270 - accuracy: 0.9354\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1457 - accuracy: 0.8888\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1279 - accuracy: 0.9336\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1454 - accuracy: 0.8948\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1260 - accuracy: 0.9398\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1419 - accuracy: 0.9014\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9450\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1449 - accuracy: 0.8927\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1273 - accuracy: 0.9373\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1444 - accuracy: 0.8973\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1263 - accuracy: 0.9359\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1428 - accuracy: 0.8995\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1245 - accuracy: 0.9428\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1436 - accuracy: 0.9004\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.9475\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0750 - accuracy: 0.8912\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0487 - accuracy: 0.9335\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0723 - accuracy: 0.8948\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0466 - accuracy: 0.9386\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0752 - accuracy: 0.8909\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0552 - accuracy: 0.9309\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0744 - accuracy: 0.8903\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0438 - accuracy: 0.9409\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0732 - accuracy: 0.8935\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0453 - accuracy: 0.9383\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0737 - accuracy: 0.8925\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0424 - accuracy: 0.9424\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0725 - accuracy: 0.8979\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0417 - accuracy: 0.9449\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0739 - accuracy: 0.8971\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0429 - accuracy: 0.9432\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0697 - accuracy: 0.8999\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0398 - accuracy: 0.9483\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3756 - accuracy: 0.8855\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2215 - accuracy: 0.9356\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.3947 - accuracy: 0.8700\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2176 - accuracy: 0.9395\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3812 - accuracy: 0.8849\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1971 - accuracy: 0.9443\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3466 - accuracy: 0.8894\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2098 - accuracy: 0.9398\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3340 - accuracy: 0.9022\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2083 - accuracy: 0.9410\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3505 - accuracy: 0.8901\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1932 - accuracy: 0.9457\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3445 - accuracy: 0.8997\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1958 - accuracy: 0.9451\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.3375 - accuracy: 0.9050\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2043 - accuracy: 0.9425\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3440 - accuracy: 0.8998\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2117 - accuracy: 0.9402\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1457 - accuracy: 0.8909\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1284 - accuracy: 0.9310\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1451 - accuracy: 0.8918\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1273 - accuracy: 0.9320\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1456 - accuracy: 0.8905\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1270 - accuracy: 0.9355\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1469 - accuracy: 0.8924\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1275 - accuracy: 0.9394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1437 - accuracy: 0.8979\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1269 - accuracy: 0.9406\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1492 - accuracy: 0.8848\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9423\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1417 - accuracy: 0.9023\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1254 - accuracy: 0.9434\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1428 - accuracy: 0.9011\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1295 - accuracy: 0.9317\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1449 - accuracy: 0.8947\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1239 - accuracy: 0.9455\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0757 - accuracy: 0.8905\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0405 - accuracy: 0.9455\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0744 - accuracy: 0.8931\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0445 - accuracy: 0.9416\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0757 - accuracy: 0.8918\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0402 - accuracy: 0.9441\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0792 - accuracy: 0.8850\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0460 - accuracy: 0.9394\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0725 - accuracy: 0.8972\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0448 - accuracy: 0.9401\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0758 - accuracy: 0.8887\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0424 - accuracy: 0.9459\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0734 - accuracy: 0.8978\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0411 - accuracy: 0.9466\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0748 - accuracy: 0.8977\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0434 - accuracy: 0.9417\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0713 - accuracy: 0.9001\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0388 - accuracy: 0.9492\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3751 - accuracy: 0.8847\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2141 - accuracy: 0.9388\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3915 - accuracy: 0.8770\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2224 - accuracy: 0.9376\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3807 - accuracy: 0.8855\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1987 - accuracy: 0.9448\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3507 - accuracy: 0.8946\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1898 - accuracy: 0.9463\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3364 - accuracy: 0.9027\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2353 - accuracy: 0.9328\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3516 - accuracy: 0.8849\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2035 - accuracy: 0.9415\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3432 - accuracy: 0.9007\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1945 - accuracy: 0.9445\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3345 - accuracy: 0.9028\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2015 - accuracy: 0.9426\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3459 - accuracy: 0.9004\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1990 - accuracy: 0.9438\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1459 - accuracy: 0.8892\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1248 - accuracy: 0.9388\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1456 - accuracy: 0.8933\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1258 - accuracy: 0.9395\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1452 - accuracy: 0.8929\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9391\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1463 - accuracy: 0.8894\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1273 - accuracy: 0.9377\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1432 - accuracy: 0.8993\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1257 - accuracy: 0.9417\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1473 - accuracy: 0.8903\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1269 - accuracy: 0.9420\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1442 - accuracy: 0.8988\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1255 - accuracy: 0.9410\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1427 - accuracy: 0.9021\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1252 - accuracy: 0.9439\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1426 - accuracy: 0.8996\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9446\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0779 - accuracy: 0.8935\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0468 - accuracy: 0.9344\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0770 - accuracy: 0.8944\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0471 - accuracy: 0.9343\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0778 - accuracy: 0.8932\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0467 - accuracy: 0.9348\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0841 - accuracy: 0.8810\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0491 - accuracy: 0.9326\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0839 - accuracy: 0.8837\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0521 - accuracy: 0.9271\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0873 - accuracy: 0.8808\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0497 - accuracy: 0.9331\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0864 - accuracy: 0.8826\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0499 - accuracy: 0.9317\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.0856 - accuracy: 0.8875\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0524 - accuracy: 0.9280\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0866 - accuracy: 0.8867\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0499 - accuracy: 0.9309\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3669 - accuracy: 0.8960\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2069 - accuracy: 0.9396\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3615 - accuracy: 0.8983\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2433 - accuracy: 0.9287\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3619 - accuracy: 0.8971\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2271 - accuracy: 0.9324\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3732 - accuracy: 0.8939\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2228 - accuracy: 0.9345\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3570 - accuracy: 0.9006\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2134 - accuracy: 0.9373\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3608 - accuracy: 0.9006\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2020 - accuracy: 0.9430\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3685 - accuracy: 0.8996\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2176 - accuracy: 0.9362\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3630 - accuracy: 0.9018\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2331 - accuracy: 0.9344\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3675 - accuracy: 0.9017\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2306 - accuracy: 0.9279\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1475 - accuracy: 0.8930\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1272 - accuracy: 0.9360\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1475 - accuracy: 0.8943\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1275 - accuracy: 0.9356\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1485 - accuracy: 0.8903\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1278 - accuracy: 0.9371\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1531 - accuracy: 0.8795\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1302 - accuracy: 0.9300\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1524 - accuracy: 0.8848\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1307 - accuracy: 0.9280\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1535 - accuracy: 0.8813\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1299 - accuracy: 0.9322\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1542 - accuracy: 0.8826\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1308 - accuracy: 0.9310\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1536 - accuracy: 0.8860\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1317 - accuracy: 0.9293\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1537 - accuracy: 0.8814\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1311 - accuracy: 0.9297\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0777 - accuracy: 0.8927\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0450 - accuracy: 0.9366\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0765 - accuracy: 0.8938\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0461 - accuracy: 0.9372\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0770 - accuracy: 0.8949\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0433 - accuracy: 0.9395\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0861 - accuracy: 0.8811\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0515 - accuracy: 0.9273\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0855 - accuracy: 0.8834\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0514 - accuracy: 0.9294\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0856 - accuracy: 0.8810\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0503 - accuracy: 0.9315\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0872 - accuracy: 0.8821\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0510 - accuracy: 0.9306\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0868 - accuracy: 0.8821\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0541 - accuracy: 0.9256\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0880 - accuracy: 0.8813\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0510 - accuracy: 0.9316\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3635 - accuracy: 0.8965\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2069 - accuracy: 0.9401\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3655 - accuracy: 0.8974\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2172 - accuracy: 0.9380\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3658 - accuracy: 0.8946\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2380 - accuracy: 0.9316\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3681 - accuracy: 0.8972\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2291 - accuracy: 0.9316\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3527 - accuracy: 0.9023\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2085 - accuracy: 0.9413\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3605 - accuracy: 0.9007\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1936 - accuracy: 0.9463\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3752 - accuracy: 0.8992\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2158 - accuracy: 0.9356\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3642 - accuracy: 0.9042\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2202 - accuracy: 0.9360\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3683 - accuracy: 0.9028\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2108 - accuracy: 0.9399\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1481 - accuracy: 0.8918\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1272 - accuracy: 0.9359\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1475 - accuracy: 0.8936\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.9327\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1480 - accuracy: 0.8924\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1282 - accuracy: 0.9330\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1527 - accuracy: 0.8832\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1300 - accuracy: 0.9313\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1535 - accuracy: 0.8819\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1314 - accuracy: 0.9275\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1535 - accuracy: 0.8804\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1303 - accuracy: 0.9304\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1538 - accuracy: 0.8851\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1303 - accuracy: 0.9305\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1539 - accuracy: 0.8844\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1311 - accuracy: 0.9304\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1538 - accuracy: 0.8851\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1305 - accuracy: 0.9309\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0781 - accuracy: 0.8921\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0476 - accuracy: 0.9333\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0762 - accuracy: 0.8971\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0459 - accuracy: 0.9356\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0777 - accuracy: 0.8935\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0440 - accuracy: 0.9396\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0871 - accuracy: 0.8781\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0500 - accuracy: 0.9317\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0833 - accuracy: 0.8866\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0516 - accuracy: 0.9311\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0865 - accuracy: 0.8791\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0498 - accuracy: 0.9326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0871 - accuracy: 0.8823\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0528 - accuracy: 0.9263\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0864 - accuracy: 0.8845\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0522 - accuracy: 0.9283\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0872 - accuracy: 0.8820\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0499 - accuracy: 0.9338\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3639 - accuracy: 0.8978\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2177 - accuracy: 0.9383\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3598 - accuracy: 0.8987\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2424 - accuracy: 0.9271\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3636 - accuracy: 0.8973\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2137 - accuracy: 0.9362\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3617 - accuracy: 0.9027\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2039 - accuracy: 0.9402\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3486 - accuracy: 0.9060\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2107 - accuracy: 0.9410\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3587 - accuracy: 0.9037\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2067 - accuracy: 0.9404\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.3646 - accuracy: 0.8996\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2058 - accuracy: 0.9406\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3676 - accuracy: 0.9016\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2221 - accuracy: 0.9359\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3703 - accuracy: 0.9011\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2122 - accuracy: 0.9403\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1478 - accuracy: 0.8925\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1274 - accuracy: 0.9355\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1481 - accuracy: 0.8927\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1281 - accuracy: 0.9348\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1481 - accuracy: 0.8910\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9342\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1541 - accuracy: 0.8784\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1307 - accuracy: 0.9285\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1530 - accuracy: 0.8840\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1318 - accuracy: 0.9258\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1536 - accuracy: 0.8790\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1307 - accuracy: 0.9295\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1533 - accuracy: 0.8849\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1302 - accuracy: 0.9320\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1536 - accuracy: 0.8842\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1321 - accuracy: 0.9248\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1536 - accuracy: 0.8855\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.9305\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0766 - accuracy: 0.8935\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0476 - accuracy: 0.9309\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0762 - accuracy: 0.8947\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0472 - accuracy: 0.9340\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0777 - accuracy: 0.8923\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0438 - accuracy: 0.9416\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0867 - accuracy: 0.8781\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0498 - accuracy: 0.9313\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0852 - accuracy: 0.8800\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0523 - accuracy: 0.9271\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0863 - accuracy: 0.8790\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0491 - accuracy: 0.9336\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0853 - accuracy: 0.8850\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0505 - accuracy: 0.9298\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0856 - accuracy: 0.8848\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0521 - accuracy: 0.9300\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0855 - accuracy: 0.8853\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0486 - accuracy: 0.9347\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3628 - accuracy: 0.8981\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1990 - accuracy: 0.9417\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3587 - accuracy: 0.8984\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2211 - accuracy: 0.9370\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3658 - accuracy: 0.8974\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2024 - accuracy: 0.9431\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3661 - accuracy: 0.9000\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2007 - accuracy: 0.9427\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3668 - accuracy: 0.9013\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2284 - accuracy: 0.9359\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3638 - accuracy: 0.8984\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1974 - accuracy: 0.9456\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3759 - accuracy: 0.8988\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2084 - accuracy: 0.9402\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3638 - accuracy: 0.9028\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2255 - accuracy: 0.9362\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.3702 - accuracy: 0.9008\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1986 - accuracy: 0.9430\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1478 - accuracy: 0.8931\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1265 - accuracy: 0.9377\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1477 - accuracy: 0.8954\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1281 - accuracy: 0.9320\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1479 - accuracy: 0.8923\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9381\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1541 - accuracy: 0.8798\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1312 - accuracy: 0.9277\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1537 - accuracy: 0.8816\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1311 - accuracy: 0.9268\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1530 - accuracy: 0.8812\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1302 - accuracy: 0.9334\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1533 - accuracy: 0.8840\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1308 - accuracy: 0.9285\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1539 - accuracy: 0.8850\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1323 - accuracy: 0.9248\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1540 - accuracy: 0.8835\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1303 - accuracy: 0.9316\n",
      "600/600 [==============================] - 2s 3ms/step - loss: 0.0547 - accuracy: 0.9216\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learn_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>momentum</th>\n",
       "      <th>nb_epoch</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>exponential</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.944800</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>1.156888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.941650</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>1.184122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>swish</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.930800</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>1.584021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tanh</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>1.233272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>softsign</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.920583</td>\n",
       "      <td>0.001511</td>\n",
       "      <td>1.468389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>elu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.918433</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>1.193996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>selu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.914817</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>1.191628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>softplus</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.914367</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>1.461108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.901400</td>\n",
       "      <td>0.002526</td>\n",
       "      <td>1.182006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>linear</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.898350</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>1.154006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hard_sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.897900</td>\n",
       "      <td>0.003756</td>\n",
       "      <td>1.402831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>softmax</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.106267</td>\n",
       "      <td>0.005967</td>\n",
       "      <td>1.539642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.942100</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>1.321403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.941233</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>1.204483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.8</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.940517</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>1.281978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.940350</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>1.236278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.6</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.940017</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>1.551555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.9</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.939150</td>\n",
       "      <td>0.001635</td>\n",
       "      <td>1.315121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.941783</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>1.444349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.940850</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>1.211069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      activation batch_size  learn_rate                 loss momentum  \\\n",
       "1    exponential        100        0.01  binary_crossentropy        0   \n",
       "4           relu        100        0.01  binary_crossentropy        0   \n",
       "10         swish        100        0.01  binary_crossentropy        0   \n",
       "11          tanh        100        0.01  binary_crossentropy        0   \n",
       "9       softsign        100        0.01  binary_crossentropy        0   \n",
       "0            elu        100        0.01  binary_crossentropy        0   \n",
       "5           selu        100        0.01  binary_crossentropy        0   \n",
       "8       softplus        100        0.01  binary_crossentropy        0   \n",
       "6        sigmoid        100        0.01  binary_crossentropy        0   \n",
       "3         linear        100        0.01  binary_crossentropy        0   \n",
       "2   hard_sigmoid        100        0.01  binary_crossentropy        0   \n",
       "7        softmax        100        0.01  binary_crossentropy        0   \n",
       "1           relu        100        0.01  binary_crossentropy      0.2   \n",
       "0           relu        100        0.01  binary_crossentropy        0   \n",
       "4           relu        100        0.01  binary_crossentropy      0.8   \n",
       "2           relu        100        0.01  binary_crossentropy      0.4   \n",
       "3           relu        100        0.01  binary_crossentropy      0.6   \n",
       "5           relu        100        0.01  binary_crossentropy      0.9   \n",
       "1           relu        100        0.10  binary_crossentropy        0   \n",
       "0           relu        100        0.01  binary_crossentropy        0   \n",
       "\n",
       "   nb_epoch optimizer  mean_test_score  std_test_score  mean_fit_time  \n",
       "1        10      adam         0.944800        0.001855       1.156888  \n",
       "4        10      adam         0.941650        0.001390       1.184122  \n",
       "10       10      adam         0.930800        0.002165       1.584021  \n",
       "11       10      adam         0.922700        0.001520       1.233272  \n",
       "9        10      adam         0.920583        0.001511       1.468389  \n",
       "0        10      adam         0.918433        0.003433       1.193996  \n",
       "5        10      adam         0.914817        0.002321       1.191628  \n",
       "8        10      adam         0.914367        0.001969       1.461108  \n",
       "6        10      adam         0.901400        0.002526       1.182006  \n",
       "3        10      adam         0.898350        0.003012       1.154006  \n",
       "2        10      adam         0.897900        0.003756       1.402831  \n",
       "7        10      adam         0.106267        0.005967       1.539642  \n",
       "1        10      adam         0.942100        0.002307       1.321403  \n",
       "0        10      adam         0.941233        0.001955       1.204483  \n",
       "4        10      adam         0.940517        0.000278       1.281978  \n",
       "2        10      adam         0.940350        0.001353       1.236278  \n",
       "3        10      adam         0.940017        0.002697       1.551555  \n",
       "5        10      adam         0.939150        0.001635       1.315121  \n",
       "1        10      adam         0.941783        0.001785       1.444349  \n",
       "0        10      adam         0.940850        0.003150       1.211069  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search parameters\n",
    "learn_rate = [\n",
    "    0.01, 0.1, 0.5, 1.0,# 1.5, 2.0\n",
    "]\n",
    "momentum = [\n",
    "    0.0,# 0.2, 0.4, 0.6, 0.8, 0.9\n",
    "]\n",
    "optimizers = [\n",
    "    'RMSprop', 'Adam', 'Nadam',\n",
    "]\n",
    "epochs = [10]\n",
    "batches = [100]\n",
    "activation = [\n",
    "    'relu', 'exponential', 'swish', #'tanh',\n",
    "]\n",
    "loss = [\n",
    "    'BinaryCrossentropy', 'CategoricalCrossentropy', 'Poisson'\n",
    "]\n",
    "\n",
    "param_grid = dict(\n",
    "    learn_rate=learn_rate,\n",
    "    momentum=momentum,\n",
    "    optimizer=optimizers,\n",
    "    nb_epoch=epochs,\n",
    "    batch_size=batches,\n",
    "    activation=activation,\n",
    "    loss=loss\n",
    ")\n",
    "\n",
    "#history = model.fit(x_train_norm,y_train_encoded, epochs=20,validation_split=0.20,batch_size = 10)\n",
    "grid_result = GridSearchCV(estimator=model, param_grid=param_grid, cv=3).fit(x_train_norm, y_train_encoded)\n",
    "df = print_results(grid_result)\n",
    "pd.set_option('Display.max_rows',None)\n",
    "results_df = results_df.append(df)\n",
    "results_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learn_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>momentum</th>\n",
       "      <th>nb_epoch</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>exponential</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.944800</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>1.156888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.941650</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>1.184122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>swish</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.930800</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>1.584021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tanh</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>1.233272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>softsign</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.920583</td>\n",
       "      <td>0.001511</td>\n",
       "      <td>1.468389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    activation  batch_size  learn_rate                 loss  momentum  \\\n",
       "0  exponential         100        0.01  binary_crossentropy       0.0   \n",
       "1         relu         100        0.01  binary_crossentropy       0.0   \n",
       "2        swish         100        0.01  binary_crossentropy       0.0   \n",
       "3         tanh         100        0.01  binary_crossentropy       0.0   \n",
       "4     softsign         100        0.01  binary_crossentropy       0.0   \n",
       "\n",
       "   nb_epoch optimizer  mean_test_score  std_test_score  mean_fit_time  \n",
       "0        10      adam         0.944800        0.001855       1.156888  \n",
       "1        10      adam         0.941650        0.001390       1.184122  \n",
       "2        10      adam         0.930800        0.002165       1.584021  \n",
       "3        10      adam         0.922700        0.001520       1.233272  \n",
       "4        10      adam         0.920583        0.001511       1.468389  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.to_csv('results.csv',index=False)\n",
    "results_df_read = pd.read_csv('results.csv')\n",
    "results_df_read.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learn_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>momentum</th>\n",
       "      <th>nb_epoch</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>mean_fit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>exponential</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.944800</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>1.156888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.941650</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>1.184122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>swish</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.930800</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>1.584021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tanh</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>1.233272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>softsign</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.920583</td>\n",
       "      <td>0.001511</td>\n",
       "      <td>1.468389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>elu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.918433</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>1.193996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>selu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.914817</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>1.191628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>softplus</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.914367</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>1.461108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.901400</td>\n",
       "      <td>0.002526</td>\n",
       "      <td>1.182006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>linear</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.898350</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>1.154006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hard_sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.897900</td>\n",
       "      <td>0.003756</td>\n",
       "      <td>1.402831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>softmax</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.106267</td>\n",
       "      <td>0.005967</td>\n",
       "      <td>1.539642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.942100</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>1.321403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.941233</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>1.204483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.8</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.940517</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>1.281978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.940350</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>1.236278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.6</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.940017</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>1.551555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.9</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.939150</td>\n",
       "      <td>0.001635</td>\n",
       "      <td>1.315121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.941783</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>1.444349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.940850</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>1.211069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      activation  batch_size  learn_rate                 loss  momentum  \\\n",
       "0    exponential         100        0.01  binary_crossentropy       0.0   \n",
       "1           relu         100        0.01  binary_crossentropy       0.0   \n",
       "2          swish         100        0.01  binary_crossentropy       0.0   \n",
       "3           tanh         100        0.01  binary_crossentropy       0.0   \n",
       "4       softsign         100        0.01  binary_crossentropy       0.0   \n",
       "5            elu         100        0.01  binary_crossentropy       0.0   \n",
       "6           selu         100        0.01  binary_crossentropy       0.0   \n",
       "7       softplus         100        0.01  binary_crossentropy       0.0   \n",
       "8        sigmoid         100        0.01  binary_crossentropy       0.0   \n",
       "9         linear         100        0.01  binary_crossentropy       0.0   \n",
       "10  hard_sigmoid         100        0.01  binary_crossentropy       0.0   \n",
       "11       softmax         100        0.01  binary_crossentropy       0.0   \n",
       "12          relu         100        0.01  binary_crossentropy       0.2   \n",
       "13          relu         100        0.01  binary_crossentropy       0.0   \n",
       "14          relu         100        0.01  binary_crossentropy       0.8   \n",
       "15          relu         100        0.01  binary_crossentropy       0.4   \n",
       "16          relu         100        0.01  binary_crossentropy       0.6   \n",
       "17          relu         100        0.01  binary_crossentropy       0.9   \n",
       "18          relu         100        0.10  binary_crossentropy       0.0   \n",
       "19          relu         100        0.01  binary_crossentropy       0.0   \n",
       "\n",
       "    nb_epoch optimizer  mean_test_score  std_test_score  mean_fit_time  \n",
       "0         10      adam         0.944800        0.001855       1.156888  \n",
       "1         10      adam         0.941650        0.001390       1.184122  \n",
       "2         10      adam         0.930800        0.002165       1.584021  \n",
       "3         10      adam         0.922700        0.001520       1.233272  \n",
       "4         10      adam         0.920583        0.001511       1.468389  \n",
       "5         10      adam         0.918433        0.003433       1.193996  \n",
       "6         10      adam         0.914817        0.002321       1.191628  \n",
       "7         10      adam         0.914367        0.001969       1.461108  \n",
       "8         10      adam         0.901400        0.002526       1.182006  \n",
       "9         10      adam         0.898350        0.003012       1.154006  \n",
       "10        10      adam         0.897900        0.003756       1.402831  \n",
       "11        10      adam         0.106267        0.005967       1.539642  \n",
       "12        10      adam         0.942100        0.002307       1.321403  \n",
       "13        10      adam         0.941233        0.001955       1.204483  \n",
       "14        10      adam         0.940517        0.000278       1.281978  \n",
       "15        10      adam         0.940350        0.001353       1.236278  \n",
       "16        10      adam         0.940017        0.002697       1.551555  \n",
       "17        10      adam         0.939150        0.001635       1.315121  \n",
       "18        10      adam         0.941783        0.001785       1.444349  \n",
       "19        10      adam         0.940850        0.003150       1.211069  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_read.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3304 - accuracy: 0.9086\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1761 - accuracy: 0.9473\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3231 - accuracy: 0.9114\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1803 - accuracy: 0.9487\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3269 - accuracy: 0.9110\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1716 - accuracy: 0.9523\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3329 - accuracy: 0.9094\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1696 - accuracy: 0.9506\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.3249 - accuracy: 0.9122\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1784 - accuracy: 0.9477\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3305 - accuracy: 0.9109\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1780 - accuracy: 0.9492\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3247 - accuracy: 0.9124\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1773 - accuracy: 0.9476\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3203 - accuracy: 0.9122\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1947 - accuracy: 0.9417\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3295 - accuracy: 0.9097\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1731 - accuracy: 0.9500\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3248 - accuracy: 0.9124\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1726 - accuracy: 0.9505\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.3169 - accuracy: 0.9155\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1801 - accuracy: 0.9495\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3200 - accuracy: 0.9133\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1709 - accuracy: 0.9521\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3227 - accuracy: 0.9100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1790 - accuracy: 0.9471\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3265 - accuracy: 0.9122\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1847 - accuracy: 0.9469\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3247 - accuracy: 0.9098\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1849 - accuracy: 0.9467\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3265 - accuracy: 0.9122\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1780 - accuracy: 0.9495\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3274 - accuracy: 0.9118\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1856 - accuracy: 0.9458\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3261 - accuracy: 0.9111\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1728 - accuracy: 0.9503\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3246 - accuracy: 0.9108\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1888 - accuracy: 0.9435\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3229 - accuracy: 0.9122\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1933 - accuracy: 0.9455\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3297 - accuracy: 0.9101\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1781 - accuracy: 0.9495\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3322 - accuracy: 0.9106\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1872 - accuracy: 0.9451\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3282 - accuracy: 0.9123\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1791 - accuracy: 0.9492\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3231 - accuracy: 0.9121\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1743 - accuracy: 0.9494\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3253 - accuracy: 0.9085\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1674 - accuracy: 0.9524\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3216 - accuracy: 0.9119\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1860 - accuracy: 0.9467\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3273 - accuracy: 0.9117\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1858 - accuracy: 0.9457\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3301 - accuracy: 0.9078\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1788 - accuracy: 0.9473\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.3242 - accuracy: 0.9141\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1765 - accuracy: 0.9499\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3242 - accuracy: 0.9119\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1854 - accuracy: 0.9469\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3267 - accuracy: 0.9097\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1746 - accuracy: 0.9487\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3310 - accuracy: 0.9095\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1870 - accuracy: 0.9459\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3250 - accuracy: 0.9112\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1672 - accuracy: 0.9513\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3344 - accuracy: 0.9085\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1706 - accuracy: 0.9505\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3216 - accuracy: 0.9133\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1830 - accuracy: 0.9496\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3295 - accuracy: 0.9119\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1920 - accuracy: 0.9435\n",
      "600/600 [==============================] - 2s 4ms/step - loss: 0.2758 - accuracy: 0.9241\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learn_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>momentum</th>\n",
       "      <th>nb_epoch</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CategoricalCrossentropy</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.950700</td>\n",
       "      <td>0.001071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CategoricalCrossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.949417</td>\n",
       "      <td>0.002112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CategoricalCrossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.949150</td>\n",
       "      <td>0.001205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CategoricalCrossentropy</td>\n",
       "      <td>0.9</td>\n",
       "      <td>10</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.948650</td>\n",
       "      <td>0.002225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CategoricalCrossentropy</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.948550</td>\n",
       "      <td>0.001975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CategoricalCrossentropy</td>\n",
       "      <td>0.8</td>\n",
       "      <td>10</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.948267</td>\n",
       "      <td>0.002913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CategoricalCrossentropy</td>\n",
       "      <td>0.8</td>\n",
       "      <td>10</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.947983</td>\n",
       "      <td>0.001330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CategoricalCrossentropy</td>\n",
       "      <td>0.6</td>\n",
       "      <td>10</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.947917</td>\n",
       "      <td>0.001993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CategoricalCrossentropy</td>\n",
       "      <td>0.9</td>\n",
       "      <td>10</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.947883</td>\n",
       "      <td>0.003086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CategoricalCrossentropy</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.946900</td>\n",
       "      <td>0.000204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CategoricalCrossentropy</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.946450</td>\n",
       "      <td>0.003465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CategoricalCrossentropy</td>\n",
       "      <td>0.6</td>\n",
       "      <td>10</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.946167</td>\n",
       "      <td>0.002521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activation  batch_size  learn_rate                     loss  momentum  \\\n",
       "3        relu         100        0.01  CategoricalCrossentropy       0.2   \n",
       "0        relu         100        0.01  CategoricalCrossentropy       0.0   \n",
       "1        relu         100        0.01  CategoricalCrossentropy       0.0   \n",
       "10       relu         100        0.01  CategoricalCrossentropy       0.9   \n",
       "5        relu         100        0.01  CategoricalCrossentropy       0.4   \n",
       "8        relu         100        0.01  CategoricalCrossentropy       0.8   \n",
       "9        relu         100        0.01  CategoricalCrossentropy       0.8   \n",
       "7        relu         100        0.01  CategoricalCrossentropy       0.6   \n",
       "11       relu         100        0.01  CategoricalCrossentropy       0.9   \n",
       "4        relu         100        0.01  CategoricalCrossentropy       0.4   \n",
       "2        relu         100        0.01  CategoricalCrossentropy       0.2   \n",
       "6        relu         100        0.01  CategoricalCrossentropy       0.6   \n",
       "\n",
       "    nb_epoch optimizer  mean_test_score  std_test_score  \n",
       "3         10     Nadam         0.950700        0.001071  \n",
       "0         10      Adam         0.949417        0.002112  \n",
       "1         10     Nadam         0.949150        0.001205  \n",
       "10        10      Adam         0.948650        0.002225  \n",
       "5         10     Nadam         0.948550        0.001975  \n",
       "8         10      Adam         0.948267        0.002913  \n",
       "9         10     Nadam         0.947983        0.001330  \n",
       "7         10     Nadam         0.947917        0.001993  \n",
       "11        10     Nadam         0.947883        0.003086  \n",
       "4         10      Adam         0.946900        0.000204  \n",
       "2         10      Adam         0.946450        0.003465  \n",
       "6         10      Adam         0.946167        0.002521  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search parameters\n",
    "learn_rate = [\n",
    "    0.01,# 0.1, 0.5, 1.0,# 1.5, 2.0\n",
    "]\n",
    "momentum = [\n",
    "    0.0, \n",
    "    0.2, \n",
    "    0.4, \n",
    "    0.6, \n",
    "    0.8, \n",
    "    0.9\n",
    "]\n",
    "optimizers = [\n",
    "    'Adam', 'Nadam',#'RMSprop', \n",
    "]\n",
    "epochs = [10]\n",
    "batches = [100]\n",
    "activation = [\n",
    "    'relu',# 'exponential',# 'swish', #'tanh',\n",
    "]\n",
    "loss = [\n",
    "    #'BinaryCrossentropy', \n",
    "    'CategoricalCrossentropy'\n",
    "]\n",
    "\n",
    "param_grid = dict(\n",
    "    learn_rate=learn_rate,\n",
    "    momentum=momentum,\n",
    "    optimizer=optimizers,\n",
    "    nb_epoch=epochs,\n",
    "    batch_size=batches,\n",
    "    activation=activation,\n",
    "    loss=loss\n",
    ")\n",
    "\n",
    "#history = model.fit(x_train_norm,y_train_encoded, epochs=20,validation_split=0.20,batch_size = 10)\n",
    "grid_result = GridSearchCV(estimator=model, param_grid=param_grid, cv=3).fit(x_train_norm, y_train_encoded)\n",
    "df = print_results(grid_result)\n",
    "results_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.0428 - accuracy: 0.9367\n",
      "2000/2000 [==============================] - 2s 822us/step - loss: 0.0234 - accuracy: 0.9659\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.0423 - accuracy: 0.9358\n",
      "2000/2000 [==============================] - 2s 816us/step - loss: 0.0261 - accuracy: 0.9625\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.0426 - accuracy: 0.9370\n",
      "2000/2000 [==============================] - 2s 834us/step - loss: 0.0245 - accuracy: 0.9653\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0422 - accuracy: 0.9388\n",
      "2000/2000 [==============================] - 2s 781us/step - loss: 0.0235 - accuracy: 0.9663\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0414 - accuracy: 0.9388\n",
      "2000/2000 [==============================] - 2s 832us/step - loss: 0.0262 - accuracy: 0.9628\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.0418 - accuracy: 0.9388\n",
      "2000/2000 [==============================] - 2s 824us/step - loss: 0.0245 - accuracy: 0.9633\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2288 - accuracy: 0.9328\n",
      "2000/2000 [==============================] - 2s 760us/step - loss: 0.1430 - accuracy: 0.9560\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2297 - accuracy: 0.9324\n",
      "2000/2000 [==============================] - 2s 775us/step - loss: 0.1341 - accuracy: 0.9604\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2316 - accuracy: 0.9320\n",
      "2000/2000 [==============================] - 2s 774us/step - loss: 0.1403 - accuracy: 0.9596\n",
      "4000/4000 [==============================] - 10s 3ms/step - loss: 0.2294 - accuracy: 0.9315\n",
      "2000/2000 [==============================] - 2s 776us/step - loss: 0.1287 - accuracy: 0.9635\n",
      "4000/4000 [==============================] - 10s 3ms/step - loss: 0.2259 - accuracy: 0.9337\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.1384 - accuracy: 0.9584\n",
      "4000/4000 [==============================] - 12s 3ms/step - loss: 0.2282 - accuracy: 0.9335\n",
      "2000/2000 [==============================] - 2s 770us/step - loss: 0.1384 - accuracy: 0.9572\n",
      "6000/6000 [==============================] - 8s 1ms/step - loss: 0.0362 - accuracy: 0.9462\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learn_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>momentum</th>\n",
       "      <th>nb_epoch</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>BinaryCrossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.964600</td>\n",
       "      <td>0.001467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>BinaryCrossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.964117</td>\n",
       "      <td>0.001560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CategoricalCrossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.959700</td>\n",
       "      <td>0.002724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CategoricalCrossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.958667</td>\n",
       "      <td>0.001921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  activation  batch_size  learn_rate                     loss  momentum  \\\n",
       "0       relu          10        0.01       BinaryCrossentropy       0.0   \n",
       "1       relu          10        0.01       BinaryCrossentropy       0.0   \n",
       "3       relu          10        0.01  CategoricalCrossentropy       0.0   \n",
       "2       relu          10        0.01  CategoricalCrossentropy       0.0   \n",
       "\n",
       "   nb_epoch optimizer  mean_test_score  std_test_score  \n",
       "0        10      Adam         0.964600        0.001467  \n",
       "1        10     Nadam         0.964117        0.001560  \n",
       "3        10     Nadam         0.959700        0.002724  \n",
       "2        10      Adam         0.958667        0.001921  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search parameters\n",
    "learn_rate = [\n",
    "    0.01,# 0.1, 0.5, 1.0,# 1.5, 2.0\n",
    "]\n",
    "momentum = [\n",
    "    0.0, \n",
    "    #0.2, \n",
    "    #0.4, \n",
    "    #0.6, \n",
    "    #0.8, \n",
    "    #0.9\n",
    "]\n",
    "optimizers = [\n",
    "    'Adam', 'Nadam',#'RMSprop', \n",
    "]\n",
    "epochs = [10]\n",
    "batches = [10]\n",
    "activation = [\n",
    "    'relu',# 'exponential',# 'swish', #'tanh',\n",
    "]\n",
    "loss = [\n",
    "    'BinaryCrossentropy', \n",
    "    'CategoricalCrossentropy'\n",
    "]\n",
    "\n",
    "param_grid = dict(\n",
    "    learn_rate=learn_rate,\n",
    "    momentum=momentum,\n",
    "    optimizer=optimizers,\n",
    "    nb_epoch=epochs,\n",
    "    batch_size=batches,\n",
    "    activation=activation,\n",
    "    loss=loss\n",
    ")\n",
    "\n",
    "#history = model.fit(x_train_norm,y_train_encoded, epochs=20,validation_split=0.20,batch_size = 10)\n",
    "grid_result = GridSearchCV(estimator=model, param_grid=param_grid, cv=3).fit(x_train_norm, y_train_encoded)\n",
    "df = print_results(grid_result)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.0431 - accuracy: 0.9352\n",
      "2000/2000 [==============================] - 2s 790us/step - loss: 0.0247 - accuracy: 0.9654\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.0422 - accuracy: 0.9377\n",
      "2000/2000 [==============================] - 2s 808us/step - loss: 0.0255 - accuracy: 0.9615\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.0420 - accuracy: 0.9377\n",
      "2000/2000 [==============================] - 2s 796us/step - loss: 0.0259 - accuracy: 0.9636\n",
      "4000/4000 [==============================] - 9s 2ms/step - loss: 0.0432 - accuracy: 0.9354\n",
      "2000/2000 [==============================] - 2s 819us/step - loss: 0.0245 - accuracy: 0.9636\n",
      "4000/4000 [==============================] - 9s 2ms/step - loss: 0.0417 - accuracy: 0.9394\n",
      "2000/2000 [==============================] - 2s 813us/step - loss: 0.0253 - accuracy: 0.9628\n",
      "4000/4000 [==============================] - 9s 2ms/step - loss: 0.0420 - accuracy: 0.9378\n",
      "2000/2000 [==============================] - 2s 805us/step - loss: 0.0241 - accuracy: 0.9661\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2360 - accuracy: 0.9294\n",
      "2000/2000 [==============================] - 2s 765us/step - loss: 0.1296 - accuracy: 0.9613\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2321 - accuracy: 0.9324\n",
      "2000/2000 [==============================] - 2s 759us/step - loss: 0.1566 - accuracy: 0.9534\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2311 - accuracy: 0.9315\n",
      "2000/2000 [==============================] - 2s 752us/step - loss: 0.1469 - accuracy: 0.9565\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.2282 - accuracy: 0.9326\n",
      "2000/2000 [==============================] - 2s 764us/step - loss: 0.1220 - accuracy: 0.9633\n",
      "4000/4000 [==============================] - 9s 2ms/step - loss: 0.2252 - accuracy: 0.9331\n",
      "2000/2000 [==============================] - 2s 760us/step - loss: 0.1352 - accuracy: 0.9590\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.2283 - accuracy: 0.9306\n",
      "2000/2000 [==============================] - 2s 755us/step - loss: 0.1340 - accuracy: 0.9602\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0624 - accuracy: 0.9111\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 0.0337 - accuracy: 0.9509\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0611 - accuracy: 0.9126\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 0.0343 - accuracy: 0.9513\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.0613 - accuracy: 0.9128\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 0.0327 - accuracy: 0.9534\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.0604 - accuracy: 0.9170\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 0.0332 - accuracy: 0.9544\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.0601 - accuracy: 0.9158\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 0.0345 - accuracy: 0.9500\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.0609 - accuracy: 0.9151\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 0.0327 - accuracy: 0.9563\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.2805 - accuracy: 0.9211\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 0.1525 - accuracy: 0.9564\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.2730 - accuracy: 0.9238\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 0.1668 - accuracy: 0.9503\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.2833 - accuracy: 0.9196\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.1460 - accuracy: 0.9572\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.2795 - accuracy: 0.9220\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 0.1610 - accuracy: 0.9513\n",
      "800/800 [==============================] - 2s 3ms/step - loss: 0.2767 - accuracy: 0.9228\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 0.1704 - accuracy: 0.9498\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.2795 - accuracy: 0.9225\n",
      "400/400 [==============================] - 0s 1ms/step - loss: 0.1514 - accuracy: 0.9559\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0786 - accuracy: 0.8877\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0415 - accuracy: 0.9409\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0769 - accuracy: 0.8917\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0438 - accuracy: 0.9385\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.0815 - accuracy: 0.8866\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0432 - accuracy: 0.9413\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0745 - accuracy: 0.8990\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0384 - accuracy: 0.9451\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0751 - accuracy: 0.8995\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0415 - accuracy: 0.9398\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.0758 - accuracy: 0.8969\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0397 - accuracy: 0.9460\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3239 - accuracy: 0.9113\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1766 - accuracy: 0.9490\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3215 - accuracy: 0.9123\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1917 - accuracy: 0.9453\n",
      "400/400 [==============================] - 1s 2ms/step - loss: 0.3265 - accuracy: 0.9094\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1873 - accuracy: 0.9444\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3370 - accuracy: 0.9093\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1895 - accuracy: 0.9457\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3220 - accuracy: 0.9140\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1827 - accuracy: 0.9484\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.3286 - accuracy: 0.9111\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1759 - accuracy: 0.9506\n",
      "6000/6000 [==============================] - 13s 2ms/step - loss: 0.0359 - accuracy: 0.9462\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learn_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>momentum</th>\n",
       "      <th>nb_epoch</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>BinaryCrossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.964167</td>\n",
       "      <td>0.001376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>BinaryCrossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.963517</td>\n",
       "      <td>0.001573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CategoricalCrossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.960817</td>\n",
       "      <td>0.001818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CategoricalCrossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.957050</td>\n",
       "      <td>0.003228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>relu</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CategoricalCrossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.954683</td>\n",
       "      <td>0.003081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>relu</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>BinaryCrossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.953583</td>\n",
       "      <td>0.002616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>relu</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CategoricalCrossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.952333</td>\n",
       "      <td>0.002605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>BinaryCrossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.951867</td>\n",
       "      <td>0.001096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CategoricalCrossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.948250</td>\n",
       "      <td>0.002002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>CategoricalCrossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.946233</td>\n",
       "      <td>0.001998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>BinaryCrossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.943633</td>\n",
       "      <td>0.002735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>BinaryCrossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.940233</td>\n",
       "      <td>0.001236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activation  batch_size  learn_rate                     loss  momentum  \\\n",
       "1        relu          10        0.01       BinaryCrossentropy       0.0   \n",
       "0        relu          10        0.01       BinaryCrossentropy       0.0   \n",
       "3        relu          10        0.01  CategoricalCrossentropy       0.0   \n",
       "2        relu          10        0.01  CategoricalCrossentropy       0.0   \n",
       "6        relu          50        0.01  CategoricalCrossentropy       0.0   \n",
       "5        relu          50        0.01       BinaryCrossentropy       0.0   \n",
       "7        relu          50        0.01  CategoricalCrossentropy       0.0   \n",
       "4        relu          50        0.01       BinaryCrossentropy       0.0   \n",
       "11       relu         100        0.01  CategoricalCrossentropy       0.0   \n",
       "10       relu         100        0.01  CategoricalCrossentropy       0.0   \n",
       "9        relu         100        0.01       BinaryCrossentropy       0.0   \n",
       "8        relu         100        0.01       BinaryCrossentropy       0.0   \n",
       "\n",
       "    nb_epoch optimizer  mean_test_score  std_test_score  \n",
       "1         10     Nadam         0.964167        0.001376  \n",
       "0         10      Adam         0.963517        0.001573  \n",
       "3         10     Nadam         0.960817        0.001818  \n",
       "2         10      Adam         0.957050        0.003228  \n",
       "6         10      Adam         0.954683        0.003081  \n",
       "5         10     Nadam         0.953583        0.002616  \n",
       "7         10     Nadam         0.952333        0.002605  \n",
       "4         10      Adam         0.951867        0.001096  \n",
       "11        10     Nadam         0.948250        0.002002  \n",
       "10        10      Adam         0.946233        0.001998  \n",
       "9         10     Nadam         0.943633        0.002735  \n",
       "8         10      Adam         0.940233        0.001236  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search parameters\n",
    "learn_rate = [\n",
    "    0.01,# 0.1, 0.5, 1.0,# 1.5, 2.0\n",
    "]\n",
    "momentum = [\n",
    "    0.0, \n",
    "    #0.2, \n",
    "    #0.4, \n",
    "    #0.6, \n",
    "    #0.8, \n",
    "    #0.9\n",
    "]\n",
    "optimizers = [\n",
    "    'Adam', 'Nadam',#'RMSprop', \n",
    "]\n",
    "epochs = [10]\n",
    "batches = [10, 50, 100]\n",
    "activation = [\n",
    "    'relu',# 'exponential',# 'swish', #'tanh',\n",
    "]\n",
    "loss = [\n",
    "    'BinaryCrossentropy', \n",
    "    'CategoricalCrossentropy'\n",
    "]\n",
    "\n",
    "param_grid = dict(\n",
    "    learn_rate=learn_rate,\n",
    "    momentum=momentum,\n",
    "    optimizer=optimizers,\n",
    "    nb_epoch=epochs,\n",
    "    batch_size=batches,\n",
    "    activation=activation,\n",
    "    loss=loss\n",
    ")\n",
    "\n",
    "#history = model.fit(x_train_norm,y_train_encoded, epochs=20,validation_split=0.20,batch_size = 10)\n",
    "grid_result = GridSearchCV(estimator=model, param_grid=param_grid, cv=3).fit(x_train_norm, y_train_encoded)\n",
    "df = print_results(grid_result)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4800/4800 [==============================] - 7s 1ms/step - loss: 0.0394 - accuracy: 0.9411 - val_loss: 0.0211 - val_accuracy: 0.9695\n",
      "Epoch 2/10\n",
      "4800/4800 [==============================] - 7s 1ms/step - loss: 0.0169 - accuracy: 0.9769 - val_loss: 0.0183 - val_accuracy: 0.9743\n",
      "Epoch 3/10\n",
      "1790/4800 [==========>...................] - ETA: 3s - loss: 0.0105 - accuracy: 0.9864"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-101058b66b01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m#grid_result = GridSearchCV(estimator=model, param_grid=param_grid, cv=3).fit(x_train_norm, y_train_encoded)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#df = print_results(grid_result)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/muody/.local/lib/python3.6/site-packages/tensorflow/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid shape for y: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/muody/.local/lib/python3.6/site-packages/tensorflow/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/muody/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/muody/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/muody/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/muody/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/muody/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2826\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2828\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2829\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/muody/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3171\u001b[0m           *args, **kwargs)\n\u001b[1;32m   3172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3173\u001b[0;31m     \u001b[0mcache_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3175\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/muody/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_cache_key\u001b[0;34m(self, args, kwargs, include_tensor_ranks_only)\u001b[0m\n\u001b[1;32m   2990\u001b[0m     \u001b[0;31m# Don't need to open an init_scope if the _cache_key call is in eager mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2991\u001b[0m     \u001b[0;31m# already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2992\u001b[0;31m     \u001b[0mexecuting_eagerly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2993\u001b[0m     \u001b[0mparent_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m     \u001b[0mxla_context_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(learn_rate=0.01, momentum=0, optimizer='adam', activation='relu', loss='binary_crossentropy'):\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(name = \"hidden_layer\", units=512, input_shape = (784,), activation=activation))\n",
    "    model.add(Dense(name = \"output_layer\", units=10, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model)\n",
    "history = model.fit(x_train_norm,y_train_encoded, epochs=10,validation_split=0.20,batch_size = 10)\n",
    "#grid_result = GridSearchCV(estimator=model, param_grid=param_grid, cv=3).fit(x_train_norm, y_train_encoded)\n",
    "#df = print_results(grid_result)\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reconfigure to Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28, 1)\n",
      "60000 training samples\n",
      "10000 testing samples\n",
      "X_train shape: (60000, 28, 28, 1)\n",
      "60000 training samples\n",
      "10000 testing samples\n",
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, Activation\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "if K.image_data_format() == 'channels_first': \n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols) \n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else: \n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1) \n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'training samples')\n",
    "print(X_test.shape[0], 'testing samples')\n",
    "\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'training samples')\n",
    "print(X_test.shape[0], 'testing samples')\n",
    "print(input_shape)\n",
    "\n",
    "x_train_norm.shape\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "960/960 [==============================] - 33s 34ms/step - loss: 0.6098 - accuracy: 0.7967 - val_loss: 0.1446 - val_accuracy: 0.9578\n",
      "Epoch 2/30\n",
      "960/960 [==============================] - 36s 38ms/step - loss: 0.1341 - accuracy: 0.9590 - val_loss: 0.0962 - val_accuracy: 0.9711\n",
      "Epoch 3/30\n",
      "960/960 [==============================] - 42s 43ms/step - loss: 0.0908 - accuracy: 0.9720 - val_loss: 0.0754 - val_accuracy: 0.9771\n",
      "Epoch 4/30\n",
      "960/960 [==============================] - 41s 43ms/step - loss: 0.0735 - accuracy: 0.9772 - val_loss: 0.0759 - val_accuracy: 0.9771\n",
      "Epoch 5/30\n",
      "960/960 [==============================] - 40s 41ms/step - loss: 0.0609 - accuracy: 0.9808 - val_loss: 0.0633 - val_accuracy: 0.9809\n",
      "Epoch 6/30\n",
      "960/960 [==============================] - 40s 42ms/step - loss: 0.0506 - accuracy: 0.9833 - val_loss: 0.0629 - val_accuracy: 0.9810\n",
      "Epoch 7/30\n",
      "960/960 [==============================] - 40s 41ms/step - loss: 0.0428 - accuracy: 0.9864 - val_loss: 0.0593 - val_accuracy: 0.9832\n",
      "Epoch 8/30\n",
      "960/960 [==============================] - 40s 42ms/step - loss: 0.0375 - accuracy: 0.9877 - val_loss: 0.0526 - val_accuracy: 0.9842\n",
      "Epoch 9/30\n",
      "960/960 [==============================] - 39s 41ms/step - loss: 0.0330 - accuracy: 0.9895 - val_loss: 0.0525 - val_accuracy: 0.9859\n",
      "Epoch 10/30\n",
      "960/960 [==============================] - 41s 43ms/step - loss: 0.0275 - accuracy: 0.9908 - val_loss: 0.0519 - val_accuracy: 0.9857\n",
      "Epoch 11/30\n",
      "960/960 [==============================] - 40s 41ms/step - loss: 0.0239 - accuracy: 0.9922 - val_loss: 0.0528 - val_accuracy: 0.9848\n",
      "Epoch 12/30\n",
      "960/960 [==============================] - 40s 41ms/step - loss: 0.0210 - accuracy: 0.9929 - val_loss: 0.0458 - val_accuracy: 0.9865\n",
      "Epoch 13/30\n",
      "960/960 [==============================] - 40s 41ms/step - loss: 0.0192 - accuracy: 0.9936 - val_loss: 0.0503 - val_accuracy: 0.9867\n",
      "Epoch 14/30\n",
      "960/960 [==============================] - 40s 42ms/step - loss: 0.0167 - accuracy: 0.9946 - val_loss: 0.0482 - val_accuracy: 0.9870\n",
      "Epoch 15/30\n",
      "960/960 [==============================] - 40s 42ms/step - loss: 0.0157 - accuracy: 0.9948 - val_loss: 0.0470 - val_accuracy: 0.9874\n",
      "Epoch 16/30\n",
      "960/960 [==============================] - 40s 42ms/step - loss: 0.0149 - accuracy: 0.9948 - val_loss: 0.0507 - val_accuracy: 0.9869\n",
      "Epoch 17/30\n",
      "960/960 [==============================] - 40s 42ms/step - loss: 0.0126 - accuracy: 0.9957 - val_loss: 0.0755 - val_accuracy: 0.9790\n",
      "Epoch 18/30\n",
      "960/960 [==============================] - 39s 41ms/step - loss: 0.0108 - accuracy: 0.9964 - val_loss: 0.0601 - val_accuracy: 0.9873\n",
      "Epoch 19/30\n",
      "960/960 [==============================] - 40s 42ms/step - loss: 0.0123 - accuracy: 0.9956 - val_loss: 0.0491 - val_accuracy: 0.9874\n",
      "Epoch 20/30\n",
      "960/960 [==============================] - 40s 42ms/step - loss: 0.0094 - accuracy: 0.9967 - val_loss: 0.0500 - val_accuracy: 0.9877\n",
      "Epoch 21/30\n",
      "960/960 [==============================] - 39s 40ms/step - loss: 0.0096 - accuracy: 0.9964 - val_loss: 0.0444 - val_accuracy: 0.9886\n",
      "Epoch 22/30\n",
      "960/960 [==============================] - 40s 42ms/step - loss: 0.0080 - accuracy: 0.9971 - val_loss: 0.0469 - val_accuracy: 0.9887\n",
      "Epoch 23/30\n",
      "960/960 [==============================] - 40s 42ms/step - loss: 0.0068 - accuracy: 0.9976 - val_loss: 0.0574 - val_accuracy: 0.9868\n",
      "Epoch 24/30\n",
      "960/960 [==============================] - 40s 41ms/step - loss: 0.0073 - accuracy: 0.9974 - val_loss: 0.0621 - val_accuracy: 0.9872\n",
      "Epoch 25/30\n",
      "960/960 [==============================] - 39s 41ms/step - loss: 0.0077 - accuracy: 0.9971 - val_loss: 0.0551 - val_accuracy: 0.9869\n",
      "Epoch 26/30\n",
      "960/960 [==============================] - 40s 42ms/step - loss: 0.0067 - accuracy: 0.9978 - val_loss: 0.0520 - val_accuracy: 0.9891\n",
      "Epoch 27/30\n",
      "960/960 [==============================] - 38s 40ms/step - loss: 0.0070 - accuracy: 0.9979 - val_loss: 0.0566 - val_accuracy: 0.9886\n",
      "Epoch 28/30\n",
      "960/960 [==============================] - 39s 41ms/step - loss: 0.0052 - accuracy: 0.9982 - val_loss: 0.0586 - val_accuracy: 0.9883\n",
      "Epoch 29/30\n",
      "960/960 [==============================] - 41s 43ms/step - loss: 0.0059 - accuracy: 0.9980 - val_loss: 0.0489 - val_accuracy: 0.9889\n",
      "Epoch 30/30\n",
      "960/960 [==============================] - 40s 42ms/step - loss: 0.0069 - accuracy: 0.9977 - val_loss: 0.0575 - val_accuracy: 0.9867\n"
     ]
    }
   ],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(learn_rate=0.01, momentum=0, optimizer='adam', activation='relu', loss='categorical_crossentropy'):\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu',input_shape=input_shape))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\")),\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2))),\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation=\"relu\")),\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2))),\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(name = \"hidden_layer\", units=512, activation=activation))\n",
    "    model.add(Dense(name = \"output_layer\", units=10, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model)\n",
    "history = model.fit(X_train,y_train, epochs=30,validation_split=0.20,batch_size = 50)\n",
    "#grid_result = GridSearchCV(estimator=model, param_grid=param_grid, cv=3).fit(x_train_norm, y_train_encoded)\n",
    "#df = print_results(grid_result)\n",
    "#df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pred_classes = np.argmax(model.predict(x_train_norm), axis=-1)\n",
    "pixel_data = {'pred_class':pred_classes}\n",
    "for k in range(0,784): \n",
    "    pixel_data[f\"pix_val_{k}\"] = x_train_norm[:,k]\n",
    "pixel_df = pd.DataFrame(pixel_data)\n",
    "pixel_df.head()\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# Separating out the features\n",
    "features = [*pixel_data][1:] # ['pix_val_0', 'pix_val_1',...]\n",
    "x = pixel_df.loc[:, features].values \n",
    "\n",
    "pca = PCA(n_components=154)\n",
    "principalComponents = pca.fit_transform(x_train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4800/4800 [==============================] - 7s 1ms/step - loss: 0.0403 - accuracy: 0.9399 - val_loss: 0.0223 - val_accuracy: 0.9689\n",
      "Epoch 2/20\n",
      "4800/4800 [==============================] - 7s 1ms/step - loss: 0.0175 - accuracy: 0.9759 - val_loss: 0.0205 - val_accuracy: 0.9698\n",
      "Epoch 3/20\n",
      "4800/4800 [==============================] - 7s 1ms/step - loss: 0.0118 - accuracy: 0.9848 - val_loss: 0.0165 - val_accuracy: 0.9790\n",
      "Epoch 4/20\n",
      "4800/4800 [==============================] - 7s 1ms/step - loss: 0.0087 - accuracy: 0.9895 - val_loss: 0.0168 - val_accuracy: 0.9791\n",
      "Epoch 5/20\n",
      "4800/4800 [==============================] - 7s 1ms/step - loss: 0.0063 - accuracy: 0.9929 - val_loss: 0.0172 - val_accuracy: 0.9794\n",
      "Epoch 6/20\n",
      "4800/4800 [==============================] - 7s 1ms/step - loss: 0.0051 - accuracy: 0.9949 - val_loss: 0.0170 - val_accuracy: 0.9807\n",
      "Epoch 7/20\n",
      "4800/4800 [==============================] - 7s 1ms/step - loss: 0.0041 - accuracy: 0.9965 - val_loss: 0.0197 - val_accuracy: 0.9801\n",
      "Epoch 8/20\n",
      "4800/4800 [==============================] - 7s 2ms/step - loss: 0.0033 - accuracy: 0.9972 - val_loss: 0.0211 - val_accuracy: 0.9804\n",
      "Epoch 9/20\n",
      "4800/4800 [==============================] - 7s 2ms/step - loss: 0.0028 - accuracy: 0.9978 - val_loss: 0.0223 - val_accuracy: 0.9809\n",
      "Epoch 10/20\n",
      "4800/4800 [==============================] - 9s 2ms/step - loss: 0.0026 - accuracy: 0.9981 - val_loss: 0.0220 - val_accuracy: 0.9808\n",
      "Epoch 11/20\n",
      "4800/4800 [==============================] - 7s 2ms/step - loss: 0.0022 - accuracy: 0.9986 - val_loss: 0.0217 - val_accuracy: 0.9833\n",
      "Epoch 12/20\n",
      "4800/4800 [==============================] - 7s 1ms/step - loss: 0.0023 - accuracy: 0.9984 - val_loss: 0.0243 - val_accuracy: 0.9827\n",
      "Epoch 13/20\n",
      "4800/4800 [==============================] - 10s 2ms/step - loss: 0.0017 - accuracy: 0.9989 - val_loss: 0.0261 - val_accuracy: 0.9808\n",
      "Epoch 14/20\n",
      "4800/4800 [==============================] - 7s 2ms/step - loss: 0.0018 - accuracy: 0.9989 - val_loss: 0.0246 - val_accuracy: 0.9834\n",
      "Epoch 15/20\n",
      "4800/4800 [==============================] - 7s 1ms/step - loss: 0.0015 - accuracy: 0.9990 - val_loss: 0.0276 - val_accuracy: 0.9820\n",
      "Epoch 16/20\n",
      "4800/4800 [==============================] - 8s 2ms/step - loss: 0.0017 - accuracy: 0.9990 - val_loss: 0.0250 - val_accuracy: 0.9828\n",
      "Epoch 17/20\n",
      "4800/4800 [==============================] - 9s 2ms/step - loss: 0.0015 - accuracy: 0.9992 - val_loss: 0.0288 - val_accuracy: 0.9811\n",
      "Epoch 18/20\n",
      "4800/4800 [==============================] - 7s 1ms/step - loss: 0.0014 - accuracy: 0.9994 - val_loss: 0.0293 - val_accuracy: 0.9810\n",
      "Epoch 19/20\n",
      "4800/4800 [==============================] - 7s 1ms/step - loss: 0.0013 - accuracy: 0.9992 - val_loss: 0.0312 - val_accuracy: 0.9805\n",
      "Epoch 20/20\n",
      "4800/4800 [==============================] - 7s 1ms/step - loss: 0.0012 - accuracy: 0.9995 - val_loss: 0.0309 - val_accuracy: 0.9825\n"
     ]
    }
   ],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(learn_rate=0.01, momentum=0, optimizer='adam', activation='relu', loss='binary_crossentropy'):\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(name = \"hidden_layer\", units=512, input_shape = (784,), activation=activation))\n",
    "    model.add(Dense(name = \"output_layer\", units=10, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model)\n",
    "history = model.fit(x_train_norm,y_train_encoded, epochs=20,validation_split=0.20,batch_size = 10)\n",
    "#grid_result = GridSearchCV(estimator=model, param_grid=param_grid, cv=3).fit(principalComponents, y_train_encoded)\n",
    "#grid_result = GridSearchCV(estimator=model, param_grid=param_grid, cv=3).fit(x_train_norm, y_train_encoded)\n",
    "#df = print_results(grid_result)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learn_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>momentum</th>\n",
       "      <th>nb_epoch</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>BinaryCrossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.964167</td>\n",
       "      <td>0.000655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  activation  batch_size  learn_rate                loss  momentum  nb_epoch  \\\n",
       "0       relu          10        0.01  BinaryCrossentropy       0.0        10   \n",
       "\n",
       "  optimizer  mean_test_score  std_test_score  \n",
       "0      Adam         0.964167        0.000655  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para = pd.DataFrame.from_dict(grid_result.cv_results_['params'])\n",
    "mean = pd.DataFrame(grid_result.cv_results_['mean_test_score'],columns=['mean_test_score'])\n",
    "stds = pd.DataFrame(grid_result.cv_results_['std_test_score'],columns=['std_test_score'])\n",
    "df = para.join(mean.join(stds)).sort_values('mean_test_score', ascending=False)\n",
    "df.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learn_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>momentum</th>\n",
       "      <th>nb_epoch</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>BinaryCrossentropy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.964167</td>\n",
       "      <td>0.000655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  activation  batch_size  learn_rate                loss  momentum  nb_epoch  \\\n",
       "0       relu          10        0.01  BinaryCrossentropy       0.0        10   \n",
       "\n",
       "  optimizer  mean_test_score  std_test_score  \n",
       "0      Adam         0.964167        0.000655  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para = pd.DataFrame.from_dict(grid_result.cv_results_['params'])\n",
    "mean = pd.DataFrame(grid_result.cv_results_['mean_test_score'],columns=['mean_test_score'])\n",
    "stds = pd.DataFrame(grid_result.cv_results_['std_test_score'],columns=['std_test_score'])\n",
    "df = para.join(mean.join(stds)).sort_values('mean_test_score', ascending=False)\n",
    "df.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
