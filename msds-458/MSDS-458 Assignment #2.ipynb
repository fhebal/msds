{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the number of hidden neurons\n",
    "* The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "* The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "* The number of hidden neurons should be less than twice the size of the input layer.\n",
    "\n",
    "https://www.heatonresearch.com/2017/06/01/hidden-layers.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the number of hidden layers\n",
    "* In artificial neural networks, hidden layers are required if and only if the data must be separated non-linearly.\n",
    "\n",
    "https://towardsdatascience.com/beginners-ask-how-many-hidden-layers-neurons-to-use-in-artificial-neural-networks-51466afa0d3e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard integrated circuit can be seen as a digital network of activation functions that can be \"ON\" (1) or \"OFF\" (0), depending on input. This is similar to the behavior of the linear perceptron in neural networks. However, only nonlinear activation functions allow such networks to compute nontrivial problems using only a small number of nodes, and such activation functions are called nonlinearities\n",
    "## Non-Linear Activation Functions\n",
    "Modern neural network models use non-linear activation functions. They allow the model to create complex mappings between the network’s inputs and outputs, which are essential for learning and modeling complex data, such as images, video, audio, and data sets which are non-linear or have high dimensionality.Almost any process imaginable can be represented as a functional computation in a neural network, provided that the activation function is non-linear.Non-linear functions address the problems of a linear activation function: They allow backpropagation because they have a derivative function which is related to the inputs. They allow “stacking” of multiple layers of neurons to create a deep neural network. Multiple hidden layers of neurons are needed to learn complex data sets with high levels of accuracy.\n",
    "### Sigmoid / Logistic\n",
    "**Advantages:** Smooth gradient, preventing “jumps” in output values.\n",
    "Output values bound between 0 and 1, normalizing the output of each neuron.\n",
    "Clear predictions—For X above 2 or below -2, tends to bring the Y value (the prediction) to the edge of the curve, very close to 1 or 0. This enables clear predictions.**Disadvantages:** Vanishing gradient—for very high or very low values of X, there is almost no change to the prediction, causing a vanishing gradient problem. This can result in the network refusing to learn further, or being too slow to reach an accurate prediction. Outputs not zero centered. Computationally expensive\n",
    "### TanH / Hyperbolic Tangent\n",
    "Zero centered—making it easier to model inputs that have strongly negative, neutral, and strongly positive values. Otherwise like the Sigmoid function.**Disadvantages:** Like the Sigmoid function\n",
    "### ReLU (Rectified Linear Unit)\n",
    "Computationally efficient—allows the network to converge very quickly\n",
    "Non-linear—although it looks like a linear function, ReLU has a derivative function and allows for backpropagation. **Disadvantages:** The Dying ReLU problem—when inputs approach zero, or are negative, the gradient of the function becomes zero, the network cannot perform backpropagation and cannot learn.\n",
    "\n",
    "### Parametric ReLU\n",
    "Allows the negative slope to be learned—unlike leaky ReLU, this function provides the slope of the negative part of the function as an argument. It is, therefore, possible to perform backpropagation and learn the most appropriate value of α. Otherwise like ReLU**Disadvantages** May perform differently for different problems.\n",
    "### Softmax\n",
    "Able to handle multiple classes only one class in other activation functions—normalizes the outputs for each class between 0 and 1, and divides by their sum, giving the probability of the input value being in a specific class.Useful for output neurons—typically Softmax is used only for the output layer, for neural networks that need to classify inputs into multiple categories.\n",
    "### Swish\n",
    "Swish is a new, self-gated activation function discovered by researchers at Google. According to their paper, it performs better than ReLU with a similar level of computational efficiency. In experiments on ImageNet with identical models running ReLU and Swish, the new function achieved top -1 classification accuracy 0.6-0.9% higher.\n",
    "\n",
    "## Notes\n",
    "Recent research by Franco Manessi and Alessandro Rozza attempted to find ways to automatically learn which is the optimal activation function for a certain neural network and to even automatically combine activation functions to achieve the highest accuracy. This is a very promising field of research because it attempts to discover an optimal activation function configuration automatically, whereas today, this parameter is manually tuned.\n",
    "\n",
    "## Derivatives of activation function\n",
    "The derivative—also known as a gradient—of an activation function is extremely important for training the neural network. Neural networks are trained using a process called backpropagation—this is an algorithm which traces back from the output of the model, through the different neurons which were involved in generating that output, back to the original weight applied to each neuron. Backpropagation suggests an optimal weight for each neuron which results in the most accurate prediction.\n",
    "\n",
    "## References\n",
    "1. https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/\n",
    "2. https://arxiv.org/pdf/1801.09403.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = [\n",
    "    'deserialize', 'elu', 'exponential', 'get', 'hard_sigmoid', 'linear', 'relu','selu', \n",
    "    'serialize', 'sigmoid', 'softmax', 'softplus', 'softsign', 'swish', 'tanh',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otimizers\n",
    "Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses.Optimization algorithms or strategies are responsible for reducing the losses and to provide the most accurate results possible.\n",
    "## Conclusions\n",
    "* Adam is the best optimizer. If one wants to train the neural network in less time and more efficiently than Adam is the optimizer.\n",
    "* For sparse data use the optimizers with dynamic learning rate.\n",
    "* If, want to use gradient descent algorithm than min-batch gradient descent is the best option.\n",
    "* TL;DR Adam works well in practice and outperforms other Adaptive techniques.\n",
    "* Use SGD+Nesterov for shallow networks, and either Adam or RMSprop for deepnets.\n",
    "\n",
    "## References\n",
    "1. https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6#:~:text=Optimizers%20are%20algorithms%20or%20methods,order%20to%20reduce%20the%20losses.&text=Optimization%20algorithms%20or%20strategies%20are,the%20most%20accurate%20results%20possible.\n",
    "2. https://www.dlology.com/blog/quick-notes-on-how-to-choose-optimizer-in-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = [\n",
    "    'SGD', \n",
    "    'RMSprop', \n",
    "    'Adam', \n",
    "    'Adadelta', \n",
    "    'Adagrad', \n",
    "    'Adamax', \n",
    "    'Nadam', \n",
    "    'Ftrl', \n",
    "    'rmsprop',\n",
    "]\n",
    "\n",
    "# Adam = RMSprop + Momentum\n",
    "# keras.optimizers.SGD(lr=0.01, nesterov=True)  <-- SGD + Nesterov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [\n",
    "    'BinaryCrossentropy',\n",
    "    'CategoricalCrossentropy',\n",
    "    'SparseCategoricalCrossentropy',\n",
    "    'Poisson',\n",
    "    'binary_crossentropy',\n",
    "    'categorical_crossentropy',\n",
    "    'sparse_categorical_crossentropy',\n",
    "    'poisson',\n",
    "    'KLDivergence',\n",
    "    'kl_divergence',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load MNIST data set:\n",
      "x_train:\t(60000, 28, 28)\n",
      "y_train:\t(60000,)\n",
      "x_test:\t\t(10000, 28, 28)\n",
      "y_test:\t\t(10000,)\n",
      "\n",
      "Encode y data:\n",
      "First ten entries of y_train:\n",
      " [5 0 4 1 9 2 1 3 1 4]\n",
      "\n",
      "First ten rows of one-hot y_train:\n",
      " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "y_train_encoded shape:  (60000, 10)\n",
      "y_test_encoded shape:  (10000, 10)\n",
      "x_train_reshaped shape:  (60000, 784)\n",
      "x_test_reshaped shape:  (10000, 784)\n",
      "\n",
      "Min-Max Normalization:\n",
      "{0.0, 0.011764706, 0.53333336, 0.07058824, 0.49411765, 0.6862745, 0.101960786, 0.6509804, 1.0, 0.96862745, 0.49803922, 0.11764706, 0.14117648, 0.36862746, 0.6039216, 0.6666667, 0.043137256, 0.05490196, 0.03529412, 0.85882354, 0.7764706, 0.7137255, 0.94509804, 0.3137255, 0.6117647, 0.41960785, 0.25882354, 0.32156864, 0.21960784, 0.8039216, 0.8666667, 0.8980392, 0.7882353, 0.52156866, 0.18039216, 0.30588236, 0.44705883, 0.3529412, 0.15294118, 0.6745098, 0.88235295, 0.99215686, 0.9490196, 0.7647059, 0.2509804, 0.19215687, 0.93333334, 0.9843137, 0.74509805, 0.7294118, 0.5882353, 0.50980395, 0.8862745, 0.105882354, 0.09019608, 0.16862746, 0.13725491, 0.21568628, 0.46666667, 0.3647059, 0.27450982, 0.8352941, 0.7176471, 0.5803922, 0.8117647, 0.9764706, 0.98039216, 0.73333335, 0.42352942, 0.003921569, 0.54509807, 0.67058825, 0.5294118, 0.007843138, 0.31764707, 0.0627451, 0.09411765, 0.627451, 0.9411765, 0.9882353, 0.95686275, 0.83137256, 0.5176471, 0.09803922, 0.1764706}\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(\"Load MNIST data set:\")\n",
    "(x_train, y_train), (x_test, y_test)= mnist.load_data()\n",
    "print('x_train:\\t{}'.format(x_train.shape))\n",
    "print('y_train:\\t{}'.format(y_train.shape))\n",
    "print('x_test:\\t\\t{}'.format(x_test.shape))\n",
    "print('y_test:\\t\\t{}'.format(y_test.shape))\n",
    "\n",
    "print()\n",
    "print(\"Encode y data:\") \n",
    "y_train_encoded = to_categorical(y_train)\n",
    "y_test_encoded = to_categorical(y_test)\n",
    "print(\"First ten entries of y_train:\\n {}\\n\".format(y_train[0:10]))\n",
    "print(\"First ten rows of one-hot y_train:\\n {}\".format(y_train_encoded[0:10,]))\n",
    "print()\n",
    "print('y_train_encoded shape: ', y_train_encoded.shape)\n",
    "print('y_test_encoded shape: ', y_test_encoded.shape)\n",
    "x_train_reshaped = np.reshape(x_train, (60000, 784))\n",
    "x_test_reshaped = np.reshape(x_test, (10000, 784))\n",
    "print('x_train_reshaped shape: ', x_train_reshaped.shape)\n",
    "print('x_test_reshaped shape: ', x_test_reshaped.shape)\n",
    "\n",
    "\n",
    "print()\n",
    "print('Min-Max Normalization:')\n",
    "x_train_norm = x_train_reshaped.astype('float32') / 255\n",
    "x_test_norm = x_test_reshaped.astype('float32') / 255\n",
    "print(set(x_train_norm[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:10000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use scikit-learn to grid search the learning rate and momentum\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "#from keras.optimizers import SGD\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(learn_rate=0.6, momentum=0, optimizer='adam', activation='relu', loss='binary_crossentropy'):\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(name = \"hidden_layer\", units=512, input_shape = (784,), activation=activation))\n",
    "    model.add(Dense(name = \"output_layer\", units=10, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model)\n",
    "# overfitting is really the issue for machine learning such a big problem\n",
    "# we will face this all the time, methods to address this include DropOut\n",
    "# research other ways to mitigate overfitting\n",
    "# how much dropout (20%, 10%, vary the dropout? dropout rates, hyperparameter tuning)\n",
    "# you should vary the nodes but i don't know how, in param_grid?\n",
    "# plot the results\n",
    "# document that i did NOT vary the node numbers etc...\n",
    "# scikit learn and tensorflow spun out of Google\n",
    "# does it matter which place the layers go?\n",
    "# squeeze between layers\n",
    "# vary the architechture of the model programatically!\n",
    "\n",
    "# Flatten layer <-- it essentially creates the 784 thing\n",
    "# The dense layer <-- this is the hidden layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(grid_result):\n",
    "    para = pd.DataFrame.from_dict(grid_result.cv_results_['params'])\n",
    "    mean = pd.DataFrame(grid_result.cv_results_['mean_test_score'],columns=['mean_test_score'])\n",
    "    stds = pd.DataFrame(grid_result.cv_results_['std_test_score'],columns=['std_test_score'])\n",
    "    df = para.join(mean.join(stds)).sort_values('mean_test_score', ascending=False)\n",
    "    df.reset_index().drop(columns=['index'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard integrated circuit can be seen as a digital network of activation functions that can be \"ON\" (1) or \"OFF\" (0), depending on input. This is similar to the behavior of the linear perceptron in neural networks. However, only nonlinear activation functions allow such networks to compute nontrivial problems using only a small number of nodes, and such activation functions are called nonlinearities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 2s 4ms/step - loss: 0.0873 - accuracy: 0.8783\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0605 - accuracy: 0.9175\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.0850 - accuracy: 0.8827\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0626 - accuracy: 0.9107\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.0888 - accuracy: 0.8786\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0606 - accuracy: 0.9160\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.0773 - accuracy: 0.8895\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0424 - accuracy: 0.9448\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.0680 - accuracy: 0.9026\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0405 - accuracy: 0.9455\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.0755 - accuracy: 0.8910\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0421 - accuracy: 0.9437\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.1141 - accuracy: 0.8390\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0703 - accuracy: 0.9016\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.1209 - accuracy: 0.8388\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0746 - accuracy: 0.8946\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.1305 - accuracy: 0.8184\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0733 - accuracy: 0.9003\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.0960 - accuracy: 0.8716\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0748 - accuracy: 0.9074\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.0936 - accuracy: 0.8740\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0796 - accuracy: 0.8965\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.0965 - accuracy: 0.8731\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0753 - accuracy: 0.9057\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.0797 - accuracy: 0.8881\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0421 - accuracy: 0.9390\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.0783 - accuracy: 0.8908\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0440 - accuracy: 0.9359\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.0785 - accuracy: 0.8877\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0423 - accuracy: 0.9402\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.0864 - accuracy: 0.8802\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0632 - accuracy: 0.9172\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.0866 - accuracy: 0.8784\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0661 - accuracy: 0.9081\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.0856 - accuracy: 0.8802\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0622 - accuracy: 0.9138\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.1235 - accuracy: 0.8270\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0729 - accuracy: 0.9022\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.1221 - accuracy: 0.8231\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0738 - accuracy: 0.8963\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.1220 - accuracy: 0.8360\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0711 - accuracy: 0.9038\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5301 - accuracy: 0.1294\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4319 - accuracy: 0.0972\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.5321 - accuracy: 0.1112\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4353 - accuracy: 0.0965\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.5321 - accuracy: 0.1160\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4350 - accuracy: 0.1007\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.1040 - accuracy: 0.8461\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0660 - accuracy: 0.9082\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.0948 - accuracy: 0.8604\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0650 - accuracy: 0.9118\n",
      "400/400 [==============================] - 2s 5ms/step - loss: 0.0926 - accuracy: 0.8647\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0645 - accuracy: 0.9117\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.0904 - accuracy: 0.8786\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0557 - accuracy: 0.9190\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.0893 - accuracy: 0.8756\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0573 - accuracy: 0.9174\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.0913 - accuracy: 0.8764\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0550 - accuracy: 0.9236\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.0850 - accuracy: 0.8835\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0498 - accuracy: 0.9320\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.0847 - accuracy: 0.8816\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0512 - accuracy: 0.9291\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.0856 - accuracy: 0.8825\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0513 - accuracy: 0.9303\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.0851 - accuracy: 0.8805\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.0552 - accuracy: 0.9212\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.0826 - accuracy: 0.8854\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0562 - accuracy: 0.9196\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.0845 - accuracy: 0.8809\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.0546 - accuracy: 0.9258\n",
      "600/600 [==============================] - 3s 4ms/step - loss: 0.0630 - accuracy: 0.9109\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learn_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>momentum</th>\n",
       "      <th>nb_epoch</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>exponential</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.944633</td>\n",
       "      <td>0.000744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.938383</td>\n",
       "      <td>0.001801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>swish</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.930467</td>\n",
       "      <td>0.001190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tanh</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.922167</td>\n",
       "      <td>0.002612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>softsign</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.919983</td>\n",
       "      <td>0.002634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>elu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.914717</td>\n",
       "      <td>0.002871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>selu</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.913033</td>\n",
       "      <td>0.003795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>softplus</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.910567</td>\n",
       "      <td>0.001639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>linear</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.903183</td>\n",
       "      <td>0.004774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.900767</td>\n",
       "      <td>0.003217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hard_sigmoid</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.898833</td>\n",
       "      <td>0.003071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>softmax</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.098167</td>\n",
       "      <td>0.001852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      activation  batch_size  learn_rate                 loss  momentum  \\\n",
       "1    exponential         100        0.01  binary_crossentropy         0   \n",
       "4           relu         100        0.01  binary_crossentropy         0   \n",
       "10         swish         100        0.01  binary_crossentropy         0   \n",
       "11          tanh         100        0.01  binary_crossentropy         0   \n",
       "9       softsign         100        0.01  binary_crossentropy         0   \n",
       "0            elu         100        0.01  binary_crossentropy         0   \n",
       "5           selu         100        0.01  binary_crossentropy         0   \n",
       "8       softplus         100        0.01  binary_crossentropy         0   \n",
       "3         linear         100        0.01  binary_crossentropy         0   \n",
       "6        sigmoid         100        0.01  binary_crossentropy         0   \n",
       "2   hard_sigmoid         100        0.01  binary_crossentropy         0   \n",
       "7        softmax         100        0.01  binary_crossentropy         0   \n",
       "\n",
       "    nb_epoch optimizer  mean_test_score  std_test_score  \n",
       "1         10      adam         0.944633        0.000744  \n",
       "4         10      adam         0.938383        0.001801  \n",
       "10        10      adam         0.930467        0.001190  \n",
       "11        10      adam         0.922167        0.002612  \n",
       "9         10      adam         0.919983        0.002634  \n",
       "0         10      adam         0.914717        0.002871  \n",
       "5         10      adam         0.913033        0.003795  \n",
       "8         10      adam         0.910567        0.001639  \n",
       "3         10      adam         0.903183        0.004774  \n",
       "6         10      adam         0.900767        0.003217  \n",
       "2         10      adam         0.898833        0.003071  \n",
       "7         10      adam         0.098167        0.001852  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search parameters\n",
    "learn_rate = [0.01]\n",
    "momentum = [0]\n",
    "optimizers = ['adam']\n",
    "epochs = [10]\n",
    "batches = [100]\n",
    "activation = [\n",
    "    'elu', 'exponential', 'hard_sigmoid', 'linear', 'relu','selu',\n",
    "    'sigmoid', 'softmax', 'softplus', 'softsign', 'swish', 'tanh',\n",
    "]\n",
    "loss = ['binary_crossentropy'] #,'categorical_crossentropy']\n",
    "#units_1 = [10,20,30,512,]\n",
    "#units_2 = [0.3]#[0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "\n",
    "param_grid = dict(\n",
    "    learn_rate=learn_rate,\n",
    "    momentum=momentum,\n",
    "    optimizer=optimizers,\n",
    "    nb_epoch=epochs,\n",
    "    batch_size=batches,\n",
    "    activation=activation,\n",
    "    loss=loss\n",
    ")\n",
    "\n",
    "#history = model.fit(x_train_norm,y_train_encoded, epochs=20,validation_split=0.20,batch_size = 10)\n",
    "grid_result = GridSearchCV(estimator=model, param_grid=param_grid, cv=3).fit(x_train_norm, y_train_encoded)\n",
    "df = print_results(grid_result)\n",
    "df\n",
    "# rmsprop and adam will differentiat based on batching...\n",
    "# some optimizers are better for \n",
    "# when you change between cpu gpu and tpu, optimized for certain things\n",
    "# if you are prebatching data - you will runa cycle where everythign through the processing units are zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([2.76603492, 2.40666739, 2.305703  , 2.33953087, 2.29458682,\n",
       "        2.27743443, 2.40332667, 2.46232883, 2.56138968, 2.35503356,\n",
       "        2.39518515, 2.51458971]),\n",
       " 'std_fit_time': array([0.35901052, 0.08911529, 0.09673487, 0.24915213, 0.06437691,\n",
       "        0.07782439, 0.07300052, 0.11983105, 0.16874432, 0.04790721,\n",
       "        0.15605043, 0.33113623]),\n",
       " 'mean_score_time': array([0.71882367, 0.74188948, 0.65586996, 0.68405954, 0.67811545,\n",
       "        0.72540466, 0.76941546, 0.84990271, 0.69938922, 0.6633153 ,\n",
       "        0.70898469, 0.70503966]),\n",
       " 'std_score_time': array([0.01082204, 0.06499944, 0.03106032, 0.07255358, 0.0111358 ,\n",
       "        0.05228122, 0.04335139, 0.10892635, 0.02667038, 0.04440396,\n",
       "        0.02340941, 0.07521151]),\n",
       " 'param_activation': masked_array(data=['elu', 'exponential', 'hard_sigmoid', 'linear', 'relu',\n",
       "                    'selu', 'sigmoid', 'softmax', 'softplus', 'softsign',\n",
       "                    'swish', 'tanh'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_batch_size': masked_array(data=[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "                    100],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learn_rate': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_loss': masked_array(data=['binary_crossentropy', 'binary_crossentropy',\n",
       "                    'binary_crossentropy', 'binary_crossentropy',\n",
       "                    'binary_crossentropy', 'binary_crossentropy',\n",
       "                    'binary_crossentropy', 'binary_crossentropy',\n",
       "                    'binary_crossentropy', 'binary_crossentropy',\n",
       "                    'binary_crossentropy', 'binary_crossentropy'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_momentum': masked_array(data=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_nb_epoch': masked_array(data=[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_optimizer': masked_array(data=['adam', 'adam', 'adam', 'adam', 'adam', 'adam', 'adam',\n",
       "                    'adam', 'adam', 'adam', 'adam', 'adam'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'activation': 'elu',\n",
       "   'batch_size': 100,\n",
       "   'learn_rate': 0.01,\n",
       "   'loss': 'binary_crossentropy',\n",
       "   'momentum': 0,\n",
       "   'nb_epoch': 10,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'exponential',\n",
       "   'batch_size': 100,\n",
       "   'learn_rate': 0.01,\n",
       "   'loss': 'binary_crossentropy',\n",
       "   'momentum': 0,\n",
       "   'nb_epoch': 10,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'hard_sigmoid',\n",
       "   'batch_size': 100,\n",
       "   'learn_rate': 0.01,\n",
       "   'loss': 'binary_crossentropy',\n",
       "   'momentum': 0,\n",
       "   'nb_epoch': 10,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'linear',\n",
       "   'batch_size': 100,\n",
       "   'learn_rate': 0.01,\n",
       "   'loss': 'binary_crossentropy',\n",
       "   'momentum': 0,\n",
       "   'nb_epoch': 10,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'relu',\n",
       "   'batch_size': 100,\n",
       "   'learn_rate': 0.01,\n",
       "   'loss': 'binary_crossentropy',\n",
       "   'momentum': 0,\n",
       "   'nb_epoch': 10,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'selu',\n",
       "   'batch_size': 100,\n",
       "   'learn_rate': 0.01,\n",
       "   'loss': 'binary_crossentropy',\n",
       "   'momentum': 0,\n",
       "   'nb_epoch': 10,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'sigmoid',\n",
       "   'batch_size': 100,\n",
       "   'learn_rate': 0.01,\n",
       "   'loss': 'binary_crossentropy',\n",
       "   'momentum': 0,\n",
       "   'nb_epoch': 10,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'softmax',\n",
       "   'batch_size': 100,\n",
       "   'learn_rate': 0.01,\n",
       "   'loss': 'binary_crossentropy',\n",
       "   'momentum': 0,\n",
       "   'nb_epoch': 10,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'softplus',\n",
       "   'batch_size': 100,\n",
       "   'learn_rate': 0.01,\n",
       "   'loss': 'binary_crossentropy',\n",
       "   'momentum': 0,\n",
       "   'nb_epoch': 10,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'softsign',\n",
       "   'batch_size': 100,\n",
       "   'learn_rate': 0.01,\n",
       "   'loss': 'binary_crossentropy',\n",
       "   'momentum': 0,\n",
       "   'nb_epoch': 10,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'swish',\n",
       "   'batch_size': 100,\n",
       "   'learn_rate': 0.01,\n",
       "   'loss': 'binary_crossentropy',\n",
       "   'momentum': 0,\n",
       "   'nb_epoch': 10,\n",
       "   'optimizer': 'adam'},\n",
       "  {'activation': 'tanh',\n",
       "   'batch_size': 100,\n",
       "   'learn_rate': 0.01,\n",
       "   'loss': 'binary_crossentropy',\n",
       "   'momentum': 0,\n",
       "   'nb_epoch': 10,\n",
       "   'optimizer': 'adam'}],\n",
       " 'split0_test_score': array([0.91745001, 0.94480002, 0.9016    , 0.90735   , 0.93895   ,\n",
       "        0.91724998, 0.90224999, 0.09725   , 0.90824997, 0.91895002,\n",
       "        0.93199998, 0.92115003]),\n",
       " 'split1_test_score': array([0.91074997, 0.94545001, 0.89455003, 0.89649999, 0.93594998,\n",
       "        0.90805   , 0.89630002, 0.0965    , 0.91180003, 0.9174    ,\n",
       "        0.92909998, 0.91960001]),\n",
       " 'split2_test_score': array([0.91595   , 0.94365001, 0.90034997, 0.90570003, 0.94024998,\n",
       "        0.9138    , 0.90375   , 0.10075   , 0.91165   , 0.92360002,\n",
       "        0.9303    , 0.92575002]),\n",
       " 'mean_test_score': array([0.91471666, 0.94463334, 0.89883333, 0.90318334, 0.93838332,\n",
       "        0.91303333, 0.90076667, 0.09816667, 0.91056667, 0.91998335,\n",
       "        0.93046665, 0.92216669]),\n",
       " 'std_test_score': array([0.00287094, 0.00074424, 0.00307145, 0.00477361, 0.00180062,\n",
       "        0.0037948 , 0.00321722, 0.00185218, 0.00163929, 0.0026345 ,\n",
       "        0.00118977, 0.00261162]),\n",
       " 'rank_test_score': array([ 6,  1, 11,  9,  2,  7, 10, 12,  8,  5,  3,  4], dtype=int32)}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum\n",
    "Momentum was invented for reducing high variance in SGD and softens the convergence. It accelerates the convergence towards the relevant direction and reduces the fluctuation to the irrelevant direction. One more hyperparameter is used in this method known as momentum symbolized by ‘γ’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search parameters\n",
    "learn_rate = [0.01]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "optimizers = ['adam']\n",
    "epochs = [10]\n",
    "batches = [10]\n",
    "activation = [\n",
    "    'elu', 'exponential', 'get', 'hard_sigmoid', 'linear', 'relu','selu', \n",
    "    'serialize', 'sigmoid', 'softmax', 'softplus', 'softsign', 'swish', 'tanh',\n",
    "]\n",
    "loss = ['binary_crossentropy'] #,'categorical_crossentropy']\n",
    "#units_1 = [10,20,30,512,]\n",
    "#units_2 = [0.3]#[0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "\n",
    "param_grid = dict(\n",
    "    learn_rate=learn_rate,\n",
    "    momentum=momentum,\n",
    "    optimizer=optimizers,\n",
    "    nb_epoch=epochs,\n",
    "    batch_size=batches,\n",
    "    activation=activation,\n",
    "    loss=loss\n",
    ")\n",
    "\n",
    "#history = model.fit(x_train_norm,y_train_encoded, epochs=20,validation_split=0.20,batch_size = 10)\n",
    "grid_result = GridSearchCV(estimator=model, param_grid=param_grid, cv=3).fit(x_train_norm, y_train_encoded)\n",
    "df = print_results(grid_result)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search parameters\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "momentum = [0]#[0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "optimizers = ['rmsprop']\n",
    "epochs = np.array([10])\n",
    "batches = np.array([10])\n",
    "activation = ['relu']#,'tanh']#,'softmax']\n",
    "loss = ['binary_crossentropy'] #,'categorical_crossentropy']\n",
    "#units_1 = [10,20,30,512,]\n",
    "#units_2 = [0.3]#[0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "\n",
    "param_grid = dict(\n",
    "    learn_rate=learn_rate,\n",
    "    momentum=momentum,\n",
    "    optimizer=optimizers,\n",
    "    nb_epoch=epochs,\n",
    "    batch_size=batches,\n",
    "    activation=activation,\n",
    "    loss=loss\n",
    ")\n",
    "\n",
    "#history = model.fit(x_train_norm,y_train_encoded, epochs=20,validation_split=0.20,batch_size = 10)\n",
    "grid_result = GridSearchCV(estimator=model, param_grid=param_grid, cv=10).fit(x_train_norm, y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pred_classes = np.argmax(model.predict(x_train_norm), axis=-1)\n",
    "pixel_data = {'pred_class':pred_classes}\n",
    "for k in range(0,784): \n",
    "    pixel_data[f\"pix_val_{k}\"] = x_train_norm[:,k]\n",
    "pixel_df = pd.DataFrame(pixel_data)\n",
    "pixel_df.head()\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# Separating out the features\n",
    "features = [*pixel_data][1:] # ['pix_val_0', 'pix_val_1',...]\n",
    "x = pixel_df.loc[:, features].values \n",
    "\n",
    "pca = PCA(n_components=154)\n",
    "principalComponents = pca.fit_transform(x_train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learn_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>momentum</th>\n",
       "      <th>nb_epoch</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.9647</td>\n",
       "      <td>0.003841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  activation  batch_size  learn_rate                 loss  momentum  nb_epoch  \\\n",
       "0       relu          10         0.6  binary_crossentropy         0        10   \n",
       "\n",
       "  optimizer  mean_test_score  std_test_score  \n",
       "0   rmsprop           0.9647        0.003841  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('Display.max_rows', None)\n",
    "para = pd.DataFrame.from_dict(grid_result.cv_results_['params'])\n",
    "mean = pd.DataFrame(grid_result.cv_results_['mean_test_score'],columns=['mean_test_score'])\n",
    "stds = pd.DataFrame(grid_result.cv_results_['std_test_score'],columns=['std_test_score'])\n",
    "df = para.join(mean.join(stds)).sort_values(['mean_test_score'], ascending=False)\n",
    "df.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5400/5400 [==============================] - 7s 1ms/step - loss: 0.0401 - accuracy: 0.9438\n",
      "600/600 [==============================] - 0s 733us/step - loss: 0.0231 - accuracy: 0.9700\n",
      "5400/5400 [==============================] - 7s 1ms/step - loss: 0.0401 - accuracy: 0.9434\n",
      "600/600 [==============================] - 0s 727us/step - loss: 0.0228 - accuracy: 0.9680\n",
      "5400/5400 [==============================] - 7s 1ms/step - loss: 0.0397 - accuracy: 0.9446\n",
      "600/600 [==============================] - 0s 696us/step - loss: 0.0277 - accuracy: 0.9637\n",
      "5400/5400 [==============================] - 7s 1ms/step - loss: 0.0403 - accuracy: 0.9433\n",
      "600/600 [==============================] - 0s 738us/step - loss: 0.0227 - accuracy: 0.9697\n",
      "5400/5400 [==============================] - 7s 1ms/step - loss: 0.0400 - accuracy: 0.9435\n",
      "600/600 [==============================] - 0s 721us/step - loss: 0.0260 - accuracy: 0.9668\n",
      "5400/5400 [==============================] - 7s 1ms/step - loss: 0.0395 - accuracy: 0.9445\n",
      "600/600 [==============================] - 0s 659us/step - loss: 0.0251 - accuracy: 0.9642\n",
      "5400/5400 [==============================] - 7s 1ms/step - loss: 0.0403 - accuracy: 0.9442\n",
      "600/600 [==============================] - 0s 698us/step - loss: 0.0253 - accuracy: 0.9652\n",
      "5400/5400 [==============================] - 7s 1ms/step - loss: 0.0395 - accuracy: 0.9453\n",
      "600/600 [==============================] - 0s 663us/step - loss: 0.0286 - accuracy: 0.9637\n",
      "5400/5400 [==============================] - 7s 1ms/step - loss: 0.0398 - accuracy: 0.9441\n",
      "600/600 [==============================] - 0s 662us/step - loss: 0.0286 - accuracy: 0.9618\n",
      "5400/5400 [==============================] - 7s 1ms/step - loss: 0.0406 - accuracy: 0.9418\n",
      "600/600 [==============================] - 0s 695us/step - loss: 0.0209 - accuracy: 0.9755\n",
      "6000/6000 [==============================] - 8s 1ms/step - loss: 0.0384 - accuracy: 0.9467\n"
     ]
    }
   ],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(learn_rate=0.6, momentum=0, optimizer='adam', activation='relu', loss='binary_crossentropy'):\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(name = \"hidden_layer\", units=512, input_shape = (154,), activation=activation))\n",
    "    model.add(Dense(name = \"output_layer\", units=10, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model)\n",
    "#history = model.fit(principalComponents,y_train_encoded, epochs=20,validation_split=0.10,batch_size = 10)\n",
    "grid_result = GridSearchCV(estimator=model, param_grid=param_grid, cv=3).fit(principalComponents, y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learn_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>momentum</th>\n",
       "      <th>nb_epoch</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.9647</td>\n",
       "      <td>0.003841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  activation  batch_size  learn_rate                 loss  momentum  nb_epoch  \\\n",
       "0       relu          10         0.6  binary_crossentropy         0        10   \n",
       "\n",
       "  optimizer  mean_test_score  std_test_score  \n",
       "0   rmsprop           0.9647        0.003841  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para = pd.DataFrame.from_dict(grid_result.cv_results_['params'])\n",
    "mean = pd.DataFrame(grid_result.cv_results_['mean_test_score'],columns=['mean_test_score'])\n",
    "stds = pd.DataFrame(grid_result.cv_results_['std_test_score'],columns=['std_test_score'])\n",
    "df = para.join(mean.join(stds)).sort_values('mean_test_score', ascending=False)\n",
    "df.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learn_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>momentum</th>\n",
       "      <th>nb_epoch</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.96685</td>\n",
       "      <td>0.003866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  activation  batch_size  learn_rate                 loss  momentum  nb_epoch  \\\n",
       "0       relu          10         0.6  binary_crossentropy         0        10   \n",
       "\n",
       "  optimizer  mean_test_score  std_test_score  \n",
       "0   rmsprop          0.96685        0.003866  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para = pd.DataFrame.from_dict(grid_result.cv_results_['params'])\n",
    "mean = pd.DataFrame(grid_result.cv_results_['mean_test_score'],columns=['mean_test_score'])\n",
    "stds = pd.DataFrame(grid_result.cv_results_['std_test_score'],columns=['std_test_score'])\n",
    "df = para.join(mean.join(stds)).sort_values('mean_test_score', ascending=False)\n",
    "df.reset_index().drop(columns=['index'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
