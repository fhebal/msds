{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the number of hidden neurons\n",
    "* The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "* The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "* The number of hidden neurons should be less than twice the size of the input layer.\n",
    "\n",
    "https://www.heatonresearch.com/2017/06/01/hidden-layers.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the number of hidden layers\n",
    "* In artificial neural networks, hidden layers are required if and only if the data must be separated non-linearly.\n",
    "\n",
    "https://towardsdatascience.com/beginners-ask-how-many-hidden-layers-neurons-to-use-in-artificial-neural-networks-51466afa0d3e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard integrated circuit can be seen as a digital network of activation functions that can be \"ON\" (1) or \"OFF\" (0), depending on input. This is similar to the behavior of the linear perceptron in neural networks. However, only nonlinear activation functions allow such networks to compute nontrivial problems using only a small number of nodes, and such activation functions are called nonlinearities\n",
    "## Non-Linear Activation Functions\n",
    "Modern neural network models use non-linear activation functions. They allow the model to create complex mappings between the network’s inputs and outputs, which are essential for learning and modeling complex data, such as images, video, audio, and data sets which are non-linear or have high dimensionality.Almost any process imaginable can be represented as a functional computation in a neural network, provided that the activation function is non-linear.Non-linear functions address the problems of a linear activation function: They allow backpropagation because they have a derivative function which is related to the inputs. They allow “stacking” of multiple layers of neurons to create a deep neural network. Multiple hidden layers of neurons are needed to learn complex data sets with high levels of accuracy.\n",
    "### Sigmoid / Logistic\n",
    "**Advantages:** Smooth gradient, preventing “jumps” in output values.\n",
    "Output values bound between 0 and 1, normalizing the output of each neuron.\n",
    "Clear predictions—For X above 2 or below -2, tends to bring the Y value (the prediction) to the edge of the curve, very close to 1 or 0. This enables clear predictions.**Disadvantages:** Vanishing gradient—for very high or very low values of X, there is almost no change to the prediction, causing a vanishing gradient problem. This can result in the network refusing to learn further, or being too slow to reach an accurate prediction. Outputs not zero centered. Computationally expensive\n",
    "### TanH / Hyperbolic Tangent\n",
    "Zero centered—making it easier to model inputs that have strongly negative, neutral, and strongly positive values. Otherwise like the Sigmoid function.**Disadvantages:** Like the Sigmoid function\n",
    "### ReLU (Rectified Linear Unit)\n",
    "Computationally efficient—allows the network to converge very quickly\n",
    "Non-linear—although it looks like a linear function, ReLU has a derivative function and allows for backpropagation. **Disadvantages:** The Dying ReLU problem—when inputs approach zero, or are negative, the gradient of the function becomes zero, the network cannot perform backpropagation and cannot learn.\n",
    "\n",
    "### Parametric ReLU\n",
    "Allows the negative slope to be learned—unlike leaky ReLU, this function provides the slope of the negative part of the function as an argument. It is, therefore, possible to perform backpropagation and learn the most appropriate value of α. Otherwise like ReLU**Disadvantages** May perform differently for different problems.\n",
    "### Softmax\n",
    "Able to handle multiple classes only one class in other activation functions—normalizes the outputs for each class between 0 and 1, and divides by their sum, giving the probability of the input value being in a specific class.Useful for output neurons—typically Softmax is used only for the output layer, for neural networks that need to classify inputs into multiple categories.\n",
    "### Swish\n",
    "Swish is a new, self-gated activation function discovered by researchers at Google. According to their paper, it performs better than ReLU with a similar level of computational efficiency. In experiments on ImageNet with identical models running ReLU and Swish, the new function achieved top -1 classification accuracy 0.6-0.9% higher.\n",
    "\n",
    "## Notes\n",
    "Recent research by Franco Manessi and Alessandro Rozza attempted to find ways to automatically learn which is the optimal activation function for a certain neural network and to even automatically combine activation functions to achieve the highest accuracy. This is a very promising field of research because it attempts to discover an optimal activation function configuration automatically, whereas today, this parameter is manually tuned.\n",
    "\n",
    "## Derivatives of activation function\n",
    "The derivative—also known as a gradient—of an activation function is extremely important for training the neural network. Neural networks are trained using a process called backpropagation—this is an algorithm which traces back from the output of the model, through the different neurons which were involved in generating that output, back to the original weight applied to each neuron. Backpropagation suggests an optimal weight for each neuron which results in the most accurate prediction.\n",
    "\n",
    "## References\n",
    "1. https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/\n",
    "2. https://arxiv.org/pdf/1801.09403.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = [\n",
    "    'deserialize', \n",
    "    'elu', \n",
    "    'exponential', \n",
    "    'get', \n",
    "    'hard_sigmoid', \n",
    "    'linear', \n",
    "    'relu', # max(x,0) nonlinear?\n",
    "    'selu', \n",
    "    'serialize', \n",
    "    'sigmoid', \n",
    "    'softmax', \n",
    "    'softplus', \n",
    "    'softsign', \n",
    "    'swish', \n",
    "    'tanh',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otimizers\n",
    "Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses.Optimization algorithms or strategies are responsible for reducing the losses and to provide the most accurate results possible.\n",
    "### Gradient Descent\n",
    "The most basic but most used optimization algorithm. It’s used heavily in linear regression and classification algorithms. Backpropagation in neural networks also uses a gradient descent algorithm.\n",
    "### Stochastic Gradient Descent\n",
    "is a variant of Gradient Descent. It tries to update the model’s parameters more frequently. In this, the model parameters are altered after computation of loss on each training example. So, if the dataset contains 1000 rows SGD will update the model parameters 1000 times in one cycle of dataset instead of one time as in Gradient Descent.\n",
    "### Mini-Batch Gradient Descent\n",
    "It’s best among all the variations of gradient descent algorithms. It is an improvement on both SGD and standard gradient descent. It updates the model parameters after every batch. So, the dataset is divided into various batches and after every batch, the parameters are updated.\n",
    "### Nesterov Accelerated Gradient\n",
    "Momentum may be a good method but if the momentum is too high the algorithm may miss the local minima and may continue to rise up. So, to resolve this issue the NAG algorithm was developed. It is a look ahead method. We know we’ll be using γV(t−1) for modifying the weights so, θ−γV(t−1) approximately tells us the future location. Now, we’ll calculate the cost based on this future parameter rather than the current one. Nesterov momentum has slightly less overshooting compare to standard momentum since it takes the \"gamble->correction\" approach has shown below.\n",
    "### Adagrad\n",
    "One of the disadvantages of all the optimizers explained is that the learning rate is constant for all parameters and for each cycle. This optimizer changes the learning rate. It changes the learning rate ‘η’ for each parameter and at every time step ‘t’. It’s a type second order optimization algorithm. It works on the derivative of an error function. It makes big updates for infrequent parameters and small updates for frequent parameters. For this reason, it is well-suited for dealing with sparse data. The main benefit of Adagrad is that we don’t need to tune the learning rate manually. Most implementations use a default value of 0.01 and leave it at that.Its main weakness is that its learning rate is always Decreasing and decaying.\n",
    "### AdaDelta\n",
    "It is an extension of AdaGrad which tends to remove the decaying learning Rate problem of it. Instead of accumulating all previously squared gradients, Adadelta limits the window of accumulated past gradients to some fixed size w. In this exponentially moving average is used rather than the sum of all the gradients. It is an extension of AdaGrad which tends to remove the decaying learning Rate problem of it. Another thing with AdaDelta is that we don’t even need to set a default learning rate.\n",
    "### Adam\n",
    "Adam (Adaptive Moment Estimation) works with momentums of first and second order. The intuition behind the Adam is that we don’t want to roll so fast just because we can jump over the minimum, we want to decrease the velocity a little bit for a careful search. In addition to storing an exponentially decaying average of past squared gradients like AdaDelta, Adam also keeps an exponentially decaying average of past gradients M(t).\n",
    "## Conclusions\n",
    "* Adam is the best optimizer. If one wants to train the neural network in less time and more efficiently than Adam is the optimizer.\n",
    "* For sparse data use the optimizers with dynamic learning rate.\n",
    "* If, want to use gradient descent algorithm than min-batch gradient descent is the best option.\n",
    "* TL;DR Adam works well in practice and outperforms other Adaptive techniques.\n",
    "* Use SGD+Nesterov for shallow networks, and either Adam or RMSprop for deepnets.\n",
    "\n",
    "## References\n",
    "1. https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6#:~:text=Optimizers%20are%20algorithms%20or%20methods,order%20to%20reduce%20the%20losses.&text=Optimization%20algorithms%20or%20strategies%20are,the%20most%20accurate%20results%20possible.\n",
    "2. https://www.dlology.com/blog/quick-notes-on-how-to-choose-optimizer-in-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = [\n",
    "    'SGD', \n",
    "    'RMSprop', \n",
    "    'Adam', \n",
    "    'Adadelta', \n",
    "    'Adagrad', \n",
    "    'Adamax', \n",
    "    'Nadam', \n",
    "    'Ftrl', \n",
    "    'rmsprop',\n",
    "]\n",
    "\n",
    "# Adam = RMSprop + Momentum\n",
    "# keras.optimizers.SGD(lr=0.01, nesterov=True)  <-- SGD + Nesterov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [\n",
    "    'BinaryCrossentropy',\n",
    "    'CategoricalCrossentropy',\n",
    "    'SparseCategoricalCrossentropy',\n",
    "    'Poisson',\n",
    "    'binary_crossentropy',\n",
    "    'categorical_crossentropy',\n",
    "    'sparse_categorical_crossentropy',\n",
    "    'poisson',\n",
    "    'KLDivergence',\n",
    "    'kl_divergence',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum\n",
    "Momentum was invented for reducing high variance in SGD and softens the convergence. It accelerates the convergence towards the relevant direction and reduces the fluctuation to the irrelevant direction. One more hyperparameter is used in this method known as momentum symbolized by ‘γ’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load MNIST data set:\n",
      "x_train:\t(60000, 28, 28)\n",
      "y_train:\t(60000,)\n",
      "x_test:\t\t(10000, 28, 28)\n",
      "y_test:\t\t(10000,)\n",
      "\n",
      "Encode y data:\n",
      "First ten entries of y_train:\n",
      " [5 0 4 1 9 2 1 3 1 4]\n",
      "\n",
      "First ten rows of one-hot y_train:\n",
      " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "y_train_encoded shape:  (60000, 10)\n",
      "y_test_encoded shape:  (10000, 10)\n",
      "x_train_reshaped shape:  (60000, 784)\n",
      "x_test_reshaped shape:  (10000, 784)\n",
      "\n",
      "Min-Max Normalization:\n",
      "{0.0, 0.011764706, 0.53333336, 0.07058824, 0.49411765, 0.6862745, 0.101960786, 0.6509804, 1.0, 0.96862745, 0.49803922, 0.11764706, 0.14117648, 0.36862746, 0.6039216, 0.6666667, 0.043137256, 0.05490196, 0.03529412, 0.85882354, 0.7764706, 0.7137255, 0.94509804, 0.3137255, 0.6117647, 0.41960785, 0.25882354, 0.32156864, 0.21960784, 0.8039216, 0.8666667, 0.8980392, 0.52156866, 0.7882353, 0.18039216, 0.30588236, 0.44705883, 0.3529412, 0.15294118, 0.6745098, 0.9490196, 0.7647059, 0.88235295, 0.99215686, 0.2509804, 0.19215687, 0.93333334, 0.9843137, 0.74509805, 0.7294118, 0.5882353, 0.50980395, 0.8862745, 0.105882354, 0.09019608, 0.16862746, 0.13725491, 0.21568628, 0.46666667, 0.3647059, 0.27450982, 0.8352941, 0.7176471, 0.5803922, 0.8117647, 0.9764706, 0.83137256, 0.98039216, 0.95686275, 0.003921569, 0.73333335, 0.54509807, 0.5294118, 0.67058825, 0.007843138, 0.31764707, 0.0627451, 0.09411765, 0.42352942, 0.627451, 0.9411765, 0.9882353, 0.5176471, 0.09803922, 0.1764706}\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(\"Load MNIST data set:\")\n",
    "(x_train, y_train), (x_test, y_test)= mnist.load_data()\n",
    "print('x_train:\\t{}'.format(x_train.shape))\n",
    "print('y_train:\\t{}'.format(y_train.shape))\n",
    "print('x_test:\\t\\t{}'.format(x_test.shape))\n",
    "print('y_test:\\t\\t{}'.format(y_test.shape))\n",
    "\n",
    "print()\n",
    "print(\"Encode y data:\") \n",
    "y_train_encoded = to_categorical(y_train)\n",
    "y_test_encoded = to_categorical(y_test)\n",
    "print(\"First ten entries of y_train:\\n {}\\n\".format(y_train[0:10]))\n",
    "print(\"First ten rows of one-hot y_train:\\n {}\".format(y_train_encoded[0:10,]))\n",
    "print()\n",
    "print('y_train_encoded shape: ', y_train_encoded.shape)\n",
    "print('y_test_encoded shape: ', y_test_encoded.shape)\n",
    "x_train_reshaped = np.reshape(x_train, (60000, 784))\n",
    "x_test_reshaped = np.reshape(x_test, (10000, 784))\n",
    "print('x_train_reshaped shape: ', x_train_reshaped.shape)\n",
    "print('x_test_reshaped shape: ', x_test_reshaped.shape)\n",
    "\n",
    "\n",
    "print()\n",
    "print('Min-Max Normalization:')\n",
    "x_train_norm = x_train_reshaped.astype('float32') / 255\n",
    "x_test_norm = x_test_reshaped.astype('float32') / 255\n",
    "print(set(x_train_norm[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5400/5400 [==============================] - 9s 2ms/step - loss: 0.0409 - accuracy: 0.9415\n",
      "600/600 [==============================] - 1s 844us/step - loss: 0.0267 - accuracy: 0.9675\n",
      "5400/5400 [==============================] - 9s 2ms/step - loss: 0.0403 - accuracy: 0.9411\n",
      "600/600 [==============================] - 1s 834us/step - loss: 0.0288 - accuracy: 0.9640\n",
      "5400/5400 [==============================] - 9s 2ms/step - loss: 0.0403 - accuracy: 0.9425\n",
      "600/600 [==============================] - 0s 824us/step - loss: 0.0326 - accuracy: 0.9622\n",
      "5400/5400 [==============================] - 9s 2ms/step - loss: 0.0407 - accuracy: 0.9413\n",
      "600/600 [==============================] - 0s 788us/step - loss: 0.0284 - accuracy: 0.9652\n",
      "5400/5400 [==============================] - 9s 2ms/step - loss: 0.0405 - accuracy: 0.9411\n",
      "600/600 [==============================] - 0s 825us/step - loss: 0.0280 - accuracy: 0.9657\n",
      "5400/5400 [==============================] - 9s 2ms/step - loss: 0.0409 - accuracy: 0.9413\n",
      "600/600 [==============================] - 0s 787us/step - loss: 0.0297 - accuracy: 0.9613\n",
      "5400/5400 [==============================] - 12s 2ms/step - loss: 0.0407 - accuracy: 0.9416\n",
      "600/600 [==============================] - 1s 862us/step - loss: 0.0301 - accuracy: 0.9620\n",
      "5400/5400 [==============================] - 12s 2ms/step - loss: 0.0400 - accuracy: 0.9422\n",
      "600/600 [==============================] - 0s 827us/step - loss: 0.0315 - accuracy: 0.9613\n",
      "5400/5400 [==============================] - 11s 2ms/step - loss: 0.0400 - accuracy: 0.9409\n",
      "600/600 [==============================] - 0s 803us/step - loss: 0.0322 - accuracy: 0.9632\n",
      "5400/5400 [==============================] - 10s 2ms/step - loss: 0.0414 - accuracy: 0.9402\n",
      "600/600 [==============================] - 0s 810us/step - loss: 0.0209 - accuracy: 0.9747\n",
      "6000/6000 [==============================] - 11s 2ms/step - loss: 0.0394 - accuracy: 0.9442\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn to grid search the learning rate and momentum\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "#from keras.optimizers import SGD\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(learn_rate=0.6, momentum=0, optimizer='adam', activation='relu', loss='binary_crossentropy'):\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(name = \"hidden_layer\", units=512, input_shape = (784,), activation=activation))\n",
    "    model.add(Dense(name = \"output_layer\", units=10, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model)\n",
    "\n",
    "# grid search parameters\n",
    "learn_rate = [0.6]#[0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "momentum = [0]#[0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "optimizers = ['rmsprop']\n",
    "epochs = np.array([10])\n",
    "batches = np.array([10])\n",
    "activation = ['relu']#,'tanh']#,'softmax']\n",
    "loss = ['binary_crossentropy'] #,'categorical_crossentropy']\n",
    "#units_1 = [10,20,30,512,]\n",
    "#units_2 = [0.3]#[0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "\n",
    "param_grid = dict(\n",
    "    learn_rate=learn_rate,\n",
    "    momentum=momentum,\n",
    "    optimizer=optimizers,\n",
    "    nb_epoch=epochs,\n",
    "    batch_size=batches,\n",
    "    activation=activation,\n",
    "    loss=loss\n",
    ")\n",
    "\n",
    "#history = model.fit(x_train_norm,y_train_encoded, epochs=20,validation_split=0.20,batch_size = 10)\n",
    "grid_result = GridSearchCV(estimator=model, param_grid=param_grid, cv=10).fit(x_train_norm, y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pred_classes = np.argmax(model.predict(x_train_norm), axis=-1)\n",
    "pixel_data = {'pred_class':pred_classes}\n",
    "for k in range(0,784): \n",
    "    pixel_data[f\"pix_val_{k}\"] = x_train_norm[:,k]\n",
    "pixel_df = pd.DataFrame(pixel_data)\n",
    "pixel_df.head()\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# Separating out the features\n",
    "features = [*pixel_data][1:] # ['pix_val_0', 'pix_val_1',...]\n",
    "x = pixel_df.loc[:, features].values \n",
    "\n",
    "pca = PCA(n_components=154)\n",
    "principalComponents = pca.fit_transform(x_train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learn_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>momentum</th>\n",
       "      <th>nb_epoch</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.9647</td>\n",
       "      <td>0.003841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  activation  batch_size  learn_rate                 loss  momentum  nb_epoch  \\\n",
       "0       relu          10         0.6  binary_crossentropy         0        10   \n",
       "\n",
       "  optimizer  mean_test_score  std_test_score  \n",
       "0   rmsprop           0.9647        0.003841  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('Display.max_rows', None)\n",
    "para = pd.DataFrame.from_dict(grid_result.cv_results_['params'])\n",
    "mean = pd.DataFrame(grid_result.cv_results_['mean_test_score'],columns=['mean_test_score'])\n",
    "stds = pd.DataFrame(grid_result.cv_results_['std_test_score'],columns=['std_test_score'])\n",
    "df = para.join(mean.join(stds)).sort_values(['mean_test_score'], ascending=False)\n",
    "df.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5400/5400 [==============================] - 7s 1ms/step - loss: 0.0401 - accuracy: 0.9438\n",
      "600/600 [==============================] - 0s 733us/step - loss: 0.0231 - accuracy: 0.9700\n",
      "5400/5400 [==============================] - 7s 1ms/step - loss: 0.0401 - accuracy: 0.9434\n",
      "600/600 [==============================] - 0s 727us/step - loss: 0.0228 - accuracy: 0.9680\n",
      "5400/5400 [==============================] - 7s 1ms/step - loss: 0.0397 - accuracy: 0.9446\n",
      "600/600 [==============================] - 0s 696us/step - loss: 0.0277 - accuracy: 0.9637\n",
      "5400/5400 [==============================] - 7s 1ms/step - loss: 0.0403 - accuracy: 0.9433\n",
      "600/600 [==============================] - 0s 738us/step - loss: 0.0227 - accuracy: 0.9697\n",
      "5400/5400 [==============================] - 7s 1ms/step - loss: 0.0400 - accuracy: 0.9435\n",
      "600/600 [==============================] - 0s 721us/step - loss: 0.0260 - accuracy: 0.9668\n",
      "5400/5400 [==============================] - 7s 1ms/step - loss: 0.0395 - accuracy: 0.9445\n",
      "600/600 [==============================] - 0s 659us/step - loss: 0.0251 - accuracy: 0.9642\n",
      "5400/5400 [==============================] - 7s 1ms/step - loss: 0.0403 - accuracy: 0.9442\n",
      "600/600 [==============================] - 0s 698us/step - loss: 0.0253 - accuracy: 0.9652\n",
      "5400/5400 [==============================] - 7s 1ms/step - loss: 0.0395 - accuracy: 0.9453\n",
      "600/600 [==============================] - 0s 663us/step - loss: 0.0286 - accuracy: 0.9637\n",
      "5400/5400 [==============================] - 7s 1ms/step - loss: 0.0398 - accuracy: 0.9441\n",
      "600/600 [==============================] - 0s 662us/step - loss: 0.0286 - accuracy: 0.9618\n",
      "5400/5400 [==============================] - 7s 1ms/step - loss: 0.0406 - accuracy: 0.9418\n",
      "600/600 [==============================] - 0s 695us/step - loss: 0.0209 - accuracy: 0.9755\n",
      "6000/6000 [==============================] - 8s 1ms/step - loss: 0.0384 - accuracy: 0.9467\n"
     ]
    }
   ],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(learn_rate=0.6, momentum=0, optimizer='adam', activation='relu', loss='binary_crossentropy'):\n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(name = \"hidden_layer\", units=512, input_shape = (154,), activation=activation))\n",
    "    model.add(Dense(name = \"output_layer\", units=10, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model)\n",
    "#history = model.fit(principalComponents,y_train_encoded, epochs=20,validation_split=0.10,batch_size = 10)\n",
    "grid_result = GridSearchCV(estimator=model, param_grid=param_grid, cv=3).fit(principalComponents, y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learn_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>momentum</th>\n",
       "      <th>nb_epoch</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.9647</td>\n",
       "      <td>0.003841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  activation  batch_size  learn_rate                 loss  momentum  nb_epoch  \\\n",
       "0       relu          10         0.6  binary_crossentropy         0        10   \n",
       "\n",
       "  optimizer  mean_test_score  std_test_score  \n",
       "0   rmsprop           0.9647        0.003841  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para = pd.DataFrame.from_dict(grid_result.cv_results_['params'])\n",
    "mean = pd.DataFrame(grid_result.cv_results_['mean_test_score'],columns=['mean_test_score'])\n",
    "stds = pd.DataFrame(grid_result.cv_results_['std_test_score'],columns=['std_test_score'])\n",
    "df = para.join(mean.join(stds)).sort_values('mean_test_score', ascending=False)\n",
    "df.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learn_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>momentum</th>\n",
       "      <th>nb_epoch</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>0.96685</td>\n",
       "      <td>0.003866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  activation  batch_size  learn_rate                 loss  momentum  nb_epoch  \\\n",
       "0       relu          10         0.6  binary_crossentropy         0        10   \n",
       "\n",
       "  optimizer  mean_test_score  std_test_score  \n",
       "0   rmsprop          0.96685        0.003866  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para = pd.DataFrame.from_dict(grid_result.cv_results_['params'])\n",
    "mean = pd.DataFrame(grid_result.cv_results_['mean_test_score'],columns=['mean_test_score'])\n",
    "stds = pd.DataFrame(grid_result.cv_results_['std_test_score'],columns=['std_test_score'])\n",
    "df = para.join(mean.join(stds)).sort_values('mean_test_score', ascending=False)\n",
    "df.reset_index().drop(columns=['index'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
